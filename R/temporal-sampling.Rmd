---
title: "Sampling requirements to detect ecosystem change"
author: "Rosalie Bruel and Easton White"
date: "03/10/2019"
output:
  html_document:
    df_print: paged
    fig_caption: yes
    number_sections: yes
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
fontsize: 11pt
editor_options:
  chunk_output_type: console
  df_print: paged
---

# Things to do:

- Easton: build theoretical results figure
- Sketch out figures for case studies
- Conceptual diagram 
- Begin writing - probably a shorter piece
- Possible journals: Methods Ecology and Evolution, Biological Conservation, Journal of Environmental Management, Environmental Monitoring and Assessment, BioScience, 

# Introduction and setup

- Set up ideas about monitoring and cost of monitoring
- Idea of detecting change over time
- Historical data and ability to choose how much to sample
- Here, we develop a set of tools to ... We first investigate these tools using a simulated-based apporach. We then apply these tools to the detection of change points for a set of lakes...


# Methods and results

Before running the script, define below in the function _getpath4data_ the local folder where you are storing the dataset(s). <br>
The Lake Zurich countings were carried out as part of a study during my PhD, comparing several lakes trajectory. I still haven't published that study.
I have an agreement with the different persons who gave me access to sequences that they will be co-author on the publication. In the case of Lake Zurich sequence, this person is Nathalie Dubois (ETH Zurich, CH). So until this is figured out, I'd rather not publish any data on my Github.

```{r setup, include=FALSE}
knitr::opts_knit$set(root.dir = dirname(getwd()))
knitr::opts_chunk$set(warning = FALSE, message = FALSE)

getpath4data <- function() {
  if(Sys.getenv("USER")=="Rosalie") return("/Volumes/-/Script R/Data-off-GitHub/temporal-sampling/")
  if(Sys.getenv("USER")=="eastonwhite") return("~/Desktop/Research/soil-temporal-sampling/")
  if(Sys.getenv("USER")!="Rosalie"|Sys.getenv("USER")!="put here your USER") stop("You need to get the data and define their location.")
}

fig_cap <- captioner::captioner()
tab_cap <- captioner::captioner("Table")

# Libraries
library(dplyr)
require(rioja)
require(vegan)
require(ade4)
require(changepoint)
require(ggplot2)
require(gridExtra)
require(plotly)
library(reshape)

```

# Lake Zurich

```{r read zurich data, include=T}
zh <- read.delim(paste0(getpath4data(),"ZH17_21_clado.txt")); is.zh=T
zh <- read.delim(paste0(getpath4data(),"VAR10-10-clado.txt")); is.zh=F
```

## Study site
Lake Zürich (CH) has a surface area of 65 km<sup>2</sup> and a maximum depth of 136 meters. The lake is divided into two basins. The lake is considered to be monomictic or dimictic, but the increase of strength of thermal stratification in the last few decades as a consequence of climate warming impedes complete mixing of the water column (Anneville et al., 2004^[Anneville, O., Souissi, S., Gammeter, S., Straile, D., 2004. Seasonal and inter-annual scales of variability in phytoplankton assemblages: comparison of phytoplankton dynamics in three peri-alpine lakes over a period of 28 years. Freshwater Biology 49, 98–115. https://doi.org/10.1046/j.1365-2426.2003.01167.x]).

## Coring and dating
A short core (ZH-17-21, 103 cm) was taken from the pelagic zone in 2017. Only the upper 50 cm were sampled and analysed within this study. The sediment of Lake Zurich presents a laminated facies on the surface (0-28.3 cm) that allowed varves counting. A linear interpolation allowed getting ages for the rest of the core. There was no change of sedimentation for the top 22.5-cm (1930-2017) which support the realism of such approximation. From 22.5 cm downward, the model fitted with only 3 points indicates a higher sedimentation rate.
The sediment sequence covers the 1945-2017 period. The core was subsampled every centimetre for biological analysis (diatoms and cladocera remains).

## Cladoceran community

Shortly, cladoceran stratigraphy  and result of CONISS clustering.

```{r coniss, include=T, echo=F}
diss<-dist(sqrt(zh[,-c(1)]/100)^2)
clust<-chclust(diss,method="coniss")
par(mfrow=c(2,1))
par(mar=c(5.1,4.1,2.1,20))
bstick(clust, cex=0.8)
par(mar=c(4,3.1,1.1,1.1))
plot(clust, labels=zh[,1], cex=0.8)
```
<br>`r fig_cap("coniss clustering zurich", "Selection of significant groups and constrained hierarchical clustering of Lake Zürich sequence")`

```{r strat plot zurich, include=T, echo=F}
par(mfrow=c(1,1))
par(mar=c(5.1,4.1,2.1,20))
clado.clust<-strat.plot(zh[,-c(1)],yvar=zh[,1],col.line="black",lwd.line=0,col.bar="black",lwd.bar=2, x.names=colnames(zh[-c(1)]), cex.xlabel=1, srt.xlabel=30,scale.minmax=FALSE, y.rev = F,clust=clust)
addClustZone(clado.clust,clust,4,col="black")
```
<br>`r fig_cap("strat plot zurich", "Stratigraphic abundance of cladoceran remains in Lake Zürich sediment core ZH17-21. (I'm aware it's impossible to read it with that many taxa, I have the same code somewhere applied to sub groups which make it way easier to read)")`

```{r strat plot zurich selected sp, echo=FALSE}
if(is.zh){
  par(mfrow=c(1,1))
par(mar=c(5.1,4.1,2.1,20))
clado.clust<-strat.plot(cbind(zh[,colnames(zh) %in% c("LK","SC","DL","EL","EC","BL","BYL")], rowSums(zh[,-1][,!colnames(zh) %in% c("LK","SC","DL","EL","EC","BL","BYL")])),yvar=zh[,1],col.line="black",lwd.line=0,col.bar="black",lwd.bar=2, x.names=c(colnames(zh[colnames(zh) %in% c("LK","SC","DL","EL","EC","BL","BYL")]), "others"), cex.xlabel=1, srt.xlabel=30,scale.minmax=FALSE, y.rev = F,clust=clust)
addClustZone(clado.clust,clust,4,col="black")
}
```
<br>`r fig_cap("strat plot zurich selected sp", "Stratigraphic abundance of dominant cladoceran remains in Lake Zürich sediment core ZH17-21. Abbreviations: BL= Bosmina longirostris, EC= Eubosmina coregoni, EL= Eubosmina longispina, DL= Daphnia spp., LK= Leptodora kindti, BYL= Bythotrephes longismanus, SC= Sida crystallina")`

CONISS clustering identified four significant different cladoceran assemblages over the past 170 years. From 1845 to 1903, _Eubosmina longispina_ and the predator _Bythotrephes longismanus_ dominate the pelagic assemblage. macrophyte-associated species are present ( _Eurycercus_ sp., _Alona quandrangularis_, _Sida crystallina_) attesting for the presence of macrophytic littoral zone. From 1905 to 1908, there is a transient assemblage dominated by pelagic taxa ( _Daphnia longispina_ and _Bosmina longirostris_). The later is a small taxa, wall the usually large Daphnia longispina lose 25 μm in average at this period . This smaller assemblage is associated with a transient disappearance of predator species ( _Leptodora kindti_). This dynamic alone would require more investigation and may come from a change in the top-down pressure, if fish happened to be introduced at that time. However, I did not find any information in the English-written literature mentioning fisheries management of Lake Zurich at that time. The following period from 1911 to 1957 is characterized by a dominance of _Daphnia_ spp. and _Bosmina longirostris_ species in the pelagic compartment. The predator _B. longismanus_ and _Leptodora kindtii_ are also present. The overall abundance of littoral taxa increases, but the macrophyte-associated taxa are replaced by more ubiquitous taxa ( _Chydorus sphaericus_). The last transition took place from 1961, and characterise the recent assemblage. The overall abundance increases for almost all groups. Larger pelagic grazers returned ( _Eubosmina_ sp., larger size _Daphnia_ spp. Figure not included here, refer to thesis). Large predators share the habitat. Chydoridae taxa remained an important part of the assemblage despite eutrophication, unlike what happened in many other lakes of the study. <br>
Boucherle & Züllig (1983)^[Boucherle, M.M., Züllig, H., 1983. Cladoceran remains as evidence of change in trophic state in three Swiss lakes. Hydrobiologia 103, 141–146. https://doi.org/10.1007/BF00028442] used the shift between assemblages dominated by Daphnia spp. and Bosmina sp. to infer the level eutrophication. Using this ratio, we are in accordance with their results and timing of water quality. The insight brought from diatoms analysis on the trophic level also corroborates a two-stages eutrophication, with an increase in water quality in the early years of World War II. Globally, Daphnia spp. group is once again (Alric et al., 2013)^[Alric, B., Möst, M., Domaizon, I., Pignol, C., Spaak, P., Perga, M.-E., 2016. Local human pressures influence gene flow in a hybridizing Daphnia species complex. J. Evol. Biol. 29, 720–735. https://doi.org/10.1111/jeb.12820] providing an early warning for eutrophication with their abundance increasing as soon as 1883. <br>
Having said that, the identification to the specie level in the _Bosmina_ sp. group differs between their study and the present sequence. We did observe the appearance of the small _Bosmina longirostris_ group from the late 19th century, but found remains of _Eubosmina longispina_ during the 20<sup>th</sup> century as well while Boucherle & Züllig (1983) reported its total disappearance from the sediment. Overall, the results are still very similar, and the difference may come from misidentifications or updated identification keys.


# Changepoint analysis

In most paleo-reconstructions, we're interested in the changepoints in the series: when the assemblage shifted. Changepoint are detected on a single vector, so we would usually run a PCA first on the dataset.

Would random sampling generate the same results as if all the samples were analyzed?


# Types of sampling

1. Random <br>
2. Regular <br>
3. Convergent <br>

```{r}
source("ar1_process.R")
source("R/uncertainty_tolerance.R")
args(ut)
```


When analysing more or less sample is easy (e.g., counting cladoceran samples vs analyzing DNA samples), can we find an optimal sampling?

Logic is a bit different, we want to know what is the minimum number of random sample we should start with, and then add samples around it to actually find the shifting point

```{r echo=FALSE, message=FALSE, warning=FALSE, cache=T}
output <- ut(y=exp(ar1), sample_size = c(20:50), threshold_error = 5, each = 100, type="mean")
names(output)
head(output$convergence_cptmean)
ggplot(output$convergence_cptmean, aes(sample_size,number_sample_added)) + geom_point(alpha=.3)

```
<br>`r fig_cap("how many samples must be added to get true changepoint method 2", "Adding progressively samples to fill in the gaps between transition and next/previous sample.")`


```{r summary, echo=FALSE, message=FALSE, warning=FALSE}
names(output$random_cptmean)

outsumm <- as.data.frame(with(output$random_cptmean, tapply(same_cptmean,list("Initial_number_samples"=sample_size,"threshold_yr"=threshold_error), sum)))
outsumm$InitialNumber <- as.numeric(rownames(outsumm))
outsumm <- melt(outsumm, id.vars = "InitialNumber")
outsumm$more80 <- ifelse(outsumm$value>=80,1,0)

ggplot(outsumm, aes(InitialNumber, variable, fill=more80)) + geom_tile() +
  xlab("Sample analyzed") + ylab("Tolerance for error (year)")

```
<br>`r fig_cap("how many random samples to get the break on average", "Dark blue: the random number of samples analyzed did not allow detection of the break")`

# Do the same but change the location of the changepoint

```{r message=FALSE, warning=FALSE, cache=T, include=FALSE}
shift_vector <- sample(2:99, size = 50, replace = F)

for (i in shift_vector) {
  # Set simulation parameters
  mu = 0
  sigma = 0.5
  phi = 1
  shift_time = i
  max_time = 100
  
  # Set up vectors
  ar1 <- vector(mode = 'numeric',100)
  ar1[1] <- 10
  
  # Build time series
  for (t in 2:max_time){
    if(t == shift_time){ar1[t-1]=-10}
    ar1[t] <- ar1[t-1] + phi*rnorm(1,mean=mu,sd = sigma)
  
  }
  
  temporary <- ut(y=exp(ar1), sample_size = c(10:50), threshold_error = c(1:4), each = 1, type="mean")
  
  if(i==shift_vector[1]) {
    out_regular_cptmean     <- temporary$regular_cptmean
    out_random_cptmean      <- temporary$random_cptmean
    out_convergence_cptmean <-temporary$convergence_cptmean
  } else {
    out_regular_cptmean     <- rbind(out_regular_cptmean, temporary$regular_cptmean)
    out_random_cptmean      <- rbind(out_random_cptmean, temporary$random_cptmean)
    out_convergence_cptmean <- rbind(out_convergence_cptmean, temporary$convergence_cptmean)
  }
}

dim(out_regular_cptmean)
dim(out_random_cptmean)
dim(out_convergence_cptmean)

outsumm <- as.data.frame(with(out_random_cptmean, tapply(same_cptmean,list("Initial_number_samples"=sample_size,"threshold_yr"=threshold_error), sum)))
outsumm$InitialNumber <- as.numeric(rownames(outsumm))
outsumm <- melt(outsumm, id.vars = "InitialNumber")
outsumm$more80 <- ifelse(outsumm$value>=50*.8,1,0)


```

```{r}
ggplot(outsumm, aes(InitialNumber, variable, fill=more80)) + geom_tile() +
  xlab("Sample analyzed") + ylab("Tolerance for error (year)")

# Something is wrong here, it is sometimes adding >900 samples.I think it has to do with the while loop in the ut() function.
ggplot(out_convergence_cptmean, aes(sample_size,number_sample_added)) + geom_point(alpha=.15)  + facet_wrap(~threshold_error)

boxplot(out_convergence_cptmean$number_sample_added ~ out_convergence_cptmean$sample_size, ylim=c(0,25))
```


# Theoretical results

We began with a theoretical exploration of the determiners of sampling required to detect a changepoint. We modeled a simple AR-1 process with a response variable that represents either population size, biodiversity, or some metric of community composition. The model includes temporal variability ($\sigma$), autocorrelation ($\phi$), the size of the shift ($\delta$), and a specific time when the shift occured ($\tau$):

$$
\mbox{Model here - need pair of equations}
$$

We then explored how each of these model components affected our ability to detect a change point. We simulate an entire time series, but we also examine the effect of not sampling every year. We specifically study how the amount of samples included and the type of sampling affects the detection probability. In particular, we examine three types of sampling: random, regular, and iterative. 

```{r theoretical_results, fig.cap='Theoretical results here',message=F,warning=F,echo=F}
# 4(or 6?) panel plot
## Example time series
## Changepoint detection vs number of samples - maybe just stick with regular sampling
## Number of samples required vs shift size, shift time, variance, autocorrelation

# Need to build function to output the time series
# Use purrr to try with a bunch of different parameters

source('R/uncertainty_tolerance.R')

build_time_series <- function(mu=0,sigma=5,phi=0.5,shift_size=25,shift_time=50,max_time=100){
  
  ar1 <- vector(mode = 'numeric',max_time)
  ar1[1] <- 100
  for (t in 2:max_time){
    if(t == shift_time){ar1[t-1]=(ar1[1]-shift_size)}
    ar1[t] <- ar1[t-1] + phi*rnorm(1,mean=mu,sd = sigma)
  }
  return(ar1) 
}

require(tidyverse)

par(mfrow=c(2,2))
# Sigma plot
params <- tibble(sigma=rep(seq(0.1,20,0.1),100))

output <- params %>%
  pmap(build_time_series) %>%
  map_dbl(function(df) ut(y=df, sample_size = 20, threshold_error = 5, each = 1, type="mean",random_sampling = F,convergence_sampling = F,regular_sampling = T)$regular_cptmean$same_cpt) %>%
  mutate(params,detection=.)

output %>%
  group_by(sigma) %>%
  summarize(prob_correct = sum(detection==1)/length(unique(sigma))) %>%
  plot(ylab='Probability correct',xlab='sigma',pch=16,cex.lab=1.3,las=1)


# phi plot
params <- tibble(phi=rep(seq(-1,1,0.01),100))

output <- params %>%
  pmap(build_time_series) %>%
  map_dbl(function(df) ut(y=df, sample_size = 20, threshold_error = 3, each = 1, type="mean",random_sampling = F,convergence_sampling = F,regular_sampling = T)$regular_cptmean$same_cpt) %>%
  mutate(params,detection=.)

output %>%
  group_by(phi) %>%
  summarize(prob_correct = sum(detection==1)/length(unique(phi))) %>%
  plot(ylab='Probability correct',xlab='phi',pch=16,cex.lab=1.3,las=1)


# shift_size plot
params <- tibble(shift_size=rep(seq(1,95,0.5),100))

output <- params %>%
  pmap(build_time_series) %>%
  map_dbl(function(df) ut(y=df, sample_size = 20, threshold_error = 3, each = 1, type="mean",random_sampling = F,convergence_sampling = F,regular_sampling = T)$regular_cptmean$same_cpt) %>%
  mutate(params,detection=.)

output %>%
  group_by(shift_size) %>%
  summarize(prob_correct = sum(detection==1)/length(unique(shift_size))) %>%
  plot(ylab='Probability correct',xlab='shift size',pch=16,cex.lab=1.3,las=1)


# Shift timing plot
params <- tibble(shift_time=rep(seq(5,90,1),100))

output <- params %>%
  pmap(build_time_series) %>%
  map_dbl(function(df) ut(y=df, sample_size = 20, threshold_error = 3, each = 1, type="mean",random_sampling = F,convergence_sampling = F,regular_sampling = T)$regular_cptmean$same_cpt) %>%
  mutate(params,detection=.)

output %>%
  group_by(shift_time) %>%
  summarize(prob_correct = sum(detection==1)/length(shift_time/unique(shift_time))) %>%
  plot(ylab='Probability correct',xlab='shift size',pch=16,cex.lab=1.3,las=1)








#params <- tibble(phi=rep(seq(-1,1,0.01),1),shift_size=rep(30,201))
#output <- params %>%
#  pmap(build_time_series)
#plot(output[[1]],type='l')
#points(output[[15]],type='l')
```




# Limits

Is changepoint necesseraly what we want? We can be interested in changes in mean and/or variance. Right now, I've just looked at mean.

A few methods looking at critical transition (e.g., Taranu et al 2018^[Taranu, Z.E., Carpenter, S.R., Frossard, V., Jenny, J.-P., Thomas, Z., Vermaire, J.C., Perga, M.-E., 2018. Can we detect ecosystem critical transitions and signals of changing resilience from paleo-ecological records? Ecosphere 9, e02438. https://doi.org/10.1002/ecs2.2438]), or increase variance (e.g., Dakos et al 2012^[Dakos, V., Carpenter, S.R., Brock, W.A., Ellison, A.M., Guttal, V., Ives, A.R., Kéfi, S., Livina, V., Seekell, D.A., van Nes, E.H., Scheffer, M., 2012. Methods for Detecting Early Warnings of Critical Transitions in Time Series Illustrated Using Simulated Ecological Data. PloS One 7, e41010. https://doi.org/10.1371/journal.pone.0041010]), etc., will value having more continuous sampling (although the online DLMs method in Taranu et al 2018 allows for non-continuous sampling).

If we know a priori the shape of the relationship, a non-random sampling is preferred, e.g., <a href="https://onlinelibrary.wiley.com/doi/full/10.1002/env.2552"> VB growth function </a>.
