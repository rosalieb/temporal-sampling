---
bibliography: Whitebib.bib
csl: ecology-letters.csl
editor_options:
  chunk_output_type: console
fontsize: 12pt
geometry: margin=1in
header-includes: \usepackage{float} \usepackage{lineno} \usepackage{setspace}\doublespacing
  \usepackage[round]{natbib} \bibpunct[; ]{(}{)}{,}{a}{}{,} \usepackage{color} \usepackage{totcount}
  \newtotcounter{citenum} \def\oldcite{} \let\oldcite=\bibcite \def\bibcite{\stepcounter{citenum}\oldcite} \newcommand\easton[1]{\textcolor{red}{#1}} \newcommand\rosalie[1]{\textcolor{blue}{#1}}
output:
  pdf_document:
    fig_caption: yes
    keep_tex: yes
    number_sections: no
---

```{r setup, include=FALSE}
knitr::opts_knit$set(root.dir = dirname(getwd()))
```

Title: Sampling requirements and approaches to detect ecosystem shifts  
	
\vspace{7 mm}
	
Authors: Rosalie Bruel$^{1}$ and Easton R. White$^{2}$
	
\vspace{5 mm}
	
Addresses: $^{1}$Rubenstein Ecosystem Science Laboratory, University of Vermont, VT 05405, USA $^{2}$Department of Biology, University of Vermont, VT 05405, USA https://orcid.org/0000-0002-0768-9555

\vspace{5 mm}
Keywords: time-series, changepoint \vspace{10 mm}
	
\vspace{5 mm}

	
Number of words: XXXX \vspace{5 mm}
	
Number of references: \total{citenum} 
	
Number of figures and panels: X
	
\vspace{5 mm}
	
Submission as: Article at \emph{Ecological Applications}
	
\vspace{5 mm}
	
Preprint available at: 


\clearpage


\begin{center}
\textbf{\Large Sampling requirements and approaches to detect ecosystem shifts}
\vspace{7 mm}
	
\textsc{Rosalie Bruel$^{1}$ and Easton R. White$^{2}$}
\vspace{5 mm}

\normalsize{\indent $^{1}$Rubenstein Ecosystem Science Laboratory, University of Vermont, VT 05405, USA \\ $^{2}$Department of Biology, University of Vermont, VT 05405, USA}
\end{center}

\textbf{Abstract}

(150-250 words)

Environmental monitoring is a key piece of managing ecosystems. Given that most monitoring efforts are still expensive and time-consuming, it is essential that monitoring programs are designed to be efficient and effective. In many situations, the expensive part of monitoring is not sample collection, but instead sample processing, which leads to only a subset of the samples being processed. For example, sediment or ice cores can be taken in the field in less than an hour, but will be processed in a laboratory setting for anytime between a week and several months, depending on how much information is extracted. Sub-sampling allows to reduce the processing time while still giving precious insight. Standard sub-sampling approaches often involve equally-spaced sampling. We use simulations to show how many samples and which types of sampling approaches are the most effective in detecting ecosystem change. We test these ideas with a case study of Cladocera community assemblage, which trajectory was reconstructed from a sediment core. We demonstrate that standard approaches to sample processing are inefficient. For our case study, using an optimal sampling approach would have resulted in savings of XX human-hours or thousands of dollars in labor costs. We also show that, compared with these standard approaches, fewer samples are typically needed to achieve high statistical power. We explain how our approach can be applied to monitoring programs that rely on video records, eDNA, remote sensing, and other common tools that allow re-sampling. 

\vspace{3 mm}

Keywords: {time series}


```{r load_packages,echo=F,warning=F,message=F}
if (!require("pacman",character.only = TRUE))
  {
    install.packages("pacman",dep=TRUE)
    if(!require("pacman",character.only = TRUE)) stop("Package not found")
  }

# Keeping below source for github package. Ask Easton whether pacman works for github packages or not.
#devtools::install_github("rensa/stickylabeller")
  
pacman::p_load(patchwork, stickylabeller, ggplot2, dplyr, wesanderson, reshape)
```


<!--
# Things to do:

- writing intro
- writing methods/results pair of section
- writing discussion
- setting up bibliography system
- Possible journals: Methods Ecology and Evolution, Biological Conservation, *Journal of Environmental Management*, *Environmental Monitoring and Assessment*, BioScience 


# Main messages of manuscript

- We can choose the number of samples and effort we use...
- Number of samples required depends on question asked and the time series parameters
- Case study of paleo data shows the utility of iterative sampling in detecting change points
--> 

# Introduction 

Environmental monitoring is one of the core components to modern research and management [@White2019d]. Within an adaptive management framework, monitoring is needed for both learning about the system under study and assessing the effectiveness of management interventions [@Lovett2007]. Increasingly, long-term monitoring programs, like the Long Term Ecological Research (LTER) Network in the USA, are becoming available [@Maguran2010a]. However, environmental monitoring is still often expensive and time-consuming, especially when further analyses are needed following sample collection (citations). In Ecology and Genetics especially, there has always been a disparity between the amount of data that can be aquired and stored, and the processing methods that could allow to extract the most information from that data. \rosalie{I feel like it could be a sensitive subject (we don't want to attack our peers and the way they do science), but there is definitely some truths. With some restoration work going on at the lab, there were some emails exchange about throwing away samples that were collected in 2015 and never looked at. If we could find the correct way to talk about this issue (without being judgemental, I'm sure it'd be interesting to many!).}

Therefore, monitoring programs need to be designed in such a way to address the question of interest while using limited resources efficiently. Key considerations of an optimal monitoring program include the frequency of surveys, the selection of sites, as well as the precision and accuracy of the monitoring itself [@McDonald-Madden2010a,@White2019d,@White2020,@Lindenmayer2020a]. Monitoring programs also need to be designed to be cost-effective given limited budgets [@Caughlan2001a,@Grantham2008b,@Bennett2016a]. 


<!-- 
Methods to detect change overtime: see summary https://lindeloev.github.io/mcp/articles/packages.html

- Historical data and ability to choose how much to sample
--> 

Accounting for these considerations in the design of a monitoring program is necessary to detect long-term ecological change. The specifics of the monitoring program will determine the power with which a question of interest can be addressed. For example, @White2019d found that 72\% of vertebrate populations required at least 10 years of monitoring to detect significant changes in the population size over time. The specific number of years required depended on the species biology and the detection method used [@White2019d]. Other work has focused on the frequency of monitoring [@Wauchope2019a], the allocation of resources spatially versus temporally [@Rhodes2011a,@Weiser2019a], and the ability to use citizen science to increase monitoring [@Weiser2020a]. Lastly, both the ecological and economical costs of failing to detect a true trend (type II error) have to be weighed against the risks of false (type I error) detection [@Mapstone1995a].

Because of new technological advances, there are many data sources that can be derived long after the actual  processes occurred. For example, sediment cores can be retrieved from lakes, allowing reconstruction of past ecological communities or conditions (Cohen, 2003). Similarly, environmental samples (e.g. water, soil) can be saved and processed later for composition, including eDNA [@Bohmann2014a]. Likewise, photo- or video-based monitoring can record snapshots of a system and be analyzed later (citations). In each of these cases, decisions have to made about how much data to extract from the previously collected samples. Should the paleoecological core be analyzed at every centimeter? Should the video be assessed once per minute? As long as processing samples is expensive, these trade-offs will remain.

Here, we develop a set of tools to determine the appropriate number of samples and sampling approach when dealing with data sources where additional samples can be added. We tailor our analysis to the detection of changepoint in a time series, were each point had a certain degree of autocorrelation with the previous, until a changepoint come. Changepoints are an important characteristic of a time series. We focus on paleoecological core samples as one example of this type of data. We examine the situation where the goal is to detect the time at which a significant change in an ecological community occurs, a changepoint. However, our approaches are widely applicable to other questions and data types. We first investigate these tools using a simulated-based approach. We then test the tools on a case study from a paleosequence. 


# Sampling approaches and changepoint detection

For both our simulations and case study, we investigate the effect of different sampling strategies on our ability to detect a changepoint. We begin by either creating simulated time series or using an actual paleoecological time series (Fig. \ref{fig:conceptual}). We then subsampled each time series (citation) to test the effect of three different sampling approaches along with varying the sample size (Fig. \ref{fig:conceptual}c-f). We compared the subsampled time series to the full time series as a measure of the effectiveness of that sampling approach.

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=7, fig.height=7.5,eval=T,fig.cap='Conceptual diagram illustrating the process of taking (a) simulations of a time series and (b) selecting a single simulation to analyze with three different sampling approaches: (c) random, (d) regular, and (e) iterative. The iterative sampling approach requires (f) adding samples around a detected changepoint until (g) a certain level of accuracy is achieved. \\label{fig:conceptual}'}
source(paste0(getwd(),"/R/Fig_conceptual.R"))
```

The random sampling approach involves taking a set number of random points throughout the time series (Fig. \ref{fig:conceptual}c). In the context of sediment cores, this would mean analyzing community composition as random location along the core. Random sampling is recommanded in designs aiming at quantifying the average size of a population (spatial approach) (Nad’o and Kaňuch, 2018). We hypothesize that random sampling will perform the worst at getting as close as possible to a "real" changepoint. Regular sampling is commonly used (citations) and requires that a set number of samples be taken at regular intervals (Fig. \ref{fig:conceptual}d). Lastly, iterative sampling involves first taking a set number of samples and then alliteratively adding samples until a pre-determined level of precision is achieved (Fig. \ref{fig:conceptual}e-g). In each scenarios, we sample the first and last sample, in case the changepoint takes place early or late in the period covered by the sampling.


We detect changepoints with the function _e.divisive()_ in the R package _ecp_ (James et al 2019). There are several methods available to detect changepoint (reviewed in James and Matteson, 2014); _e.divisive_ is a divisive hierarchical estimation algorithm for multiple change point analysis. We chose this method because it is able to perform multiple change point analysis for both uni- and multi-variate time serie, without a priori knowledge of the number of change points. Herein, we focus on detecting the most important changepoint, although we tested the method on a time-serie that would have multiple changepoints (Supplementary Materials 1).


We wrote three functions:  
  * sample_random(): sample first and last sample, in addition of n-2 random samples in between (n = maximum number of sample, chosen by the user),
  * sample_regular(): sample first and last sample, in addition of n-2 evenly spaced samples in between (n = maximum number of samples, chosen by the user),
  * sample_regular(): sample first and last sample as well as 3 evenly spaced samples in between (3 is the default but can be modified), in order to initiate the changepoint detection. Upon detection of the changepoint on this 5 samples time series, a new sample is added between the changepoint and the previous sample, to narrow down the real changepoint. Detection of changepoint / addition of sample is repeated, until it was narrowed down to two consecutive samples, or n (maximum number of samples, chosen by the user) was reached, whichever comes first.
  

In order to test the performance, we detected the "real" changepoint on the whole time-serie, and compare the changepoint found on the sub-sample with the "real" one. The distance to real changepoint served as performance diagnostic.


# Theoretical approach

<!--
- explain the modeling approach
- show the model
- explain the analysis we will do
- explain the theoretical model results with a figure
- explain effect of different sampling strategies in this context

Theoretical plots

- One combined plot to see the effect (on prob correct or number of samples) of each parameter - should do this with one of the three types of sampling (probably regular)
- Theoretical plot comparing the three different methods - we could choose one set of parameter values and show how the various methods perform with different sampling amounts (could use same plot structure as the case study)
- SM plot that shows how the three different methods are affected by changes in the underlying parameter values

-->

## Simulation model

We began with a theoretical exploration of the sampling requirements to detect a changepoint. We modeled a simple first order autoregressive (AR-1) process (the discrete-time version of the Ornstein–Uhlenbeck process) with a response variable ($X_t$) that represents either population size, biodiversity, or some metric of community composition at time $t$. The model includes temporal autocorrelation ($\phi$), the mean of the process ($\mu_X$), and a white noise term ($w_t$). The white noise term is a normal distribution with mean ($\mu_w$) and variance ($\sigma^2$):

$$
\begin{array}{rcl} 
X_t & = & \mu_X + \phi (X_{t-1} - \mu_X) + w_t \\ 
w_t & \sim & Norm(\mu_w,\sigma^2).
\end{array}
$$
We included a changepoint by shifting $\mu_X$ at time $\tau$ given a specific shift size ($\delta$).

We explored how each of these model parameters affected our ability to detect a change point. We simulate an entire time series (the "true" data for comparison), but we also examine the effect of not sampling every interval. We specifically study how the amount of samples included and the type of sampling affects the detection probability. For simulations, statistical power is the fraction of simulations that were able to detect a changepoint. We define a detection as an estimate within five time points (over 100 time points) of the true changepoint. We define $T_{min}$ as the minimum number of samples required to achieve 0.8 statistical power.


```{r theoretical_results_old,message=F,warning=F,echo=F,eval=F}

source('R/sampling_regular.R')
source('R/sampling_random.R')
source('R/build_time_series.R')
source('R/updated_param_sensitivity.R')

require(tidyverse)

par(mfrow=c(2,2))
# Sigma plot
sigma15 <- param_sensitivity('sigma',15)
plot(sigma15,ylab='Probability correct',xlab='sigma',pch=16,cex.lab=1.3,las=1,ylim=c(0,1),col=1)

sigma20 <- param_sensitivity('sigma',20)
points(sigma20,ylab='Probability correct',xlab='sigma',pch=15,cex.lab=1.3,las=1,ylim=c(0,1),col=3)

# phi plot
phi15 <- param_sensitivity('phi',15)
plot(phi15,ylab='Probability correct',xlab='phi',pch=16,cex.lab=1.3,las=1,ylim=c(0,1),col=1)

phi20 <- param_sensitivity('phi',20)
points(phi20,ylab='Probability correct',xlab='phi',pch=15,cex.lab=1.3,las=1,ylim=c(0,1),col=3)

```


```{r simulation_min_time,message=F,warning=F,echo=F,eval=F}

source('R/sampling_regular.R')
source('R/sampling_random.R')
source('R/build_time_series.R')
source('R/updated_param_sensitivity.R')
source('R/calc_min_time2.R')
require(tidyverse)

# Sigma results
sigma_vec = seq(0.1,30,4)
min_time  = rep(0,times=length(sigma_vec))
sigma_output <- data.frame(sigma_vec,min_time)

for (param_num in 1:nrow(sigma_output)){
  sigma_output$min_time[param_num] <- calc_min_time('sigma',sigma_output$sigma_vec[param_num])
  print(sigma_output)
}

write.csv(x = sigma_output,file = 'Output/theoretical_results_sigma.csv',quote=FALSE)

# Phi results

phi_vec = seq(-0.8,0.8,0.2)
min_time  = rep(0,times=length(phi_vec))
phi_output <- data.frame(phi_vec,min_time)

for (param_num in 1:nrow(phi_output)){
  phi_output$min_time[param_num] <- calc_min_time('phi',phi_output$phi_vec[param_num])
  print(phi_output)
}

write.csv(x = phi_output,file = 'Output/theoretical_results_phi.csv',quote=FALSE)


# Shift size

shift_size_vec = rev(seq(0.1,0.9,0.025))
min_time  = rep(0,times=length(shift_size_vec))
shift_size_output <- data.frame(shift_size_vec,min_time)

for (param_num in 1:nrow(shift_size_output)){
  shift_size_output$min_time[param_num] <- calc_min_time('shift_size',shift_size_output$shift_size_vec[param_num])
  print(shift_size_output)
}

write.csv(x = shift_size_output,file = 'Output/theoretical_results_shift_size.csv',quote=FALSE)


```


```{r read case study data and calculate some parameters to plot them on theoritical plot, message=FALSE, warning=FALSE, include=FALSE}
getpath4data <- function() {
  if(Sys.getenv("USER")=="Rosalie") return("/Volumes/-/Script R/Data-off-GitHub/temporal-sampling/")
  if(Sys.getenv("USER")=="eastonwhite") return("~/Desktop/Research/soil-temporal-sampling/")
  if(Sys.getenv("USER")!="Rosalie"|Sys.getenv("USER")!="put here your USER") stop("You need to get the data and define their location.")
}
# Read data
vr <- read.delim(paste0(getpath4data(),"VAR10-10-clado.txt"))

# Detect 'true' changepoint
vrall <- sample_regular(vr, n=nrow(vr), input_vector = F, xcol = 1, DCA_axis = 1, messages = F)
vrall$matrix$age <- vr$AGE[vrall$matrix$index]

# Calculate the 3 time-series parameter needed for comparison to the theoritical results

# 1 - Calculate sigma
sd_vr <- sd(vrall$matrix$x)

# 2 - Calculate autocorrelation of real time serie
ar_vr <- acf(vrall$matrix$x, lag.max = 5, plot = FALSE)
  
# 3 - Calculate shift size
shift_vr <- vrall$matrix$x
# Normalize
shift_vr <- (shift_vr-min(shift_vr))/(max(shift_vr)-min(shift_vr))
# Difference between the two values before/after shift
shift_vr <- abs(shift_vr[vrall$changepoint]-shift_vr[vrall$changepoint-1])

```


\rosalie{I wonder whether we should show the results with random sampling for Fig. \ref{fig:simulation_min_time}: we show later that it is the worst case scenario. But maybe that's a discussion we already had and I am missing on something.

I calculated the autocorrelation at lag-1 for the case_study time series and add it to the plot here.}

```{r simulation_min_time_plot, fig.width = 7.2, fig.height= 3, fig.cap='Minimum number of samples required for 0.8 statistical power given different levels of (a) temporal variability ($\\sigma$), (b) temporal autocorrelation ($\\phi$), and (c) shift size. Random sampling was used with the default parameters are $\\sigma = 3$, $\\phi = 0.5$, and a shift size $=0.75$. The exact timing of the true changepoint varied for each simulation between time steps 30 and 70. Vertical line indicate respective parameter calculated from the case study time series. \\label{fig:simulation_min_time}',message=F,warning=F,echo=F,eval=T}

par(mfrow=c(1,3))

sigma <- read.csv('Output/theoretical_results_sigma.csv',header=TRUE)
#plot(sigma$sigma_vec,sigma$min_time,ylab='Minimum number of samples required',xlab='sigma',pch=16,cex.lab=1.3,las=1,ylim=c(20,50),col=1,type='o')

p1 <- ggplot(sigma, aes(sigma_vec, min_time)) +
  geom_point() +
  geom_line() +
  geom_vline(xintercept = ar_vr$acf[2], lty=2) +
  ylim(20,50) +
  labs(x="sigma", y= "Minimum number of samples required") + 
  theme_bw() +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())




# Phi results

phi <- read.csv('Output/theoretical_results_phi.csv',header=TRUE)
#plot(phi$phi_vec,phi$min_time,ylab=' ',xlab='phi',pch=16,cex.lab=1.3,las=1,ylim=c(20,50),col=1,type='o')

p2 <- ggplot(phi, aes(phi_vec, min_time)) +
  geom_point() +
  geom_line() +
  geom_vline(xintercept = sd_vr, lty=2) +
  ylim(20,50) +
  labs(x="phi", y= "") + 
  theme_bw() +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())

# Shift size results

shift_size <- read.csv('Output/theoretical_results_shift_size.csv',header=TRUE)
#plot(shift_size$shift_size_vec,shift_size$min_time,ylab=' ',xlab='shift size',pch=16,cex.lab=1.3,las=1,ylim=c(20,50),col=1,type='o')

p3 <- ggplot(shift_size, aes(shift_size_vec, min_time)) +
  geom_point() +
  geom_line() + 
  geom_vline(xintercept = shift_vr, lty=2) +
  ylim(20,50) +
  labs(x="phi", y= "") + 
  theme_bw() +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())
  
  
(p1 | p2 | p3)

```



## Theoretical results 

In line with theory about monitoring, we found that the probability of correcting identifying a changepoint decreased with smaller levels of population variability ($\sigma$) and autocorrelation ($\phi$). We also found that the probability of correct changepoint detection increases with larger shift sizes, which is essentially the effect size. Thus, the number of sample required to obtain high statistical power (above 0.8) increased with larger population variability, lower autocorrelation, and smaller shift sizes (Fig. \ref{fig:simulation_min_time}).


```{r simulation_results_sampling_approaches,message=F,warning=F,echo=F,eval=F}

source('R/sampling_regular.R')
source('R/sampling_random.R')
source('R/sampling_iterative.R')
source('R/build_time_series.R')
source('R/updated_param_sensitivity.R')
source('R/calc_min_time2.R')
require(tidyverse)

# Need to run each method on a set of simulated time series. We would then have 100 runs for each method and each sampling amount

approach <- c('Random','Regular','Iterative')
number_samples <- c(seq(5,20,2),25,30,35)
simulation_number <- 1:20
output_theory_approach <- data.frame(expand.grid(approach,number_samples,simulation_number))
names(output_theory_approach) <- c('approach', 'number_samples', 'simulation_number')
output_theory_approach$distance_to_real = 0

# Build function that takes in approach, simulated time series, and number of samples. The function should then spit out the distance to the real changepoint

# Set of time series (need other params)
params <- tibble(shift_time = sample(30:70,max(simulation_number),replace = T))

time_series <- params %>%
  pmap(build_time_series) 

# Run through params

for (i in 1:nrow(output_theory_approach)){
  
  output_theory_approach$distance_to_real[i] <- abs(params$shift_time[output_theory_approach$simulation_number[i]] - calc_distance_to_real(output_theory_approach$approach[i],output_theory_approach$number_samples[i],time_series[[output_theory_approach$simulation_number[i]]]))
  print(i)
}


write.csv(x = output_theory_approach,file = 'Output/theoretical_comparing_approaches.csv',quote = FALSE )
```


```{r simulation_approaches_graph, echo=FALSE, message=FALSE, warning=FALSE, fig.width = 7.2, fig.height= 3,fig.cap='Distance to real changepoint per total number of samples analyzed for simulations, following (a) random sampling, (b) regular sampling, and (c) iterative sampling. Each sample size and approach combination was simulated 50 times with the same parameters as Figure \\ref{fig:simulation_min_time}.\\label{fig:simulation_number_sample}'}

myoutput <- read.csv(file='Output/theoretical_comparing_approaches.csv',header=T)
myoutput$approach <- factor(myoutput$approach,levels = c('Random', 'Regular', 'Iterative'))
myoutput_summary <- myoutput %>%
  group_by(approach,number_samples) %>%
  summarize(mean_distance = mean(distance_to_real),sd_distance=sd(distance_to_real))
  
p1  <- ggplot(myoutput_summary, aes(x=number_samples,y=mean_distance)) + geom_line()  + geom_ribbon(aes(ymin = mean_distance - 1.96*sd_distance, ymax = mean_distance + 1.96*sd_distance), fill = "grey80") +geom_line(size=1) + coord_cartesian(ylim = c(2.2, 50))  + 
  facet_wrap(~approach,
             labeller = label_glue('({.l}) {approach} sampling ')) + 
  theme_bw() +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  labs(x="Total number of samples analyzed", y="Distance to real changepoint") #+
 # theme(legend.position = c(0.93, 0.72),
#       legend.background = element_rect(fill = "#ffffffaa", colour = NA),
#        legend.title=element_text(size=7), 
 #       legend.text=element_text(size=8)) +
#  guides(pch=guide_legend(title="Initial number\nof sample"))

p1


```





# Case study

## Case study background

As one application of our approach, we examined paleosequence data. To obtain this data sediment cores are taken from aquatic environments to reconstruct past ecosystems. Samples can be taken before and after perturbation to compare community composition (e.g., ), or at continuously to identify the timing of change, and contribution of environmental variable in driving these changes (e.g., @Perga2015).

We tested different sampling methods on a real community time-series, from Lake Varese (IT). Lake Varese is a small (14.8 km^2^), deep (z$_{max}$= 26m), monomictic lake, in the subalpine region of north-western Italy (238 m asl). It underwent hyper-eutrophication over the 20^th^ century due to increase in nutrient loads from the watershed (ref). Nutrient status was responsible for restructuration of the lake communities across trophic levels [@Crosta1999,@Bruel2018]. Air temperature is now driving changes in plankton communities (@Bruel2018).

In a previous study, Cladoceran assemblage was reconstructed at a 2-3 years resolution, from a 74-cm sediment core covering the 1816(±26)—2010 time period [@Bruel2018]. We used the same ordination method as Bruel et al (2018) to reduce the dimensionality of the assemblage-level data. As a result, we detected several changepoints on the first component (in 1926/1928, 1946/1948 and 1983/1988). We defined these as the "true" changepoints given they came from an analysis of the complete data. The 1983-1988 changepoint was the largest in magnitude. 

In line with our simulation approach, we subsampled the full record (74 observations) using the three methods described earlier (random, regular, iterative). For each of the three methods, we examined the error (difference between the true changepoint and the detected changepoint) when using different numbers of samples.



## Case study results

We found that random sampling performed the worst, as changepoint analysis was left to chance. Regular sampling provided good estimates from 8 samples (0-3 sample of absolute error), but detecting the true changepoint depended on the interval falling close to the true changepoint (i.e., left to chance). Iterative sampling, where 5 samples are initially sampled, then other samples are slowly added, performed the best, as no more than 9 samples were ever necessary to get the true changepoint (Fig. \ref{fig:case_study_number_sample} (c)). We show how iterative sampling changes the ordination, and the resulting changepoint, as more samples are added (Fig. \ref{fig:case_study_iterative_sampling}) <!--Note that when the initial number of samples was set to 7 Fig. \ref{fig:case_study_number_sample} (c), square markers), the distance to the real changepoint was initially very high. Without any prior, the initial number sampled fell far away from the true changepoint.-->

We tested how the three methods performed at detecting other changepoints of lower magnitude (as three changepoints were detected in the initial study, @Bruel2018). Iterative sampling still performed best, especially if an higher number of initial samples was chosen (7 samples out of 74, Supplementary Materials \textcolor{red}{2}).
<!--This could probably be moved to the discussion: The initial number of samples should be chosen depending on the objective of the study. If investigators want to find the biggest transition only, then a low number might be enough (include here results from theoretical results?). If investigators suspect or want to test for several changepoints, we recommend analyzing initially about 10% of the length of the time series to have a good initial ranking.-->



```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width = 7.2, fig.height= 3,fig.cap='WHY ARE THERE NEGATIVE VALUES HERE? Distance to real changepoint per total number of samples analyzed, following (a) random sampling, (b) regular sampling, and (c) iterative sampling. Total number of samples was set between 5 and 30, out of the 74 initial time series.  \\label{fig:case_study_number_sample}'}
# Read data
myoutput <- read.table(paste0(getwd(),"/Output/output_changepoint_varese.txt"))
myoutput$method <- factor(myoutput$method, levels=c('Random', 'Regular', 'Iterative'))
myoutput$diff_real <- abs(myoutput$diff_real)

# Change to TRUE if we decide it is worth it to show several initial number of n2. It looks more confusing -- but it would bring more information
several_n2_init = F

# Plot
p1 <- if(several_n2_init) {
  ggplot(myoutput[myoutput$target_cpt==1,], aes(final_n,diff_real, pch=factor(n2_init))) + 
  geom_hline(aes(yintercept=0), col=adjustcolor("black", alpha.f = .4)) + 
  geom_point(alpha = .3) +
  facet_wrap(~method,
             labeller = label_glue('({.l}) {method} sampling ')) + 
  theme_bw() +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  labs(x="Total number of samples analyzed", y="Distance to real changepoint") +
  theme(legend.position = c(0.93, 0.72),
        legend.background = element_rect(fill = "#ffffffaa", colour = NA),
        legend.title=element_text(size=7), 
        legend.text=element_text(size=8)) +
  guides(pch=guide_legend(title="Initial number\nof sample"))
} else {
  ggplot(myoutput[myoutput$target_cpt==1 & myoutput$n2_init == 5,], aes(final_n,diff_real)) + 
  geom_hline(aes(yintercept=0), col=adjustcolor("black", alpha.f = .4)) + 
  geom_point(alpha = .3) +
  facet_wrap(~method,
             labeller = label_glue('({.l}) {method} sampling ')) + 
  theme_bw() +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  labs(x="Total number of samples analyzed", y="Distance to real changepoint") 
}

p1


```


```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width = 8, fig.height= 7.5, fig.cap='Iterative sampling and impact on ordination. (a) Initial community dataset. Samples are initially sampled at regular interval. The multivariate data is converted to univariate vector by Detrended Component Analysis. Samples were iteratively added, after computation of changepoint at each step. (b) Full time series, on which "true" changepoint was detected. (c) First step of iterative sampling, n = 5. (d) Final step of iterative samples: by adding 4 samples, the true changepoint was detected. \\label{fig:case_study_iterative_sampling}'}

# Creating a dataset with all scores 
vr_out_matrix <- read.table(paste0(getwd(),"/Output/output_iterative_sampling_varese.txt"))

index_temp <- vr_out_matrix$index[vr_out_matrix$n==max(vr_out_matrix$n)]
vr_out_order <-
  data.frame("index" = index_temp,
             "age"   = vrall$matrix$age[index_temp],
             "x"     = vrall$matrix$x[index_temp],
             "count" = summary(as.factor(vr_out_matrix$index)))
vr_out_order$count <- abs(vr_out_order$count-max(vr_out_order$count)-1)



mypal <- wes_palette("GrandBudapest2", max(vr_out_order$count),type = "continuous")
 
# Plot with all data
 p_init <- ggplot(vrall$matrix, aes(age, x)) + geom_point(alpha = .3) + geom_line(alpha = .3) +
  theme_bw()  +
      theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) + 
           geom_rect(mapping=aes(xmin=vrall$matrix$age[which(vrall$matrix$index==vrall$changepoint)-1], xmax=vrall$matrix$age[which(vrall$matrix$index==vrall$changepoint)], ymin=-1, ymax=2), alpha=0.05, fill="grey") +
   labs(subtitle = paste0("b. Full time-series, n = ", nrow(vr)), x="Year", y="DCA first component",
        color='samples added\nat iteration t=') +
   geom_point(vr_out_order, mapping=aes(age, x, color = as.factor(count)), cex=2) +
   geom_text(vr_out_order, mapping=aes(age, x, label =count), vjust = 1.5, nudge_x = 1.5) +
   scale_color_manual(values=mypal) 



 
# Initial plot, then Adding samples
for (i in 5:12) {
  # Start a new loop only if the final changepoint hasn't been found at the previous iteration.  
  if (i==5 || vrsub$matrix$index[vrsub$matrix$index==vrsub$changepoint] - vrsub$matrix$index[which(vrsub$matrix$index==vrsub$changepoint)-1]>1) {
    
  vrsub <- sample_iterative(vr, n=i, n2=5, input_vector = F, xcol = 1, DCA_axis = 1, messages = F)
  vrsub$matrix$age <- vr$AGE[vrsub$matrix$index]
  vrsub$matrix$count <- vr_out_order$count[vr_out_order$age %in% vrsub$matrix$age]
  
    if (i==5) {
      vrsub1 <- vrsub
    p1 <- ggplot(vrsub$matrix, aes(age, x, color=as.factor(count))) +  geom_line(alpha = .4, col="grey") + geom_point() + guides(col=FALSE) +
   scale_color_manual(values=mypal) +
  theme_bw() +
      theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
   labs(x="Year", y="DCA first component",
        color='samples added\nat iteration t=')
    p2 <- p1
  } else {
    if (i == 6) {
      p2 <- p1 + 
        geom_line(vrsub$matrix, mapping=aes(age, x), alpha = .4, col="grey") + 
        geom_point(vrsub$matrix, mapping=aes(age, x,color=as.factor(count)))   + guides(col=FALSE) 
    } else {
        p2 <- p2 + geom_line(vrsub$matrix, mapping=aes(age, x), alpha = .4, col="grey") + 
          geom_point(vrsub$matrix, mapping=aes(age, x,color=as.factor(count))) + 
          guides(col=FALSE)
      }
  }
      # Save plot
    ggsave(p2 + 
             geom_rect(stat = "identity", mapping=aes(xmin=vrsub$matrix$age[which(vrsub$matrix$index==vrsub$changepoint)-1], xmax=vrsub$matrix$age[which(vrsub$matrix$index==vrsub$changepoint)], ymin=-1, ymax=3), col=NA, fill="grey", alpha=0.05) +
             annotate("text", label = paste0("Number of samples = ",i), x = min(vrsub$matrix$age), y = 2.8, colour = "black",  hjust=0,vjust=0), 
           filename = paste0("Output/GIF_adding_sample_case_study/Iterative_",i,".pdf"), width = 6, height = 4)

  }
}

p1 <- p1 + geom_line(vrsub1$matrix, mapping=aes(age, x), alpha = .8, col="black") + 
          geom_point(vrsub1$matrix, mapping=aes(age, x,color=as.factor(count))) + geom_rect(stat = "identity", mapping=aes(xmin=vrsub1$matrix$age[which(vrsub1$matrix$index==vrsub1$changepoint)-1], xmax=vrsub1$matrix$age[which(vrsub1$matrix$index==vrsub1$changepoint)], ymin=-1, ymax=3), col=NA, fill="grey", alpha=0.05) +
  labs(subtitle="c. Initial number of samples, n = 5")

p2 <- p2 + geom_line(vrsub$matrix, mapping=aes(age, x), alpha = .8, col="black") + 
          geom_point(vrsub$matrix, mapping=aes(age, x,color=as.factor(count))) + 
  geom_rect(stat = "identity", mapping=aes(xmin=vrsub$matrix$age[which(vrsub$matrix$index==vrsub$changepoint)-1], xmax=vrsub$matrix$age[which(vrsub$matrix$index==vrsub$changepoint)], ymin=-1, ymax=3), col=NA, fill="grey", alpha=0.05) +
  labs(subtitle=paste0("d. n =", nrow(vrsub$matrix)))


# Fake dataset to explain sample selection
fakedata <- data.frame(matrix(rep(NA,74*10), nrow=74))
colnames(fakedata) <- 1:ncol(fakedata)
fakedata[is.na(fakedata)] <- 0
fakedata[vr_out_order$index[vr_out_order$count==1],] <- 1 
fakedata[vr_out_order$index[vr_out_order$count==2],] <- 2 
fakedata[vr_out_order$index[vr_out_order$count==3],] <- 3 
fakedata[vr_out_order$index[vr_out_order$count==4],] <- 4 
fakedata[vr_out_order$index[vr_out_order$count==5],] <- 5 
fakedata[,1] <- 6
fakedata <- rbind(rep(7,ncol(fakedata)),fakedata)
fakedata <- cbind(Year=rep(75:1,ncol(fakedata)), melt(fakedata))
head(fakedata)

p3 <- ggplot(fakedata, aes(variable,Year)) +
  geom_tile(aes(fill = as.factor(value)), colour="black") +
  scale_fill_manual(values=c("white",mypal,grey(.8), grey(.8))) + guides(fill=FALSE)  + theme(panel.border = element_blank(), panel.grid.major = element_blank(),
panel.grid.minor = element_blank(), axis.line = element_blank(), panel.background = element_blank(), axis.title=element_blank(),
        axis.text=element_blank(),
        axis.ticks=element_blank()) +
  geom_text(fakedata[fakedata$Year==75,], mapping = aes(variable,Year, label=c("Year", paste("Sp.", 1:9))),  size=2.5,hjust=0,angle = 90) +
  labs(subtitle="a. Continuous community dataset")

# Assemble plot

 p3 | (p_init / p1 / p2)


```




# General Discussion

Outline:

- theoretical results
- example with the case study
- how this fits in with other work
- limitations and future work


Analyzing a subsample of the whole time-series inevitably leads to a lesser understanding of the phenomenon observed [@White2019d]. However, in some cases, analyzing the whole sample collected is not an option. We show here that an informed subsampling can still allow detection of critical information, such as time series changepoint.



Other dataset that could use our method, e.g., paleo science in general? see review by Wingard et al 2017 https://www.frontiersin.org/articles/10.3389/fevo.2017.00011/full#B140

Case study: With the iterative method, we found the main changepoint with maximum 9 samples analyzed, with a starting point of 5 randomly distributed samples. Same results for the major change, with only 12% of the samples analyzed. However, need more samples to detect further changes. __Note that it would be possible to edit the function to go ahead and detect next changes once the main one has been detected. Is it worth coding? Shouldn't be too hard to do__

@Bruel2018 processed one sample at each centimeter in a 74-centimeter sediment core. Each sample took an average of 3 hours to process. We found that using an interval approach would have eliminated `r 3*(74-9)` hours of sample processing, or about `r round(3*(74-9)/8)` days, which is just a little over a month of work. This correspond to several thousands of US dollars depending on labor costs.


Note that the number of sample needed varies depending on the characteristics of the time series. For perspective, the autocorrelation at lag-1 in the first DCA component we used in the case study example is `r round(ar_vr$acf[2],3)`

Our approach goes beyond just paleoecological analyses. Running simulations or using past data to understand the amount of sampling effort required is important in many systems where sample collection or processing is expensive (citations). The specific sampling techniques can also be compared to determine the optimal strategy in terms of accuracy and cost. Our specific approach in this paper was on data where more samples can be added long after the dynamics occurred. Our interval sampling approach involved taking a fixed number of regularly-spaced samples to start and then adding more samples strategically to detect a changepoint. Suppose instead that the goal was to detect a change in observed individuals with video-based approaches. It is often not practical to watch entire videos, so it can be useful to choose strategic time-points that would address our question of interest. Using an interval sampling approach, one could take a fixed number of samples to start. The trend over time from simple linear regression could be taken. Then, samples can be taken at random locations one-by-one and to see which samples have the largest effect on the trend estimates. If a particular sample has a large effect on the trend, then it would be best to choose another nearby sample. Sampling would continue until the trend estimate began to settle down. Thus, the iterative sampling approach is particularly relevant to data sources where additional samples can be taken long after the initial dynamics. These approaches are also applicable to other questions related to the optimal sample size and approach in determining long-term trends, biodiversity estimates, and more (citations). 

<!--Limitations paragraph-->

Although our approach is flexible to be applied to various data sources and questions, this is also one of its limitations. The specific number of samples and optimal sampling approach will largely be system specific. A similar system may be used to get some sense for the best sampling scheme, but it will depend on the VARIOUS FACTORS HERE. Similarly, selecting a set number of samples or specific approach may also limit what future questions can be asked. However, more samples could be added to address these additional questions.

- It is not limited to paleo data
- Depends on the question being asked
- Specific number of samples depends on the system dynamics

Systematic sampling of a time-series is required to detect critical slowing-down or early warning of shifts (ref, maybe among others Frossard et al 2015, Doncaster et al 2016). The lower number of input data prior to ordination did not impact the finding of the main transition. However, if other metrics are of interest to the investigators, such as species richness, a more systematic or at least more extensive approach would be require.


Many ecological processes are non linear (e.g., D'Amario et al 2019, Scientific reports). Detecting a changepoint on one aspect of the ecosystem does not guarantee that the sampling resolution is appropriate to detect the driver for the changepoint.

Concluding paragraph:

About recommendation to reduce the frequency of data collected as part of monitoring program:  
While long-term monitoring program are often expensive, these cost are nothing compared to the value of the resources monitored and the policies affecting these resources (@Lovett2007). Monitoring programs should try to anticipate the potential questions of tomorrow, and reducing the data collected must be done with the best foresight possible on how these data may indeed not be necessary to manage ecosystems in the future.  

\clearpage

# References