---
bibliography: Whitebib.bib
csl: ecology-letters.csl
editor_options:
  chunk_output_type: console
fontsize: 12pt
geometry: margin=1in
header-includes: \usepackage{float} \usepackage{lineno} \usepackage{setspace}\doublespacing
  \usepackage[round]{natbib} \bibpunct[; ]{(}{)}{,}{a}{}{,} \usepackage{color} \usepackage{totcount}
  \newtotcounter{citenum} \def\oldcite{} \let\oldcite=\bibcite \def\bibcite{\stepcounter{citenum}\oldcite}
output:
  pdf_document:
    fig_caption: yes
    keep_tex: yes
    number_sections: yes
---

```{r setup, include=FALSE}
knitr::opts_knit$set(root.dir = dirname(getwd()))
```

Title: Sampling requirements to detect ecosystem change
	
\vspace{7 mm}
	
Authors: Rosalie Bruel and Easton R. White
	
\vspace{5 mm}
	
Addresses: \normalsize{\indent $^{1}$Department of Biology, University of Vermont, VT 05405, USA}

\vspace{5 mm}
Keywords: {time-series} \vspace{10 mm}
	
\vspace{5 mm}

	
Number of words: XXXX \vspace{5 mm}
	
Number of references: 
	
Number of figures and panels: X
	
\vspace{5 mm}
	
Submission as: 
	
\vspace{5 mm}
	
Preprint available at: 


\clearpage


\begin{center}
\textbf{\Large Title here}
\vspace{7 mm}
	
Authors: \textsc{Rosalie Bruel and Easton R. White}
\vspace{5 mm}

Addresses: {\indent $^{1}$Department of Biology, University of Vermont, VT 05405, USA}

\end{center}

\textbf{Abstract}

Abstract here

\vspace{3 mm}

Keywords: {time series}






# Things to do:

- writing intro
- writing methods/results pair of section
- writing discussion
- setting up bibliography system
- Possible journals: Methods Ecology and Evolution, Biological Conservation, *Journal of Environmental Management*, *Environmental Monitoring and Assessment*, BioScience 

# Main messages of manuscript

- We can choose the number of samples and effort we use...
- Number of samples required depends on question asked and the time series parameters
- Case study of paleo data shows the utility of iterative sampling in detecting change points

# Introduction and setup


Environmental monitoring is one of the core components to modern research and management (citations). Within an adaptive management framework, monitoring is needed for both learning about the system under study and assessing the effectiveness of management interventions (citations). Increasingly, long-term monitoring programs, like the Long Term Ecological Research (LTER) Network in the USA, are becoming available (Magurran2010). However, environmental monitoring is still often expensive and time-consuming (citations). Therefore, monitoring programs need to be designed in such a way to address the question of interest. Key considerations of an optimal monitoring program include the frequency of surveys, the selection of sites, as well as the precision and accuracy of the monitoring itself (citation). Monitoring programs also need to be designed to be cost-effective given limited bugets (citations). 


Methods to dectect change overtime: see summary https://lindeloev.github.io/mcp/articles/packages.html

- Historical data and ability to choose how much to sample

Accounting for these considerations in the design of a monitoring program is necessary to detect long-term ecological change. The specifics of the monitoring program will determine the power with which a question of interest can be addressed. For example, @White2019 found that 72\% of vertebrate populations required at least 10 years of monitoring to detect significant changes in the population size over time. Other work has focused on the frequency of monitoring (citations), the allocation of resources spatially versus temporally (citations), and the ability to use citizen science to increase monitoring (citations). Lastly, both the ecological and economical costs of failing to detect a true trend (type II error) have to be weighed against the risks of false (type I error) detection (citation).

Because of new technological advances, there are many data sources that can be derived long after the actual  processes occurred. For example, paleoecological lake cores can be extracted and historical ecological communities can be reconstructured (citations from Rosalie). Similarly, environmental samples (e.g. water, soil) can be saved with eDNA processed later (citation). Similarly, video-graphical approaches can record snapshots of a system and be analyzed later (citations). In each of these cases, decisions have to made about how much data to extract from the previously collected samples. Should the paleoecological core be analyzed at every centimeter? Should the video be assessed at every minute? As long as processsing samples is expensive, these trade-offs will remain.

Here, we develop a set of tools to determine the appropriate number of samples and sampling approach when dealing with data sources where additional samples can be added. We focus on paleoecological core samples as one example of this type of data. We examine the situation where the goal is to detect the time at which a significant change in an ecological community occurs, a changepoint. However, our approaches are widely applicable to other questions and data types. We first investigate these tools using a simulated-based approach to As a case study, we apply these tools to the detection of change points for a set of lakes. (more from Rosalie here).

# Which sampling strategies are available?


- Random
- Regular
- Iterative

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=7, fig.height=7.5,eval=T,fig.cap='Conceptual figure for... \\label{fig:conceptual}'}
source(paste0(getwd(),"/R/Fig_conceptual.R"))

```


We hypothesize  that random sampling will perform the worst at getting as close as possible to a "real" changepoint. 

# Theoretical approach

- explain the modeling approach
- show the model
- explain the analysis we will do
- explain the theoretical model results with a figure
- explain effect of different sampling strategies in this context

Theoretical plots

- One combined plot to see the effect (on prob correct or number of samples) of each parameter - should do this with one of the three types of sampling (probably regular)
- Theoretical plot comparing the three different methods - we could choose one set of parameter values and show how the various methods perform with different sampling amounts (could use same plot structure as the case study)
- SM plot that shows how the three different methods are affected by changes in the underlying parameter values

## Theoretical model

We began with a theoretical exploration of the sampling requirements to detect a changepoint. We modeled a simple first order autoregressive (AR-1) process with a response variable that represents either population size, biodiversity, or some metric of community composition. The model includes temporal autocorrelation ($\phi$), the mean of the process ($\mu_X$), and a white noise term ($w_t$). The white noise term is a normal distribution with mean ($\mu_w$) and variance ($\sigma$).

$$
\begin{array}{rcl} 
X_t & = & \mu_X + \phi (X_{t-1} - \mu_X) + w_t \\ 
w_t & \sim & Norm(\mu_w,\sigma)
\end{array}
$$
We included a changepoint by shifting $\mu_X$ at time $\tau$ given a specific shift size ($\delta$).

We explored how each of these model parameters affected our ability to detect a change point. We simulate an entire time series (the "true" data for comparison), but we also examine the effect of not sampling every year. We specifically study how the amount of samples included and the type of sampling affects the detection probability. 


```{r theoretical_results, fig.cap='Theoretical results here detailing parameter sensitivity - we should make this about sample size instead of probability correct',message=F,warning=F,echo=F,eval=F}

source('R/sampling_regular.R')
source('R/build_time_series.R')
source('R/updated_param_sensitivity.R')

require(tidyverse)

par(mfrow=c(2,2))
# Sigma plot
sigma15 <- param_sensitivity('sigma',15)
plot(sigma15,ylab='Probability correct',xlab='sigma',pch=16,cex.lab=1.3,las=1,ylim=c(0,1),col=1)

sigma20 <- param_sensitivity('sigma',20)
points(sigma20,ylab='Probability correct',xlab='sigma',pch=15,cex.lab=1.3,las=1,ylim=c(0,1),col=3)

# phi plot
phi15 <- param_sensitivity('phi',15)
plot(phi15,ylab='Probability correct',xlab='phi',pch=16,cex.lab=1.3,las=1,ylim=c(0,1),col=1)

phi20 <- param_sensitivity('phi',20)
points(phi20,ylab='Probability correct',xlab='phi',pch=15,cex.lab=1.3,las=1,ylim=c(0,1),col=3)

```



## Theoretical results 

In line with theory about monitoring, we found that the probability of correcting identifying a changepoint decreased with larger levels of population variability ($\sigma$) and autocorrelation ($\phi$). We also found that the probability of correct changepoint detection increases with larger shift sizes. 

The probability of correcting identifying a changepoint, of course, depends strongly on the number of samples. 


# Case study

We tested the three sampling methods on a real community time-series, from Lake Varese (IT). Lake Varese is a small (14.8 km^2^), deep (z<sub>max</sub>= 26m), monomictic lake, in the subalpine region of north-western Italy (238m asl). It underwent hyper-eutrophication over the 20^th^ century due to increase in nutrient loads from the watershed (ref). Nutrient status was responsible for restructuration of the lake communities across trophic levels (e.g., Ceccuzi et al 2011, Bruel et al, 2018). Air temperature is now driving changes in plankton communities (Bruel et al, 2018).

Data are reconstructed cladoceran assemblage from a 74-cm sediment core (VAR10-10) covering the 1816(±26)—2010 period. Half of VAR 10.10 was sliced at a 1-cm interval and was used to count cladoceran remains (Bruel et al, 2018). We defined the "true" changepoint as the one identified from the complete record, in 1983/1988. 
Changepoints were detected on first component computed from the data sub-sampled. We used the same ordination method as in Bruel et al (2018), but note that DCA is best used to conserve multiple gradients. As a result, several changepoints were detected on the first component (in 1926/1928, 1946/1948 and 1983/1988). We conducted the analysis on the second component as well, where only one changepoint was detected (in 1944/1946).

We used the full record (74 observation) and subsampled following the three strategies described earlier (random, regular, iterative), varrying the maximum number analyzed from 5 to 15. Iterative sampling final number could be lower than maximum number if both samples framing the shift were analyzed.

Random sampling perform the worst, as changepoint analysis was left to chance. Regular sampling provided good estimates from 8 samples (0-2 sample of absolute error). Iterative sample, where 5 samples are initially sampled, then other samples are slowly added, performed the best, as no more than 9 samples were ever necessary to get the true changepoint

```{r case study read data output, include=FALSE}
myoutput <- read.table(paste0(getwd(),"/Output/output_changepoint_varese.txt"))
myoutput$method_f <- factor(myoutput$method, levels=c('Random', 'Regular', 'Iterative'))

```


```{r echo=FALSE, message=FALSE, warning=FALSE}
library(ggplot2)
library(gridExtra)
p1 <- ggplot(myoutput, aes(final_n,diff_real)) + 
  geom_point(alpha = .3) +
  facet_wrap(~method_f) + theme_bw() +
  labs(title="a",x="", y="Distance to real changepoint")
p2 <- ggplot(myoutput, aes(rep(0, nrow(myoutput)),diff_real))  +
  facet_wrap(~method_f) + 
  geom_violin() +
  geom_dotplot(binaxis='y', stackdir='center', dotsize=.3) + 
  theme(axis.text.x=element_blank(),
        axis.ticks.x=element_blank()) + theme_bw() +
  labs(title="b",x="", y="Distance to real changepoint")
grid.arrange(p1, p2, nrow = 2)
```

__Main point__: Changepoint was detected even though we used a transformed vector (from community to first component of DCA) 

# Discussion

Case study: With the iterative method, we found the main changepoint with maximum 9 samples analyzed, with a starting point of 5 randomly distributed samples. Same results for the major change, with only 12% of the samples analyzed. However, need more samples to detect further changes. __Note that it would be possible to edit the function to go ahead and detect next changes once the main one has been detected. Is it worth coding? Shouldn't be too hard to do__

\clearpage

Limits: Systematic sampling of a time-series is required to detect critical slowing-down or early warning of shifts (ref, maybe among others Frossard et al 2015, Doncaster et al 2016). The lower number of input data prior to ordination did not impact the finding of the main transition. However, if other metrics are of interest to the investigators, such as species richness, a more systematic or at least more extensive approach would be require.


# References