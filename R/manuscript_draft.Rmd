---
title: "Sampling requirements to detect ecosystem change"
author: "Rosalie Bruel and Easton White"
date: "03/10/2019"
output:
  html_document:
    df_print: paged
    fig_caption: yes
    number_sections: yes
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
fontsize: 11pt
editor_options:
  chunk_output_type: console
  df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_knit$set(root.dir = dirname(getwd()))
```


# Things to do:

- writing intro
- writing methods/results pair of section
- writing dicussion
- setting up bibliography system
- Possible journals: Methods Ecology and Evolution, Biological Conservation, *Journal of Environmental Management*, *Environmental Monitoring and Assessment*, BioScience 

# Main messages of manuscript

- We can choose the number of samples and effort we use...
- Number of samples required depends on question asked and the time series parameters
- Case study of paleo data shows the utility of iterative sampling in detecting change points

# Introduction and setup

- Set up ideas about monitoring and cost of monitoring

Environmental monitoring is one of the core components to modern research and management (citations). However, many monitoring programs are expensive and time-consuming (citations). Therefore, an optimal...

- Idea of detecting change over time

One of the primary goals of monitoring programs is to detect long-term ecological change...

- Historical data and ability to choose how much to sample

Because of new technological advances, there are many data sources that can be derived long after the actual  processes occured. For example, paleoecological lake cores can be extracted and ecological communities can be built from this... Similarly, samples for eDNA or videographical approaches can record time points of a system and be analyzed later (citations). In each of these cases, decisions have to made about how much data to extract from the previously collected samples. Should the paleo core be analyzed at every centimeter? Should the video be assessed at every minute? As long as analyzing samples is expensive, these tradeoffs will remain.

Here, we develop a set of tools to determine the appropriate number of samples and sampling approach when dealing with data sources where additional samples can be added. We first investigate these tools using a simulated-based apporach. We then apply these tools to the detection of change points for a set of lakes...

# Which sampling strategies are available?

- Random
- Regular
- Iterative

<img src="Fig_conceptual.pdf" alt="conceptual figure"  width="4200" height="6200">

# Theoretical results

- explain the modeling approach
- show the model
- explain the analysis we will do
- explain the theoretical model results with a figure
- explain effect of different sampling strategies in this context

We began with a theoretical exploration of the determiners of sampling required to detect a changepoint. We modeled a simple AR-1 process with a response variable that represents either population size, biodiversity, or some metric of community composition. The model includes temporal variability ($\sigma$), autocorrelation ($\phi$), the size of the shift ($\delta$), and a specific time when the shift occured ($\tau$):

$$
\mbox{Model here - need pair of equations}
$$

We then explored how each of these model components affected our ability to detect a change point. We simulate an entire time series (the "true" data for comparison), but we also examine the effect of not sampling every year. We specifically study how the amount of samples included and the type of sampling affects the detection probability. 

In line with theory about monitoring, we found that the probability of correcting identifying a changepoint decreased with larger levels of population variability ($\\sigma$) and autocorrelation ($\\phi$). We also found that the probability of correct changepoint detection increases with larger shift sizes. 

The probability of correcting identifying a changepoint, of course, depends strongly on the number of samples. 


# Lake case study

```{r case study read data output, include=FALSE}
myoutput <- read.table(paste0(getwd(),"/Output/output_changepoint_varese.txt"))
myoutput$method_f <- factor(myoutput$method, levels=c('Random', 'Regular', 'Iterative'))

```

With the iterative method, we found the main changepoint with maximum 9 samples analyzed, with a starting point of 5 randomly distributed samples.

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(ggplot2)
library(gridExtra)
p1 <- ggplot(myoutput, aes(final_n,diff_real)) + 
  geom_point(alpha = .3) +
  facet_wrap(~method_f) + theme_bw() +
  labs(title="a",x="", y="Distance to real changepoint")
p2 <- ggplot(myoutput, aes(rep(0, nrow(myoutput)),diff_real))  +
  facet_wrap(~method_f) + 
  geom_violin() +
  geom_dotplot(binaxis='y', stackdir='center', dotsize=.3) + 
  theme(axis.text.x=element_blank(),
        axis.ticks.x=element_blank()) + theme_bw() +
  labs(title="b",x="", y="Distance to real changepoint")
grid.arrange(p1, p2, nrow = 2)
```


# Discussion

# References