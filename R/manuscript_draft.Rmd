---
bibliography: Whitebib.bib
csl: ecology-letters.csl
editor_options:
  chunk_output_type: console
fontsize: 12pt
geometry: margin=1in
header-includes: \usepackage{float} \usepackage{lineno} \usepackage{setspace}\doublespacing
  \usepackage[round]{natbib} \bibpunct[; ]{(}{)}{,}{a}{}{,} \usepackage{color} \usepackage{totcount} \usepackage{soul}
  \newtotcounter{citenum} \def\oldcite{} \let\oldcite=\bibcite \def\bibcite{\stepcounter{citenum}\oldcite} \renewcommand{\thefootnote}{\fnsymbol{footnote}} \addtocounter{footnote}{-1} \newcommand\easton[1]{\textcolor{red}{#1}} \newcommand\rosalie[1]{\textcolor{blue}{#1}}
output:
  pdf_document:
    fig_caption: yes
    keep_tex: yes
    number_sections: no
---

```{r setup, include=FALSE}
knitr::opts_knit$set(root.dir = dirname(getwd()))
```

Title: Sampling requirements and approaches to detect ecosystem shifts
	
\vspace{7 mm}
	
Authors: Rosalie Bruel$^{1*}$ and Easton R. White$^{2*}$
	
\vspace{5 mm}
	
Addresses: $^{1}$Rubenstein Ecosystem Science Laboratory, University of Vermont, VT 05405, USA, https://orcid.org/0000-0001-8351-7539 $^{2}$Department of Biology, University of Vermont, VT 05405, USA, https://orcid.org/0000-0002-0768-9555
* Both authors contributed contributed equally and order was chosen alphabetically 

\vspace{5 mm}
Keywords: time-series, changepoint \vspace{10 mm}
	
\vspace{5 mm}

	
Number of words: XXXX \vspace{5 mm}
	
Number of references: \total{citenum} 
	
Number of figures and panels: X
	
\vspace{5 mm}
	
Submission as: Article at \emph{Journal of Applied Ecology}
	
\vspace{5 mm}
	
Preprint available at: 


\clearpage


\begin{center}
\textbf{\Large Sampling requirements and approaches to detect ecosystem shifts}
\vspace{5 mm}
	
\textsc{Rosalie Bruel$^{*1}$\footnote{*The authors contributed equally to this work.} and Easton R. White$^{*2}$}
\vspace{3 mm}

\normalsize{\indent $^{1}$Rubenstein Ecosystem Science Laboratory, University of Vermont, VT 05405, USA \\ $^{2}$Department of Biology, University of Vermont, VT 05405, USA}
\end{center}

\textbf{Abstract}

Environmental monitoring is a key component of managing ecosystems. Given that most monitoring efforts are still expensive and time-consuming, it is essential that monitoring programs are designed to be efficient and effective. In many situations, the expensive part of monitoring is not sample collection, but instead sample processing, which leads to only a subset of the samples being processed. For example, sediment or ice cores can be quickly obtained in the field, but they require weeks or months of processing in a laboratory setting. Standard sub-sampling approaches often involve equally-spaced sampling. We use simulations to show how many samples and which types of sampling approaches are the most effective in detecting ecosystem change. We test these ideas with a case study of Cladocera community assemblage, which was reconstructed from a sediment core. We demonstrate that standard approaches to sample processing are inefficient. For our case study, using an optimal sampling approach would have resulted in savings of 195 person-hours---thousands of dollars in labor costs. We also show that, compared with these standard approaches, fewer samples are typically needed to achieve high statistical power. We explain how our approach can be applied to monitoring programs that rely on video records, eDNA, remote sensing, and other common tools that allow re-sampling, and where the objective is to detect ecosystem change. 

\vspace{3 mm}

Keywords: {time series}


```{r load_packages,echo=F,warning=F,message=F}
if (!require("pacman",character.only = TRUE))
  {
    install.packages("pacman",dep=TRUE)
    if(!require("pacman",character.only = TRUE)) stop("Package not found")
  }

# Keeping below source for github package. Ask Easton whether pacman works for github packages or not.
#devtools::install_github("rensa/stickylabeller")
  
pacman::p_load(patchwork, stickylabeller, ggplot2, dplyr, wesanderson, reshape, cowplot)
```


<!--
# Things to do:

- clean up introduction
- writing methods/results pair of section
- build out the supplementary material
- run theoretical results for more simulations
- concluding paragraph
- add citations
- Possible journals: Journal of Environmental Management, Environmental Monitoring and Assessment, Ecological Applications

--> 

# Introduction 

Environmental monitoring is one of the core components to modern ecosystem research and management [@McDonald-Madden2010a, @White2019d, @Lindenmayer2020a]. Within an adaptive management framework, monitoring is needed for both learning about the system under study and assessing the effectiveness of management interventions [@Lovett2007]. Increasingly, long-term monitoring programs, like the Long Term Ecological Research (LTER) Network in the USA, are becoming available [@Maguran2010a]. However, environmental monitoring is still often expensive and time-consuming, especially when further procesing is needed following sample collection (citations). Thus, for many fields there is a disparity between the amount of data that can be aquired and stored, and the ultimate number of samples that can be processed. Therefore, monitoring programs need to be designed in such a way to address the question of interest while using limited resources efficiently.<!-- processing methods that could allow to extract the most information from that data. \rosalie{I feel like it could be a sensitive subject (we don't want to attack our peers and the way they do science), but there is definitely some truths. With some restoration work going on at the lab, there were some emails exchange about throwing away samples that were collected in 2015 and never looked at. If we could find the correct way to talk about this issue (without being judgemental, I'm sure it'd be interesting to many!).}

Therefore, monitoring programs need to be designed in such a way to address the question of interest while using limited resources efficiently. Key considerations of an optimal monitoring program include the frequency of surveys, the selection of sites, as well as the precision and accuracy of the monitoring itself [@McDonald-Madden2010a,@White2019d,@Lindenmayer2020a]. -->

Accounting for key considerations in the design of a monitoring program is necessary to detect long-term ecological change. The specifics of the monitoring program will determine the power with which a question of interest can be addressed. For example, @White2019d found that 72\% of vertebrate populations required at least 10 years of monitoring to detect significant changes in the population size over time. The specific number of years required depended on the species biology and the detection method used [@White2019d]. Other work has focused on the frequency of monitoring [@Wauchope2019a], showing that if a significant trend is observed from a dataset with limited temporal perspective, it is likely to reliably describe qualitatively the complete trend (increase or decline), but is unlikely to provide an accurate quantitave change in population. Further research investigated the impact of allocating monitoring resources spatially versus temporally [@Rhodes2011a, @Weiser2019a], and the ability to use citizen science to increase monitoring [@Weiser2020a]. Lastly, both the ecological and economical costs of failing to detect a true trend (type II error) have to be weighed against the risks of false (type I error) detection [@Mapstone1995a]. The approach to optimal sampling is thus a topical subject of research, as under limited budgets, monitoring programs need to be designed to be cost-effective [@Caughlan2001a, @Grantham2008b, @Bennett2016a]. 

Because of new technological advances, there are many data sources that can be derived long after the actual  processes occurred. For example, sediment cores can be retrieved from lakes, allowing reconstruction of past ecological communities or conditions (Cohen, 2003). Similarly, environmental samples (e.g. water, soil) can be saved and processed later for composition, including eDNA [@Bohmann2014a]. Likewise, photo- or video-based monitoring can record snapshots of a system and be analyzed later (citations). In each of these cases, decisions have to made about how much data to extract from the previously collected samples. Should the paleoecological core be analyzed at every centimeter? Should the video be assessed once per minute? As long as processing samples is expensive, these trade-offs will remain.

Here, we develop a set of tools to determine the appropriate number of samples and sampling approach when dealing with data sources where additional samples can be added. We tailor our analysis to the detection of a changepoint in a time series. Changepoints are an important characteristic of a time series as they can indicate a underlying change in ecosystem processes (citations). We focus on paleoecological core samples as one example of this type of data. We examine the situation where the goal is to detect the time at which a significant change in an ecological community occurs, i.e. a changepoint. However, our approaches are widely applicable to other questions and data types. We first investigate these tools using a simulation-based approach. We then test the tools on a case study from a paleosequence of Cladocera community assemblage from Lake Varese located in the subalpine region of north-western Italy (citation).


# Sampling approaches and changepoint detection

For both our simulations and case study, we investigate the effect of different sampling strategies on our ability to detect a changepoint. We begin by either creating simulated time series or using an actual paleoecological time series (Fig. \ref{fig:conceptual}). We then subsampled each time series [@White2020] to test the effect of three different sampling approaches along with varying the sample size (Fig. \ref{fig:conceptual}c-f). We compared the estimated changepoint from the subsampled time series to that of the full time series as a measure of the effectiveness.

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=7, fig.height=7.5,eval=T,fig.cap='Conceptual diagram illustrating the process of taking (a) simulations of a time series and (b) selecting a single simulation to analyze with three different sampling approaches: (c) random, (d) regular, and (e) iterative. The iterative sampling approach requires (f) adding samples around a detected changepoint until (g) a certain level of accuracy is achieved. \\label{fig:conceptual}'}
source(paste0(getwd(),"/R/Fig_conceptual.R"))
```

The random sampling approach involves taking a set number of random points throughout the time series (Fig. \ref{fig:conceptual}c). In the context of sediment cores, this would mean analyzing community composition at random locations along the core. Random sampling is recommended in designs aimed at quantifying the average size of a population (spatial approach) (Nad’o and Kaňuch, 2018). We hypothesize that random sampling will perform the worst at getting as close as possible to a "real" changepoint. Regular sampling is commonly used (citations) and requires that a set number of samples be taken at regular intervals (Fig. \ref{fig:conceptual}d). Lastly, iterative sampling involves first taking a set number of samples and then iteratively adding samples until a pre-determined level of precision is achieved (Fig. \ref{fig:conceptual}e-g). In each scenario, we begin by sampling the first and last sample to ensure coverage of the whole time period. We describe each approach in more detail in the supplementary material and provide code. 

We detect changepoints with the function _e.divisive_ in the R package _ecp_ (James et al 2019). There are several methods available to detect changepoint (reviewed in James and Matteson, 2014); _e.divisive_ is a divisive hierarchical estimation algorithm for multiple change point analysis. We chose this method because it is able to perform multiple change point analysis for both uni- and multi-variate time serie, without a priori knowledge of the number of changepoints. Herein, we focus on detecting the most important changepoint (i.e. the one of largest magnitude), although we tested the method on a time-series that would have multiple changepoints (Supplementary Materials 1). In order to test the performance, we detected the "real" changepoint on the whole time-series, and compare the changepoint found on the sub-sample with the "real" one. The distance to real changepoint served as performance diagnostic.


# Simulation approach

## Simulation model

We began with a theoretical exploration of the sampling requirements to detect a changepoint. We modeled a simple first order autoregressive (AR-1) process (the discrete-time version of the Ornstein–Uhlenbeck process) with a response variable ($X_t$) that represents either population size, biodiversity, or some other unidimensional metric of community composition at time $t$. The model includes temporal autocorrelation ($\phi$), the mean of the process ($\mu_X$), and a white noise term ($w_t$). The white noise term is a normal distribution with mean ($\mu_w$) and variance ($\sigma^2$):

$$
\begin{array}{rcl} 
X_t & = & \mu_X + \phi (X_{t-1} - \mu_X) + w_t \\ 
w_t & \sim & Norm(\mu_w,\sigma^2).
\end{array}
$$
We included a changepoint by shifting $\mu_X$ at time $\tau$ given a specific shift size ($\delta$).

We explored how each of these model parameters affected our ability to detect a change point. We simulate an entire time series to serve as the "true" data for comparison [@White2020]. We specifically study how the amount of samples included and the type of sampling affects the detection probability. For simulations, statistical power is the fraction of simulations that were able to detect a changepoint. We define an accurate changepont detection if an estimate is within five time points (over 100 time points) of the true changepoint. We define $T_{min}$ as the minimum number of samples required to achieve 0.8 statistical power.


```{r theoretical_results_old,message=F,warning=F,echo=F,eval=F}

source('R/sampling_regular.R')
source('R/sampling_random.R')
source('R/build_time_series.R')
source('R/updated_param_sensitivity.R')

require(tidyverse)

par(mfrow=c(2,2))



```


```{r simulation_min_time,message=F,warning=F,echo=F,eval=F}

source('R/sampling_regular.R')
source('R/sampling_random.R')
source('R/build_time_series.R')
source('R/updated_param_sensitivity.R')
source('R/calc_min_time2.R')
require(tidyverse)

# Sigma results
sigma_vec = seq(0.01,0.5,0.025)
min_time  = rep(0,times=length(sigma_vec))
sigma_output <- data.frame(sigma_vec,min_time)

for (param_num in 1:nrow(sigma_output)){
  sigma_output$min_time[param_num] <- calc_min_time('sigma',sigma_output$sigma_vec[param_num])
  print(sigma_output)
}

write.csv(x = sigma_output,file = 'Output/theoretical_results_sigma.csv',quote=FALSE)

# Phi results

phi_vec = seq(-0.8,0.8,0.4)
min_time  = rep(0,times=length(phi_vec))
phi_output <- data.frame(phi_vec,min_time)

for (param_num in 1:nrow(phi_output)){
  phi_output$min_time[param_num] <- calc_min_time('phi',phi_output$phi_vec[param_num])
  print(phi_output)
}

write.csv(x = phi_output,file = 'Output/theoretical_results_phi.csv',quote=FALSE)


# Shift size

shift_size_vec = rev(seq(0.1,0.9,0.05))
min_time  = rep(0,times=length(shift_size_vec))
shift_size_output <- data.frame(shift_size_vec,min_time)

for (param_num in 1:nrow(shift_size_output)){
  shift_size_output$min_time[param_num] <- calc_min_time('shift_size',shift_size_output$shift_size_vec[param_num])
  print(shift_size_output)
}

write.csv(x = shift_size_output,file = 'Output/theoretical_results_shift_size.csv',quote=FALSE)


```


```{r read case study data and calculate some parameters to plot them on theoritical plot, message=FALSE, warning=FALSE, include=FALSE}
getpath4data <- function() {
  if(Sys.getenv("USER")=="Rosalie") return("/Volumes/-/Script R/Data-off-GitHub/temporal-sampling/")
  if(Sys.getenv("USER")=="eastonwhite") return("~/Desktop/Research/soil-temporal-sampling/")
  if(Sys.getenv("USER")!="Rosalie"|Sys.getenv("USER")!="put here your USER") stop("You need to get the data and define their location.")
}
# Read data
vr <- read.delim(paste0(getpath4data(),"VAR10-10-clado.txt"))

# Detect 'true' changepoint
vrall <- sample_regular(vr, n=nrow(vr), input_vector = F, xcol = 1, DCA_axis = 1, messages = F)
vrall$matrix$age <- vr$AGE[vrall$matrix$index]

# Calculate the 3 time-series parameter needed for comparison to the theoritical results

vr_time_series = vrall$matrix$x
vr_time_series = (vr_time_series -min(vr_time_series ))/(max(vr_time_series )-min(vr_time_series ))

# 1 - Calculate coefficient of variation
cv_vr <- sd(vr_time_series)/mean(vr_time_series)

# 2 - Calculate autocorrelation of real time serie
ar_vr <- acf(vr_time_series, lag.max = 5, plot = FALSE)$acf[2]
  
# 3 - Calculate shift size
# Difference between the two values before/after shift
shift_vr <- abs(vr_time_series[vrall$changepoint]-vr_time_series[vrall$changepoint-1])/vr_time_series[vrall$changepoint-1]

```



```{r simulation_min_time_plot, fig.width = 7.2, fig.height= 3, fig.cap='Minimum number of samples required for 0.8 statistical power given different levels of (a) temporal variability ($\\sigma$), (b) temporal autocorrelation ($\\phi$), and (c) shift size. Random sampling was used with the default parameters are $\\sigma = 3$, $\\phi = 0.5$, and a shift size $=0.75$. The exact timing of the true changepoint varied for each simulation between time steps 30 and 70. Vertical line indicate respective parameter calculated from the case study time series. \\label{fig:simulation_min_time}',message=F,warning=F,echo=F,eval=T}

par(mfrow=c(1,3))

sigma <- read.csv('Output/theoretical_results_sigma.csv',header=TRUE)
#plot(sigma$sigma_vec,sigma$min_time,ylab='Minimum number of samples required',xlab='sigma',pch=16,cex.lab=1.3,las=1,ylim=c(20,50),col=1,type='o')

# Phi results

phi <- read.csv('Output/theoretical_results_phi.csv',header=TRUE)
#plot(phi$phi_vec,phi$min_time,ylab=' ',xlab='phi',pch=16,cex.lab=1.3,las=1,ylim=c(20,50),col=1,type='o')


# Shift size results

shift_size <- read.csv('Output/theoretical_results_shift_size.csv',header=TRUE)
#plot(shift_size$shift_size_vec,shift_size$min_time,ylab=' ',xlab='shift size',pch=16,cex.lab=1.3,las=1,ylim=c(20,50),col=1,type='o')

# Melt the 3 dataset to be able to use facet_wrap()
colnames(sigma) <- colnames(phi) <- colnames(shift_size) <- c("X", "Param", "min_time")
simulation_all <- 
  rbind(cbind("Parameter" = rep("Standard deviation", nrow(sigma)), sigma),
      cbind("Parameter" = rep("Autocorrelation at lag-1", nrow(phi)), phi),
      cbind("Parameter" = rep("Normalized amplitude of shift", nrow(shift_size)), shift_size))
# param_vr <- data.frame("Parameter" = unique(simulation_all$Parameter),
#                        "value" = c(ar_vr, cv_vr, shift_vr))

p1 <- 
ggplot(simulation_all %>% filter(Parameter == unique(simulation_all$Parameter)[1]), aes(Param, min_time)) +
  geom_point() +
  geom_line() +
  geom_vline(xintercept=0.05, lty=2) + 
  facet_wrap(~Parameter, scales="free_x", 
             labeller = label_glue('a. {Parameter}')) +
  ylim(10,50) +
  labs(x=c("sigma"), y= "Minimum number of samples required") + 
  theme_bw() +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        strip.background = element_rect(colour="transparent", fill="transparent"),
        strip.text.x = element_text(angle = 0, hjust = 0))
p2 <- ggplot(simulation_all %>% filter(Parameter == unique(simulation_all$Parameter)[2]), aes(Param, min_time)) +
  geom_point() +
  geom_line() +
  geom_vline(xintercept = 0.404, lty=2) +
  facet_wrap(~Parameter, scales="free_x", 
             labeller = label_glue('b. {Parameter}')) +
  ylim(10,50) +
  labs(x="phi", y= "") + 
  theme_bw() +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        strip.background = element_rect(colour="transparent", fill="transparent"),
        strip.text.x = element_text(angle = 0, hjust = 0))
p3 <- ggplot(simulation_all %>% filter(Parameter == unique(simulation_all$Parameter)[3]), aes(Param, min_time))  +
  geom_point() +
  geom_line() + 
  geom_vline(xintercept = 0.81, lty=2) +
  facet_wrap(~Parameter, scales="free_x", 
             labeller = label_glue('c. {Parameter}')) +
  ylim(10,50) +
  labs(x="shift size", y= "") + 
  theme_bw() +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        strip.background = element_rect(colour="transparent", fill="transparent"),
        strip.text.x = element_text(angle = 0, hjust = 0))
  
  
# Remove the y-axis for the 2nd plot - p2 then merge 2 plots
cowplot::plot_grid(p1 + theme(plot.margin = unit(c(0.15, 0.1, 0.15, 0.15), "cm")), 
                   p2 + 
                     theme(plot.margin = unit(c(0, 0.1, 0, 0), "cm"),
                           axis.text.y = element_blank(),
                           axis.line.y = element_blank(),
                           axis.title.y= element_blank(),
                           axis.ticks.y= element_blank()),
                   p3 + 
                     theme(plot.margin = unit(c(0, 0.1, 0, 0), "cm"),
                           axis.text.y = element_blank(),
                           axis.line.y = element_blank(),
                           axis.title.y= element_blank(),
                           axis.ticks.y= element_blank()),
                   nrow = 1,
                   rel_widths = c(1.2, 1,1),
                   align = 'h', axis = 'tb')

```



# Simulation results 

In line with theory about monitoring, we found that the probability of correcting identifying a changepoint decreased with smaller levels of population variability ($\sigma$) and autocorrelation ($\phi$). We also found that the probability of correct changepoint detection increases with larger shift sizes, which is essentially the effect size. Thus, the number of sample required to obtain high statistical power (above 0.8) increased with larger population variability, lower autocorrelation, and smaller shift sizes (Fig. \ref{fig:simulation_min_time}). For perspective, the autocorrelation at lag-1 in the first DCA component we used in the case study example later is `r round(ar_vr,3)`, the coefficient of variation is `r round(cv_vr,3)`, and the normalized amplitude of the shift was `r round(shift_vr,3)`.



```{r simulation_results_sampling_approaches,message=F,warning=F,echo=F,eval=F}

source('R/sampling_regular.R')
source('R/sampling_random.R')
source('R/sampling_iterative.R')
source('R/build_time_series.R')
source('R/updated_param_sensitivity.R')
source('R/calc_min_time2.R')
require(tidyverse)

# Need to run each method on a set of simulated time series. We would then have 100 runs for each method and each sampling amount

approach <- c('Random','Regular','Iterative')
number_samples <- c(seq(5,20,2),25,30,35)
simulation_number <- 1:20
output_theory_approach <- data.frame(expand.grid(approach,number_samples,simulation_number))
names(output_theory_approach) <- c('approach', 'number_samples', 'simulation_number')
output_theory_approach$distance_to_real = 0

# Build function that takes in approach, simulated time series, and number of samples. The function should then spit out the distance to the real changepoint

# Set of time series (need other params)
params <- tibble(shift_time = sample(30:70,max(simulation_number),replace = T))

time_series <- params %>%
  pmap(build_time_series) 

# Run through params

for (i in 1:nrow(output_theory_approach)){
  
  output_theory_approach$distance_to_real[i] <- abs(params$shift_time[output_theory_approach$simulation_number[i]] - calc_distance_to_real(output_theory_approach$approach[i],output_theory_approach$number_samples[i],time_series[[output_theory_approach$simulation_number[i]]]))
  print(i)
}


write.csv(x = output_theory_approach,file = 'Output/theoretical_comparing_approaches.csv',quote = FALSE )
```


```{r simulation_approaches_graph, echo=FALSE, message=FALSE, warning=FALSE, fig.width = 7.2, fig.height= 3,fig.cap='Distance to real changepoint per total number of samples analyzed for simulations, following (a) random sampling, (b) regular sampling, and (c) iterative sampling. Each sample size and approach combination was simulated 50 times with the same parameters as Figure \\ref{fig:simulation_min_time}.\\label{fig:simulation_number_sample}'}

myoutput <- read.csv(file='Output/theoretical_comparing_approaches.csv',header=T)
myoutput$approach <- factor(myoutput$approach,levels = c('Random', 'Regular', 'Iterative'))
myoutput_summary <- myoutput %>%
  group_by(approach,number_samples) %>%
  summarize(mean_distance = mean(distance_to_real),sd_distance=sd(distance_to_real))
  
p1  <- ggplot(myoutput_summary, aes(x=number_samples,y=mean_distance)) + geom_line()  + geom_ribbon(aes(ymin = mean_distance - 1.96*sd_distance, ymax = mean_distance + 1.96*sd_distance), fill = "grey80") +geom_line(size=1) + coord_cartesian(ylim = c(2.2, 50))  + 
  facet_wrap(~approach,
             labeller = label_glue('{.l}. {approach} sampling ')) + 
  theme_bw() +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        strip.background = element_rect(colour="transparent", fill="transparent"),
        strip.text.x = element_text(angle = 0, hjust = 0)) +
  labs(x="Total number of samples analyzed", y="Distance to real changepoint") #+
 # theme(legend.position = c(0.93, 0.72),
#       legend.background = element_rect(fill = "#ffffffaa", colour = NA),
#        legend.title=element_text(size=7), 
 #       legend.text=element_text(size=8)) +
#  guides(pch=guide_legend(title="Initial number\nof sample"))

p1


```





# Case study

## Case study background

As one application of our approach, we examined paleosequence data. To obtain this data sediment cores were taken from aquatic environments to reconstruct past ecosystems. Samples can be taken before and after perturbation to compare community composition (e.g., ), or at continuously to identify the timing of change, and contribution of environmental variable in driving these changes (e.g., @Perga2015).

We tested different sampling methods on a real community time-series, from Lake Varese (IT). Lake Varese is a small (14.8 km^2^), deep (z$_{max}$= 26m), monomictic lake, in the subalpine region of north-western Italy (238 m asl). It underwent hyper-eutrophication over the 20^th^ century due to increase in nutrient loads from the watershed (ref). Nutrient status was responsible for restructuration of the lake communities across trophic levels [@Crosta1999,@Bruel2018]. Air temperature is now driving changes in plankton communities (@Bruel2018).

In a previous study, Cladoceran assemblage was reconstructed at a 2-3 years resolution, from a 74-cm sediment core covering the 1816(±26)—2010 time period [@Bruel2018]. We used the same ordination method as Bruel et al (2018) to reduce the dimensionality of the assemblage-level data. As a result, we detected several changepoints on the first component (in 1926/1928, 1946/1948 and 1983/1988). We defined these as the "true" changepoints given they came from an analysis of the complete data. The 1983-1988 changepoint was the largest in magnitude. 

In line with our simulation approach, we subsampled the full record (74 observations) using the three methods described earlier (random, regular, iterative). For each of the three methods, we examined the error (difference between the true changepoint and the detected changepoint) when using different numbers of samples.

Each subsampling was done from the initial community dataset (Fig. \ref{fig:case_study_iterative_sampling}a). An ordination was run on the number of samples selected, and the changepoint analysis was run on that new vector. In case of the iterative method, a new sample was added and the ordination was run again (Fig. \ref{fig:case_study_iterative_sampling}d).


## Case study results

We found that random sampling performed the worst, as changepoint analysis was left to chance. Regular sampling provided good estimates from 8 samples (0-3 sample of absolute error), but detecting the true changepoint depended on the interval falling close to the true changepoint (i.e., left to chance). Iterative sampling, where 5 samples are initially sampled, then other samples are slowly added, performed the best, as no more than 9 samples were ever necessary to get the true changepoint (Fig. \ref{fig:case_study_number_sample} (c)). We show how iterative sampling changes the ordination, and the resulting changepoint, as more samples are added (Fig. \ref{fig:case_study_iterative_sampling}) <!--Note that when the initial number of samples was set to 7 Fig. \ref{fig:case_study_number_sample} (c), square markers), the distance to the real changepoint was initially very high. Without any prior, the initial number sampled fell far away from the true changepoint.-->

We tested how the three methods performed at detecting other changepoints of lower magnitude (as three changepoints were detected in the initial study, @Bruel2018). Iterative sampling still performed best, especially if an higher number of initial samples was chosen (7 samples out of 74, Supplementary Materials \textcolor{red}{2}).
<!--This could probably be moved to the discussion: The initial number of samples should be chosen depending on the objective of the study. If investigators want to find the biggest transition only, then a low number might be enough (include here results from theoretical results?). If investigators suspect or want to test for several changepoints, we recommend analyzing initially about 10% of the length of the time series to have a good initial ranking.-->



```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width = 8, fig.height= 7.5, fig.cap='Iterative sampling and impact on ordination. (a) Initial community dataset. Samples are initially sampled at regular interval. The multivariate data is converted to univariate vector by Detrended Component Analysis. Samples were iteratively added, after computation of changepoint at each step. (b) Full time series, on which "true" changepoint was detected. (c) First step of iterative sampling, n = 5. (d) Final step of iterative samples: by adding 4 samples, the true changepoint was detected. \\label{fig:case_study_iterative_sampling}'}

# Creating a dataset with all scores 
vr_out_matrix <- read.table(paste0(getwd(),"/Output/output_iterative_sampling_varese.txt"))

index_temp <- vr_out_matrix$index[vr_out_matrix$n==max(vr_out_matrix$n)]
vr_out_order <-
  data.frame("index" = index_temp,
             "age"   = vrall$matrix$age[index_temp],
             "x"     = vrall$matrix$x[index_temp],
             "count" = summary(as.factor(vr_out_matrix$index)))
vr_out_order$count <- abs(vr_out_order$count-max(vr_out_order$count)-1)



mypal <- wes_palette("GrandBudapest2", max(vr_out_order$count),type = "continuous")
 
# Plot with all data
 p_init <- ggplot(vrall$matrix, aes(age, x)) + geom_point(alpha = .3) + geom_line(alpha = .3) +
  theme_bw()  +
      theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) + 
           geom_rect(mapping=aes(xmin=vrall$matrix$age[which(vrall$matrix$index==vrall$changepoint)-1], xmax=vrall$matrix$age[which(vrall$matrix$index==vrall$changepoint)], ymin=-1, ymax=2), alpha=0.05, fill="grey") +
   labs(subtitle = paste0("b. Full time-series, n = ", nrow(vr)), x="Year", y="DCA first component",
        color='samples added\nat iteration t=') +
   geom_point(vr_out_order, mapping=aes(age, x, color = as.factor(count)), cex=2) +
   geom_text(vr_out_order, mapping=aes(age, x, label =count), vjust = 1.5, nudge_x = 1.5) +
   scale_color_manual(values=mypal) 



 
# Initial plot, then Adding samples
for (i in 5:12) {
  # Start a new loop only if the final changepoint hasn't been found at the previous iteration.  
  if (i==5 || vrsub$matrix$index[vrsub$matrix$index==vrsub$changepoint] - vrsub$matrix$index[which(vrsub$matrix$index==vrsub$changepoint)-1]>1) {
    
  vrsub <- sample_iterative(vr, n=i, n2=5, input_vector = F, xcol = 1, DCA_axis = 1, messages = F)
  vrsub$matrix$age <- vr$AGE[vrsub$matrix$index]
  vrsub$matrix$count <- vr_out_order$count[vr_out_order$age %in% vrsub$matrix$age]
  
    if (i==5) {
      vrsub1 <- vrsub
    p1 <- ggplot(vrsub$matrix, aes(age, x, color=as.factor(count))) +  geom_line(alpha = .4, col="grey") + geom_point() + guides(col=FALSE) +
   scale_color_manual(values=mypal) +
  theme_bw() +
      theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
   labs(x="Year", y="DCA first component",
        color='samples added\nat iteration t=')
    p2 <- p1
  } else {
    if (i == 6) {
      p2 <- p1 + 
        geom_line(vrsub$matrix, mapping=aes(age, x), alpha = .4, col="grey") + 
        geom_point(vrsub$matrix, mapping=aes(age, x,color=as.factor(count)))   + guides(col=FALSE) 
    } else {
        p2 <- p2 + geom_line(vrsub$matrix, mapping=aes(age, x), alpha = .4, col="grey") + 
          geom_point(vrsub$matrix, mapping=aes(age, x,color=as.factor(count))) + 
          guides(col=FALSE)
      }
  }
      # Save plot
    ggsave(p2 + 
             geom_rect(stat = "identity", mapping=aes(xmin=vrsub$matrix$age[which(vrsub$matrix$index==vrsub$changepoint)-1], xmax=vrsub$matrix$age[which(vrsub$matrix$index==vrsub$changepoint)], ymin=-1, ymax=3), col=NA, fill="grey", alpha=0.05) +
             annotate("text", label = paste0("Number of samples = ",i), x = min(vrsub$matrix$age), y = 2.8, colour = "black",  hjust=0,vjust=0), 
           filename = paste0("Output/GIF_adding_sample_case_study/Iterative_",i,".pdf"), width = 6, height = 4)

  }
}

p1 <- p1 + geom_line(vrsub1$matrix, mapping=aes(age, x), alpha = .8, col="black") + 
          geom_point(vrsub1$matrix, mapping=aes(age, x,color=as.factor(count))) + geom_rect(stat = "identity", mapping=aes(xmin=vrsub1$matrix$age[which(vrsub1$matrix$index==vrsub1$changepoint)-1], xmax=vrsub1$matrix$age[which(vrsub1$matrix$index==vrsub1$changepoint)], ymin=-1, ymax=3), col=NA, fill="grey", alpha=0.05) +
  labs(subtitle="c. Initial number of samples, n = 5")

p2 <- p2 + geom_line(vrsub$matrix, mapping=aes(age, x), alpha = .8, col="black") + 
          geom_point(vrsub$matrix, mapping=aes(age, x,color=as.factor(count))) + 
  geom_rect(stat = "identity", mapping=aes(xmin=vrsub$matrix$age[which(vrsub$matrix$index==vrsub$changepoint)-1], xmax=vrsub$matrix$age[which(vrsub$matrix$index==vrsub$changepoint)], ymin=-1, ymax=3), col=NA, fill="grey", alpha=0.05) +
  labs(subtitle=paste0("d. n =", nrow(vrsub$matrix)))


# Fake dataset to explain sample selection
fakedata <- data.frame(matrix(rep(NA,74*10), nrow=74))
colnames(fakedata) <- 1:ncol(fakedata)
fakedata[is.na(fakedata)] <- 0
fakedata[vr_out_order$index[vr_out_order$count==1],] <- 1 
fakedata[vr_out_order$index[vr_out_order$count==2],] <- 2 
fakedata[vr_out_order$index[vr_out_order$count==3],] <- 3 
fakedata[vr_out_order$index[vr_out_order$count==4],] <- 4 
fakedata[vr_out_order$index[vr_out_order$count==5],] <- 5 
fakedata[,1] <- 6
fakedata <- rbind(rep(7,ncol(fakedata)),fakedata)
fakedata <- cbind(Year=rep(75:1,ncol(fakedata)), melt(fakedata))
#head(fakedata)

p3 <- ggplot(fakedata, aes(variable,Year)) +
  geom_tile(aes(fill = as.factor(value)), colour="black") +
  scale_fill_manual(values=c("white",mypal,grey(.8), grey(.8))) + guides(fill=FALSE)  + theme(panel.border = element_blank(), panel.grid.major = element_blank(),
panel.grid.minor = element_blank(), axis.line = element_blank(), panel.background = element_blank(), axis.title=element_blank(),
        axis.text=element_blank(),
        axis.ticks=element_blank(),
plot.subtitle=element_text(size=11)) +
  geom_text(fakedata[fakedata$Year==75,], mapping = aes(variable,Year, label=c("Year", paste("Sp.", c(1:8,"n")))),  size=2.5,hjust=0,angle = 90) +
  geom_text(fakedata[fakedata$variable==1&fakedata$Year!=75,], mapping = aes(variable,Year, label=c(vr$AGE)),  size=2.1) +
  labs(subtitle="a. Continuous community dataset")


# Assemble plot

p3 | (p_init / p1 / p2)


```



```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width = 7.2, fig.height= 3,fig.cap='Distance to real changepoint in commuper total number of samples analyzed, following (a) random sampling, (b) regular sampling, and (c) iterative sampling. Total number of samples was set between 5 and 30, out of the 74 initial time series.  \\label{fig:case_study_number_sample_change_biomass}'}
# Read data
myoutput <- read.table(paste0(getwd(),"/Output/output_changepoint_varese.txt"))
myoutput$method <- factor(myoutput$method, levels=c('Random', 'Regular', 'Iterative'))
myoutput$diff_real <- abs(myoutput$diff_real)

# Change to TRUE if we decide it is worth it to show several initial number of n2. It looks more confusing -- but it would bring more information
several_n2_init = F

# Plot
p1 <- if(several_n2_init) {
  ggplot(myoutput[myoutput$target_cpt==1,], aes(final_n,diff_real, pch=factor(n2_init))) + 
  geom_hline(aes(yintercept=0), col=adjustcolor("black", alpha.f = .4)) + 
  geom_point(alpha = .3) +
  facet_wrap(~method,
             labeller = label_glue('{.l}. {method} sampling ')) + 
  theme_bw() +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        strip.background = element_rect(colour="transparent", fill="transparent"),
        strip.text.x = element_text(angle = 0, hjust = 0)) +
  labs(x="Total number of samples analyzed", y="Distance to real changepoint") +
  theme(legend.position = c(0.93, 0.72),
        legend.background = element_rect(fill = "#ffffffaa", colour = NA),
        legend.title=element_text(size=7), 
        legend.text=element_text(size=8)) +
  guides(pch=guide_legend(title="Initial number\nof sample"))
} else {
  ggplot(myoutput[myoutput$target_cpt==1 & myoutput$n2_init == 5,], aes(final_n,diff_real)) + 
  geom_hline(aes(yintercept=0), col=adjustcolor("black", alpha.f = .4)) + 
  geom_point(alpha = .3) +
  facet_wrap(~method,
             labeller = label_glue('{.l}. {method} sampling ')) + 
  theme_bw() +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        strip.background = element_rect(colour="transparent", fill="transparent"),
        strip.text.x = element_text(angle = 0, hjust = 0)) +
  labs(x="Total number of samples analyzed", y="Distance to real changepoint") 
}

p1


```



```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width = 7.2, fig.height= 3,fig.cap='Distance to real changepoint in biomass per total number of samples analyzed, following (a) random sampling, (b) regular sampling, and (c) iterative sampling. Total number of samples was set between 5 and 30, out of the 74 initial time series.  \\label{fig:case_study_number_sample}'}
#investigating species average biomass
vr_count <- rowSums(vr[,-1])
myoutput_count <- NULL
for (i in 5:30) {
  vrsub <- sample_regular(vr_count, i, input_vector = T, messages = F)
  myoutput_count <- c(myoutput_count, vrsub$method, vrsub$final_n, vrsub$changepoint_all$order.found[j], length(vrsub$changepoint_all$estimates)-2)
  vrsub <- sample_random(vr_count, i, input_vector = T, messages = F)
  myoutput_count <- c(myoutput_count, vrsub$method, vrsub$final_n, vrsub$changepoint_all$order.found[j], length(vrsub$changepoint_all$estimates)-2)
  vrsub <- sample_iterative(vr_count, i, input_vector = T, messages = F)
  myoutput_count <- c(myoutput_count, vrsub$method, vrsub$final_n, vrsub$changepoint_all$order.found[j], length(vrsub$changepoint_all$estimates)-2)
  vrsub <- sample_iterative(vr_count, i, input_vector = T, messages = F, n2 = 6)
  myoutput_count <- c(myoutput_count, vrsub$method, vrsub$final_n, vrsub$changepoint_all$order.found[j], length(vrsub$changepoint_all$estimates)-2)
  vrsub <- sample_iterative(vr_count, i, input_vector = T, messages = F, n2 = 7)
  myoutput_count <- c(myoutput_count, vrsub$method, vrsub$final_n, vrsub$changepoint_all$order.found[j], length(vrsub$changepoint_all$estimates)-2)
}

myoutput <- read.table(paste0(getwd(),"/Output/output_changepoint_varese_biomass.txt"))
myoutput$method <- factor(myoutput$method, levels=c('Random', 'Regular', 'Iterative'))
myoutput$diff_real <- abs(myoutput$diff_real)


# Plot
p1 <-
  ggplot(myoutput[myoutput$target_cpt==1 & myoutput$n2_init == 5,], aes(final_n,diff_real)) + 
  geom_hline(aes(yintercept=0), col=adjustcolor("black", alpha.f = .4)) + 
  geom_point(alpha = .3) +
  facet_wrap(~method,
             labeller = label_glue('{.l}. {method} sampling ')) + 
  theme_bw() +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        strip.background = element_rect(colour="transparent", fill="transparent"),
        strip.text.x = element_text(angle = 0, hjust = 0)) +
  labs(x="Total number of samples analyzed", y="Distance to real changepoint") 


p1


```


\rosalie{About figure \ref{fig:case_study_number_sample_change_biomass}, I tried testing the real changepoint on another potential metric, i.e., biomass. Potential tools for discussion? I don't know! Maybe worth talking about that during our next Skype.}

# General Discussion

Outline:

- reexplain and compare the 3 methods (theoretical (Fig 3) vs case study (Fig 4), and how they relate (Fig 2))
- money implications (before or now)  
- how this fits in with other work: examples of situation it would be applied (Rosalie)  
- how this doesn't fit with other work (limitations) (Rosalie)
- future work  (?)

<!---- reexplain and compare the 3 methods (theoretical (Fig 3) vs case study (Fig 4), and how they relate (Fig 2)) ---->
There is often a difference between the number of samples analyzed and the total number of samples collected, due to time or funding limitations. When the processing time is disproportionally higher than the collection time especially, a subsampling can be done posterior to sampling. A decision must then be taken of which subsamples to analyze. To address this question in the context of detecting changepoints, we tested three subsampling methods: subsampling at regular and random interval, and an iterative sampling approach (Fig. \ref{fig:conceptual}). 

We found that the iterative methods was systematically better at detecting changes than the two other methods, random subsampling being the least efficient (Figs. \ref{simulation_number_sample} and \ref{fig:case_study_number_sample}). Autocorrelation, variance, and shift size, had an effect on how many samples were needed to detect the shift, but did not changed this conclusion.  (Fig. \ref{fig:simulation_min_time}).

<!---- money implications ---->

Multiple subsampling strategies can be chosen (Fig. \ref{fig:conceptual}), but only iterative sampling detected the real changepoint with a limited number of samples (Fig. \ref{fig:simulation_min_time}c). Analyzing 11% of the sample was enough in most cases to approach the "true" changepoint.

Applied to the real case study, the iterative method allowed to find the main changepoint with 9 samples analyzed (Fig. \ref{fig:case_study_number_sample}), with a starting point of 5 randomly distributed samples. Same results for the major change, with only 12% of the samples analyzed. The method also work to detect further shifts (Supplementary Materials 1).
@Bruel2018 processed one sample at each centimeter in a 74-centimeter sediment core. Each sample took an average of 3 hours to process. We found that using an interval approach would have eliminated `r 3*(74-9)` hours of sample processing, or about `r round(3*(74-9)/8)` days, which is just a little over a month of work. This correspond to several thousands of US dollars depending on labor costs. 


<!---- how this fits in with other work: examples of situation it would be applied (potential) ---->

Our approach goes beyond just paleoecological analyses. Running simulations or using past data to understand the amount of sampling effort required is important in many systems where sample collection or processing is expensive [@White2019,@White2020]. The specific sampling techniques can also be compared to determine the optimal strategy in terms of accuracy and cost. Our specific approach applies to situation where more subsamples can be added, or processed, after the dynamics occurred. Our interval sampling approach involved taking a fixed number of regularly-spaced samples to start and then adding more samples strategically to detect a changepoint. It matches very well the paleoecological approach (Wingard et al, 2017). However, both different types of data and different questions can be addressed with the same approach. Suppose instead that the goal was to detect a change in observed individuals with video-based approaches. It is often not practical to watch entire videos, so it can be useful to choose strategic time-points that would address our question of interest (citations). Using an interval sampling approach, one could take a fixed number of samples to start. The trend over time from simple linear regression could be taken. Then, samples can be taken at random locations one-by-one and to see which samples have the largest effect on the trend estimates. If a particular sample has a large effect on the trend, then it would be best to choose another nearby sample. Sampling would continue until the trend estimate reached an equilibrium. Thus, the iterative sampling approach is particularly relevant to data sources where additional samples can be taken long after the initial dynamics. These approaches would also be appropriate for environmental samples, such as water or soil, that can be analyzed later or eDNA that can be extracted from previously-collected samples (citations). 


The approach is versatile: it also finds in 9 samples (11 to confirm) the change in species biomass (Fig. \ref{fig:case_study_number_sample_change_biomass}). The initial 5 samples analyzed are the same: a well tailored question from the start allows to reach the changepoint much more efficiently than random and regular subsampling methods. (\rosalie{do we want to include that? not sure})

<!---- how this doesn't fit with other work (limitations) ---->

Our approach is able to address other data sources and questions, but not any. When processing time and resources are less than the collection time, investigators will be likely to process every sample. If the studied phenomenon follow the rules of a time-series, and the question concerns a changepoint, it is worth noting that a regular sampling is desirable compared to a random one (Fig. \ref{simulation_number_sample}). More generally, some questions require a continuous time serie, or at least a regular sampling interval. For example, continuous subsampling of a time-series is required to detect critical slowing-down or early warning of shifts (Frossard et al 2015, Doncaster et al 2016). However, critical slowing down does not necesseraly result in a shift, and a shift can occur without critical slowing down (Spears et al, 2017). Signs of critical slowing downs are important to understand and recognize because they provide potential early warnings (Doncaster et al 2016), but in terms of management, knowing most accurately the timing of a shift can have larger implication in adressing the driver. 

\rosalie{I don't understand what you meant here. I tried reformulating some of it, but we can talk about it more if needed}:  
\st{The specific number of samples and optimal sampling approach will largely be system specific. A similar system may be used to get some sense for the best sampling scheme, but it will depend on the system dynamics (e.g. interannual variation, trend effect size). The specific sampling scheme also depends on the question of interest. For instance, changepoint detection may require fewer samples than detecting a change in biodiversity indices (citations). Thus, selecting a set number of samples or specific approach may also limit what future questions can be asked. However, more samples could be processed later to address these additional questions.}


<!--not sure where to include this 
Many ecological processes are non linear (e.g., D'Amario et al 2019, Scientific reports). Detecting a changepoint on one aspect of the ecosystem does not guarantee that the sampling resolution is appropriate to detect the driver for the changepoint.--->


<!---- conclusion ---->

Analyzing a subsample of a time series as opposed to the whole time series will inevitably leads to a lesser understanding of the phenomenon observed [@White2019d]. For example, a continuous sampling allows the calculation of autocorrelation or variance, that can be valuable indicators of critical slowing down (e.g., Dakos and Bascompte, 2014). We show here that an informed subsampling can still allow detection of critical information, such as time series changepoint. This is valuable given the high costs of many monitoring programs (citations). Monitoring programs have to be able to address our questions of interest with sufficient statistical power. The costs of monitoring have to weighed against the value gained from monitoring---a value of information approach (@Lovett2007). Monitoring programs should try to anticipate the potential questions of tomorrow, and reducing the data collected must be done with the best foresight possible on how these data may be necessary to manage ecosystems in the future.  
We make the point that if a subsample of the samples must be used, the temptation to be unbiased by using a regular subsampling will lead to losing a lot of accuracy in the process. If the objective is clearly to detect changepoint, an iterative, partial approach, is desirable. 


<!--
Eventually, it is the researchers choice to choose their approach. Our objective was to quantify the implications of an approach vs. another. In the eventuality that the main objective is to detect a changepoint, we find that an approach is largely superior to the two others.
-->





\clearpage

# References