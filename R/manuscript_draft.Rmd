---
bibliography: Whitebib.bib
csl: ecology-letters.csl
editor_options:
  chunk_output_type: console
fontsize: 12pt
geometry: margin=1in
header-includes: \usepackage{float} \usepackage{lineno} \usepackage{setspace}\doublespacing
  \usepackage[round]{natbib} \bibpunct[; ]{(}{)}{,}{a}{}{,} \usepackage{color} \usepackage{totcount}
  \newtotcounter{citenum} \def\oldcite{} \let\oldcite=\bibcite \def\bibcite{\stepcounter{citenum}\oldcite}
output:
  pdf_document:
    fig_caption: yes
    keep_tex: yes
    number_sections: no
---

```{r setup, include=FALSE}
knitr::opts_knit$set(root.dir = dirname(getwd()))
```

Title: Sampling requirements to detect ecosystem change
	
\vspace{7 mm}
	
Authors: Rosalie Bruel$^{1}$ and Easton R. White$^{2}$
	
\vspace{5 mm}
	
Addresses: $^{1}$Rubenstein Ecosystem Science Laboratory, University of Vermont, VT 05405, USA $^{2}$Department of Biology, University of Vermont, VT 05405, USA

\vspace{5 mm}
Keywords: {time-series} \vspace{10 mm}
	
\vspace{5 mm}

	
Number of words: XXXX \vspace{5 mm}
	
Number of references: 
	
Number of figures and panels: X
	
\vspace{5 mm}
	
Submission as: 
	
\vspace{5 mm}
	
Preprint available at: 


\clearpage


\begin{center}
\textbf{\Large Title here}
\vspace{7 mm}
	
\textsc{Rosalie Bruel$^{1}$ and Easton R. White$^{2}$}
\vspace{5 mm}

\normalsize{\indent $^{1}$Rubenstein Ecosystem Science Laboratory, University of Vermont, VT 05405, USA \\ $^{2}$Department of Biology, University of Vermont, VT 05405, USA}
\end{center}

\textbf{Abstract}

Abstract here

\vspace{3 mm}

Keywords: {time series}





<!--
# Things to do:

- writing intro
- writing methods/results pair of section
- writing discussion
- setting up bibliography system
- Possible journals: Methods Ecology and Evolution, Biological Conservation, *Journal of Environmental Management*, *Environmental Monitoring and Assessment*, BioScience 


# Main messages of manuscript

- We can choose the number of samples and effort we use...
- Number of samples required depends on question asked and the time series parameters
- Case study of paleo data shows the utility of iterative sampling in detecting change points
--> 

# Introduction 


Environmental monitoring is one of the core components to modern research and management (citations). Within an adaptive management framework, monitoring is needed for both learning about the system under study and assessing the effectiveness of management interventions (Lovett et al, 2007). Increasingly, long-term monitoring programs, like the Long Term Ecological Research (LTER) Network in the USA, are becoming available (Magurran2010). However, environmental monitoring is still often expensive and time-consuming (citations). Therefore, monitoring programs need to be designed in such a way to address the question of interest. Key considerations of an optimal monitoring program include the frequency of surveys, the selection of sites, as well as the precision and accuracy of the monitoring itself (citation). Monitoring programs also need to be designed to be cost-effective given limited bugets (citations). 


Methods to dectect change overtime: see summary https://lindeloev.github.io/mcp/articles/packages.html

- Historical data and ability to choose how much to sample

Accounting for these considerations in the design of a monitoring program is necessary to detect long-term ecological change. The specifics of the monitoring program will determine the power with which a question of interest can be addressed. For example, @White2019 found that 72\% of vertebrate populations required at least 10 years of monitoring to detect significant changes in the population size over time. Other work has focused on the frequency of monitoring (citations), the allocation of resources spatially versus temporally (citations), and the ability to use citizen science to increase monitoring (citations). Lastly, both the ecological and economical costs of failing to detect a true trend (type II error) have to be weighed against the risks of false (type I error) detection (citation).

Because of new technological advances, there are many data sources that can be derived long after the actual  processes occurred. For example, paleoecological lake cores can be extracted and historical ecological communities can be reconstructured (citations from Rosalie). Similarly, environmental samples (e.g. water, soil) can be saved with eDNA processed later (citation). Similarly, video-graphical approaches can record snapshots of a system and be analyzed later (citations). In each of these cases, decisions have to made about how much data to extract from the previously collected samples. Should the paleoecological core be analyzed at every centimeter? Should the video be assessed at every minute? As long as processsing samples is expensive, these trade-offs will remain.

Here, we develop a set of tools to determine the appropriate number of samples and sampling approach when dealing with data sources where additional samples can be added. We focus on paleoecological core samples as one example of this type of data. We examine the situation where the goal is to detect the time at which a significant change in an ecological community occurs, a changepoint. However, our approaches are widely applicable to other questions and data types. We first investigate these tools using a simulated-based approach to As a case study, we apply these tools to the detection of change points for a set of lakes. (more from Rosalie here).

# Sampling approaches and changepoint detection

For both our simulations and case study, we investigate the effect of different sampling strategies on our ability to detect a changepoint. We begin by either creating simulated time series or using an actual paleoecological time series (Fig. \ref{fig:conceptual}). We then subsampled each time series (citation) to test the effect of three different sampling approaches along with varying the sample size (Fig. \ref{fig:conceptual}c-f). We compared the subsampled time series to the full time series as a measure of the effectiveness of that sampling approach.

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=7, fig.height=7.5,eval=T,fig.cap='Conceptual diagram illustrating the process of taking (a) simulations of a time series and (b) selecting a single simulation to analyze with three different sampling approaches: (c) random, (d) regular, and (e) iterative. The iterative sampling approach requires (f) adding samples around a detected changepoint until (g) a certain level of accuracy is achieved. \\label{fig:conceptual}'}
source(paste0(getwd(),"/R/Fig_conceptual.R"))
```

The random sampling approach involves taking a set number of random points throughout the time series (Fig. \ref{fig:conceptual}c). In the context of sediment cores, this would mean analzying community composition as random spots along the core. We hypothesize that random sampling will perform the worst at getting as close as possible to a "real" changepoint. Regular sampling is commonly used (citations) and requires that a set number of samples be taken at regular intervals (Fig. \ref{fig:conceptual}d). Lastly, iterative sampling involves first taking a set number of samples and then iteratively adding samples until a pre-determined level of precision is achieved (Fig. \ref{fig:conceptual}e-g). 

Rosalie: Can you write a paragraph here detailing the changepoint detection process?





# Simulation approach

<!--
- explain the modeling approach
- show the model
- explain the analysis we will do
- explain the theoretical model results with a figure
- explain effect of different sampling strategies in this context

Theoretical plots

- One combined plot to see the effect (on prob correct or number of samples) of each parameter - should do this with one of the three types of sampling (probably regular)
- Theoretical plot comparing the three different methods - we could choose one set of parameter values and show how the various methods perform with different sampling amounts (could use same plot structure as the case study)
- SM plot that shows how the three different methods are affected by changes in the underlying parameter values

-->

## Theoretical model

We began with a theoretical exploration of the sampling requirements to detect a changepoint. We modeled a simple first order autoregressive (AR-1) process (the discrete-time version of the Ornstein–Uhlenbeck process) with a response variable that represents either population size, biodiversity, or some metric of community composition. The model includes temporal autocorrelation ($\phi$), the mean of the process ($\mu_X$), and a white noise term ($w_t$). The white noise term is a normal distribution with mean ($\mu_w$) and variance ($\sigma$).

$$
\begin{array}{rcl} 
X_t & = & \mu_X + \phi (X_{t-1} - \mu_X) + w_t \\ 
w_t & \sim & Norm(\mu_w,\sigma)
\end{array}
$$
We included a changepoint by shifting $\mu_X$ at time $\tau$ given a specific shift size ($\delta$).

We explored how each of these model parameters affected our ability to detect a change point. We simulate an entire time series (the "true" data for comparison), but we also examine the effect of not sampling every interval. We specifically study how the amount of samples included and the type of sampling affects the detection probability. For simulations, statistical power is the fraction of simulations that were able to detect a changepoint. We define a detection as an estimate within five time points (over 100 time points) of the true changepoint. We define $T_{min}$ as the minimum number of samples required to achieve 0.8 statistical power.


```{r theoretical_results,message=F,warning=F,echo=F,eval=F}

source('R/sampling_regular.R')
source('R/sampling_random.R')
source('R/build_time_series.R')
source('R/updated_param_sensitivity.R')

require(tidyverse)

par(mfrow=c(2,2))
# Sigma plot
sigma15 <- param_sensitivity('sigma',15)
plot(sigma15,ylab='Probability correct',xlab='sigma',pch=16,cex.lab=1.3,las=1,ylim=c(0,1),col=1)

sigma20 <- param_sensitivity('sigma',20)
points(sigma20,ylab='Probability correct',xlab='sigma',pch=15,cex.lab=1.3,las=1,ylim=c(0,1),col=3)

# phi plot
phi15 <- param_sensitivity('phi',15)
plot(phi15,ylab='Probability correct',xlab='phi',pch=16,cex.lab=1.3,las=1,ylim=c(0,1),col=1)

phi20 <- param_sensitivity('phi',20)
points(phi20,ylab='Probability correct',xlab='phi',pch=15,cex.lab=1.3,las=1,ylim=c(0,1),col=3)

```


```{r theoretical_results_min_time,message=F,warning=F,echo=F,eval=F}

source('R/sampling_regular.R')
source('R/sampling_random.R')
source('R/build_time_series.R')
source('R/updated_param_sensitivity.R')
source('R/calc_min_time2.R')
require(tidyverse)

# Sigma results
sigma_vec = seq(0.1,30,4)
min_time  = rep(0,times=length(sigma_vec))
sigma_output <- data.frame(sigma_vec,min_time)

for (param_num in 1:nrow(sigma_output)){
  sigma_output$min_time[param_num] <- calc_min_time('sigma',sigma_output$sigma_vec[param_num])
  print(sigma_output)
}

write.csv(x = sigma_output,file = 'Output/theoretical_results_sigma.csv',quote=FALSE)

# Phi results

phi_vec = seq(-0.8,0.8,0.2)
min_time  = rep(0,times=length(phi_vec))
phi_output <- data.frame(phi_vec,min_time)

for (param_num in 1:nrow(phi_output)){
  phi_output$min_time[param_num] <- calc_min_time('phi',phi_output$phi_vec[param_num])
  print(phi_output)
}

write.csv(x = phi_output,file = 'Output/theoretical_results_phi.csv',quote=FALSE)


# Shift size

shift_size_vec = rev(seq(0.1,0.9,0.025))
min_time  = rep(0,times=length(shift_size_vec))
shift_size_output <- data.frame(shift_size_vec,min_time)

for (param_num in 1:nrow(shift_size_output)){
  shift_size_output$min_time[param_num] <- calc_min_time('shift_size',shift_size_output$shift_size_vec[param_num])
  print(shift_size_output)
}

write.csv(x = shift_size_output,file = 'Output/theoretical_results_shift_size.csv',quote=FALSE)


```


```{r theoretical_results_min_time_plot, fig.cap='Minimum number of samples required for 0.8 statistical power given different levels of (a) temporal variability ($\\sigma$), (b) temporal autocorrelation ($\\phi$), and (c) shift size. The default parameters are $\\sigma = 3$, $\\phi = 0.5$, and a shift size $=0.75$. The exact timing of the true changepoint varied for each simulation between time steps 30 and 70. \\label{fig:theoretical_min_time}',message=F,warning=F,echo=F,eval=T}

par(mfrow=c(1,3))

sigma <- read.csv('Output/theoretical_results_sigma.csv',header=TRUE)
plot(sigma$sigma_vec,sigma$min_time,ylab='Minimum number of samples required',xlab='sigma',pch=16,cex.lab=1.3,las=1,ylim=c(20,50),col=1,type='o')


# Phi results

phi <- read.csv('Output/theoretical_results_phi.csv',header=TRUE)
plot(phi$phi_vec,phi$min_time,ylab=' ',xlab='phi',pch=16,cex.lab=1.3,las=1,ylim=c(20,50),col=1,type='o')


# Shift size results

shift_size <- read.csv('Output/theoretical_results_shift_size.csv',header=TRUE)
plot(shift_size$shift_size_vec,shift_size$min_time,ylab=' ',xlab='shift size',pch=16,cex.lab=1.3,las=1,ylim=c(20,50),col=1,type='o')


```



## Theoretical results 

In line with theory about monitoring, we found that the probability of correcting identifying a changepoint decreased with smaller levels of population variability ($\sigma$) and autocorrelation ($\phi$). We also found that the probability of correct changepoint detection increases with larger shift sizes, which is essentially the effect size. Thus, the number of sample required to obtain high statistical power (above 0.8) increased with larger population variability, lower autocorrelation, and smaller shift sizes (Fig. \ref{fig:theoretical_min_time}).


```{r theoretical_results_sampling_approaches,message=F,warning=F,echo=F,eval=F}

source('R/sampling_regular.R')
source('R/sampling_random.R')
source('R/build_time_series.R')
source('R/updated_param_sensitivity.R')
source('R/calc_min_time2.R')
require(tidyverse)

# Need to run each method on a set of simulated time series. We would then have 100 runs for each method and each sampling amount

approach <- c('random')#,'regular','iterative')
number_samples <- 5:30
simulation_number <- 1:50
output_theory_approach <- data.frame(expand.grid(approach,number_samples,simulation_number))
names(output_theory_approach) <- c('approach', 'number_samples', 'simulation_number')
output_theory_approach$distance_to_real = 0

# Build function that takes in approach, simulated time series, and number of samples. The function should then spit out the distance to the real changepoint

# Set of time series (need other params)
params <- tibble(shift_time = sample(30:70,max(simulation_number),replace = T))

time_series <- params %>%
  pmap(build_time_series) 

# Run through params

for (i in 1:nrow(output_theory_approach)){
  
  output_theory_approach$distance_to_real[i] <- abs(params$shift_time[output_theory_approach$simulation_number[i]] - calc_distance_to_real(output_theory_approach$approach[i],output_theory_approach$number_samples[i],time_series[[output_theory_approach$simulation_number[i]]]))
  #print(i)
}


write.csv(x = output_theory_approach,file = 'Output/theoretical_comparing_approaches.csv',quote = FALSE )
```








# Case study

## Case study background

An application of our study we foresee applies to paleosequences. Sediment cores are taken from aquatic environment to reconstruct past ecosystems, in order to extend our perspective on ecosystem trajectories. Samples can be taken before and after perturbation to compare community compostion (e.g., ), or at continuously to identify the timing of change, and contribution of environmental variable in driving these changes (e.g., @perga_high-resolution_2015).

We tested the three sampling methods on a real community time-series, from Lake Varese (IT). Lake Varese is a small (14.8 km^2^), deep (z$_{max}$= 26m), monomictic lake, in the subalpine region of north-western Italy (238 m asl). It underwent hyper-eutrophication over the 20^th^ century due to increase in nutrient loads from the watershed (ref). Nutrient status was responsible for restructuration of the lake communities across trophic levels (e.g., Crosta 1999, @bruel_seeking_2018). Air temperature is now driving changes in plankton communities (@bruel_seeking_2018).

Cladoceran assemblage was reconstructed at a 2-3 years resolution, from a 74-cm sediment core (VAR10-10) covering the 1816(±26)—2010 period (@bruel_seeking_2018). We defined the "true" changepoint as the one identified from the complete record, in 1983/1988. 
Changepoints were detected on first component computed from the data sub-sampled. We used the same ordination method as in Bruel et al (2018), but note that DCA is best used to conserve multiple gradients. As a result, several changepoints were detected on the first component (in 1926/1928, 1946/1948 and 1983/1988). We conducted the analysis on the second component as well, where only one changepoint was detected (in 1944/1946).

We used the full record (74 observation) and subsampled following the three strategies described earlier (random, regular, iterative), varrying the maximum number analyzed from 5 to 15. Iterative sampling final number could be lower than maximum number if both samples framing the shift were analyzed.

## Case study results

Random sampling perform the worst, as changepoint analysis was left to chance. Regular sampling provided good estimates from 8 samples (0-3 sample of absolute error), but detecting the true changepoint was left  to chance. Iterative sampling (Fig. \ref{fig:case_study_number_sample} (c)), where 5 samples are initially sampled, then other samples are slowly added, performed the best, as no more than 9 samples were ever necessary to get the true changepoint. Fig. \ref{fig:case_study_iterative_sampling} further illustrates the iterative sampling. Note that when the initial number of samples was set to 7 Fig. \ref{fig:case_study_number_sample} (c), square markers), the distance to the real changepoint was initially very high. Without any prior, the initial number sampled fell far away from the true changepoint. 

We tested how the three methods performed at detecting other changepoints (as three changepoints were detected in the initial study, @bruel_seeking_2018). Iterative sampling performed best, especially if an higher number of initial samples was chosen (7 samples out of 74, Supplementary Materials \textcolor{red}{2}).
The initial number of samples should be chosen depending on the objective of the study. If investigators want to find the biggest transition only, then a low number might be enough (include here results from theoritical results?). If investigators suspect or want to test for several changepoints, we recommend analyzing initially about 10% of the length of the time serie to have a good initial ranking.


```{r case study read data output, include=FALSE}
myoutput <- read.table(paste0(getwd(),"/Output/output_changepoint_varese.txt"))
myoutput$method <- factor(myoutput$method, levels=c('Random', 'Regular', 'Iterative'))

```


```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width = 7.2, fig.height= 3,fig.cap='Distance to real changepoint per total number of samples analyzed, following (a) random sampling, (b) regular sampling, and (c) iterative sampling. Total number of samples was set between 5 and 30, out of the 74 initial time series.  \\label{fig:case_study_number_sample}'}
library(ggplot2)
#devtools::install_github("rensa/stickylabeller")
library(stickylabeller)
p1 <- ggplot(myoutput[myoutput$target_cpt==1,], aes(final_n,diff_real, pch=factor(n2_init))) + 
  geom_hline(aes(yintercept=0), col=adjustcolor("black", alpha.f = .4)) + 
  geom_point(alpha = .3) +
  facet_wrap(~method,
             labeller = label_glue('({.l}) {method} sampling ')) + 
  theme_bw() +
  labs(x="Total number of samples analyzed", y="Distance to real changepoint") +
  theme(legend.position = c(0.93, 0.72),
        legend.background = element_rect(fill = "#ffffffaa", colour = NA),
        legend.title=element_text(size=7), 
        legend.text=element_text(size=8)) +
  guides(pch=guide_legend(title="Initial number\nof sample"))

p1


```


```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width = 8, fig.height= 7.5, fig.cap='Iterative sampling and impact on ordination. (a) Initial community dataset. Samples are initially sampled at regular interval. The multivariate data is converted to univariate vector by Detrended Component Analysis. Samples were iteratively added, after computation of changepoint at each step. (b) Full time series, on which "true" changepoint was detected. (c) First step of iterative sampling, n = 5. (d) Final step of iterative samples: by adding 4 samples, the true changepoint was detected. \\label{fig:case_study_iterative_sampling}'}

getpath4data <- function() {
  if(Sys.getenv("USER")=="Rosalie") return("/Volumes/-/Script R/Data-off-GitHub/temporal-sampling/")
  if(Sys.getenv("USER")=="eastonwhite") return("~/Desktop/Research/soil-temporal-sampling/")
  if(Sys.getenv("USER")!="Rosalie"|Sys.getenv("USER")!="put here your USER") stop("You need to get the data and define their location.")
}
# Read data
vr <- read.delim(paste0(getpath4data(),"VAR10-10-clado.txt"))

# Detect 'true' changepoint
vrall <- sample_regular(vr, n=nrow(vr), input_vector = F, xcol = 1, DCA_axis = 1, messages = F)
 vrall$matrix$age <- vr$AGE[vrall$matrix$index]

# Creating a dataset with all scores 
vr_out_matrix <- read.table(paste0(getwd(),"/Output/output_iterative_sampling_varese.txt"))

index_temp <- vr_out_matrix$index[vr_out_matrix$n==max(vr_out_matrix$n)]
vr_out_order <-
  data.frame("index" = index_temp,
             "age"   = vrall$matrix$age[index_temp],
             "x"     = vrall$matrix$x[index_temp],
             "count" = summary(as.factor(vr_out_matrix$index)))
vr_out_order$count <- abs(vr_out_order$count-max(vr_out_order$count)-1)



library(wesanderson)
mypal <- wes_palette("GrandBudapest2", max(vr_out_order$count),type = "continuous")
 
# Plot with all data
 p_init <- ggplot(vrall$matrix, aes(age, x)) + geom_point(alpha = .3) + geom_line(alpha = .3) +
  theme_bw()  +
      theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) + 
           geom_rect(mapping=aes(xmin=vrall$matrix$age[which(vrall$matrix$index==vrall$changepoint)-1], xmax=vrall$matrix$age[which(vrall$matrix$index==vrall$changepoint)], ymin=-1, ymax=2), alpha=0.05, fill="grey") +
   labs(subtitle = paste0("b. Full time-series, n = ", nrow(vr)), x="Year", y="DCA first component",
        color='samples added\nat iteration t=') +
   geom_point(vr_out_order, mapping=aes(age, x, color = as.factor(count)), cex=2) +
   geom_text(vr_out_order, mapping=aes(age, x, label =count), vjust = 1.5, nudge_x = 1.5) +
   scale_color_manual(values=mypal) 



 
# Initial plot, then Adding samples
for (i in 5:12) {
  # Start a new loop only if the final changepoint hasn't been found at the previous iteration.  
  if (i==5 || vrsub$matrix$index[vrsub$matrix$index==vrsub$changepoint] - vrsub$matrix$index[which(vrsub$matrix$index==vrsub$changepoint)-1]>1) {
    
  vrsub <- sample_iterative(vr, n=i, n2=5, input_vector = F, xcol = 1, DCA_axis = 1, messages = F)
  vrsub$matrix$age <- vr$AGE[vrsub$matrix$index]
  vrsub$matrix$count <- vr_out_order$count[vr_out_order$age %in% vrsub$matrix$age]
  
    if (i==5) {
      vrsub1 <- vrsub
    p1 <- ggplot(vrsub$matrix, aes(age, x, color=as.factor(count))) +  geom_line(alpha = .4, col="grey") + geom_point() + guides(col=FALSE) +
   scale_color_manual(values=mypal) +
  theme_bw() +
      theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
   labs(x="Year", y="DCA first component",
        color='samples added\nat iteration t=')
    p2 <- p1
  } else {
    if (i == 6) {
      p2 <- p1 + 
        geom_line(vrsub$matrix, mapping=aes(age, x), alpha = .4, col="grey") + 
        geom_point(vrsub$matrix, mapping=aes(age, x,color=as.factor(count)))   + guides(col=FALSE) 
    } else {
        p2 <- p2 + geom_line(vrsub$matrix, mapping=aes(age, x), alpha = .4, col="grey") + 
          geom_point(vrsub$matrix, mapping=aes(age, x,color=as.factor(count))) + 
          guides(col=FALSE)
      }
  }
      # Save plot
    ggsave(p2 + 
             geom_rect(stat = "identity", mapping=aes(xmin=vrsub$matrix$age[which(vrsub$matrix$index==vrsub$changepoint)-1], xmax=vrsub$matrix$age[which(vrsub$matrix$index==vrsub$changepoint)], ymin=-1, ymax=3), col=NA, fill="grey", alpha=0.05) +
             annotate("text", label = paste0("Number of samples = ",i), x = min(vrsub$matrix$age), y = 2.8, colour = "black",  hjust=0,vjust=0), 
           filename = paste0("Output/GIF_adding_sample_case_study/Iterative_",i,".pdf"), width = 6, height = 4)

  }
}

p1 <- p1 + geom_line(vrsub1$matrix, mapping=aes(age, x), alpha = .8, col="black") + 
          geom_point(vrsub1$matrix, mapping=aes(age, x,color=as.factor(count))) + geom_rect(stat = "identity", mapping=aes(xmin=vrsub1$matrix$age[which(vrsub1$matrix$index==vrsub1$changepoint)-1], xmax=vrsub1$matrix$age[which(vrsub1$matrix$index==vrsub1$changepoint)], ymin=-1, ymax=3), col=NA, fill="grey", alpha=0.05) +
  labs(subtitle="c. Initial number of samples, n = 5")

p2 <- p2 + geom_line(vrsub$matrix, mapping=aes(age, x), alpha = .8, col="black") + 
          geom_point(vrsub$matrix, mapping=aes(age, x,color=as.factor(count))) + 
  geom_rect(stat = "identity", mapping=aes(xmin=vrsub$matrix$age[which(vrsub$matrix$index==vrsub$changepoint)-1], xmax=vrsub$matrix$age[which(vrsub$matrix$index==vrsub$changepoint)], ymin=-1, ymax=3), col=NA, fill="grey", alpha=0.05) +
  labs(subtitle=paste0("d. n =", nrow(vrsub$matrix)))


# Fake dataset to explain sample selection
fakedata <- data.frame(matrix(rep(NA,74*10), nrow=74))
colnames(fakedata) <- 1:ncol(fakedata)
fakedata[is.na(fakedata)] <- 0
fakedata[vr_out_order$index[vr_out_order$count==1],] <- 1 
fakedata[vr_out_order$index[vr_out_order$count==2],] <- 2 
fakedata[vr_out_order$index[vr_out_order$count==3],] <- 3 
fakedata[vr_out_order$index[vr_out_order$count==4],] <- 4 
fakedata[vr_out_order$index[vr_out_order$count==5],] <- 5 
fakedata[,1] <- 6
fakedata <- rbind(rep(7,ncol(fakedata)),fakedata)
library(reshape)
fakedata <- cbind(Year=rep(75:1,ncol(fakedata)), melt(fakedata))
head(fakedata)

p3 <- ggplot(fakedata, aes(variable,Year)) +
  geom_tile(aes(fill = as.factor(value)), colour="black") +
  scale_fill_manual(values=c("white",mypal,grey(.8), grey(.8))) + guides(fill=FALSE)  + theme(panel.border = element_blank(), panel.grid.major = element_blank(),
panel.grid.minor = element_blank(), axis.line = element_blank(), panel.background = element_blank(), axis.title=element_blank(),
        axis.text=element_blank(),
        axis.ticks=element_blank()) +
  geom_text(fakedata[fakedata$Year==75,], mapping = aes(variable,Year, label=c("Year", paste("Sp.", 1:9))),  size=2.5,hjust=0,angle = 90) +
  labs(subtitle="a. Continuous community dataset")

# Assemble plot
library(patchwork)

 p3 | (p_init / p1 / p2)


```


__Main point__: Changepoint was detected even though we used a transformed vector (from community to first component of DCA) 






# General Discussion

Outline:

- theoretical results
- example with the case study
- how this fits in with other work
- limitations and future work


Other dataset that could use our method, e.g., paleo science in general? see review by Wingard et al 2017 https://www.frontiersin.org/articles/10.3389/fevo.2017.00011/full#B140



Case study: With the iterative method, we found the main changepoint with maximum 9 samples analyzed, with a starting point of 5 randomly distributed samples. Same results for the major change, with only 12% of the samples analyzed. However, need more samples to detect further changes. __Note that it would be possible to edit the function to go ahead and detect next changes once the main one has been detected. Is it worth coding? Shouldn't be too hard to do__

\clearpage

Limitations:

- It is not limited to paleo data
- Depends on the question being asked
- Specific number of samples depends on the system dynamics

Systematic sampling of a time-series is required to detect critical slowing-down or early warning of shifts (ref, maybe among others Frossard et al 2015, Doncaster et al 2016). The lower number of input data prior to ordination did not impact the finding of the main transition. However, if other metrics are of interest to the investigators, such as species richness, a more systematic or at least more extensive approach would be require.


Concluding paragraph:

About recommendation to reduce the frequency of data collected as part of monitoring program:  
While long-term monitoring program are often expensive, these cost are nothing compared to the value of the resources monitored and the policies affecting these resources (@lovett_who_2007). Monitoring programs should try to anticipate the potential questions of tomorrow, and reducing the data collected must be done with the best foresight possible on how these data may indeed not be necessary to manage ecosystems in the future.  


# References