---
bibliography: ChangepointSampling.bib
csl: ecology-letters.csl
editor_options:
  chunk_output_type: console
fontsize: 12pt
geometry: margin=1in
header-includes: \usepackage{float} \usepackage{lineno} \usepackage{setspace}\doublespacing
  \usepackage[round]{natbib} \bibpunct[; ]{(}{)}{,}{a}{}{,} \usepackage{color} \usepackage{totcount}
  \newtotcounter{citenum} \def\oldcite{} \let\oldcite=\bibcite \def\bibcite{\stepcounter{citenum}\oldcite}
  \renewcommand{\thepage}{S\arabic{page}}
  \renewcommand{\thesection}{Appendix S\arabic{section}} \renewcommand{\thetable}{S\arabic{table}}
  \renewcommand{\thefigure}{S\arabic{figure}}
  \usepackage{amsmath}
  \renewcommand{\theequation}{S\arabic{equation}} \floatplacement{figure}{H}
output:
  pdf_document:
    fig_caption: yes
    keep_tex: yes
    number_sections: no
---

```{r setup, include=FALSE}
knitr::opts_knit$set(root.dir = dirname(getwd()))
```




\begin{center}
\textbf{\Large Supplementary material for: Sampling requirements and approaches to detect ecosystem shifts}
\vspace{5 mm}
	
\textsc{Rosalie Bruel$^{*}$\footnote{*The authors contributed equally to this work. $\dagger$Corresponding author (eastonrwhite@gmail.com)} and Easton R. White$^{* \dagger 2,3}$}
\vspace{3 mm}

\normalsize{\indent $^{1}$Rubenstein Ecosystem Science Laboratory, University of Vermont, VT, USA \\ $^{2}$Department of Biology, University of Vermont, VT, USA \\
$^{3}$Gund Institute for Environment, University of Vermont, VT, USA}
\end{center}

\vspace{2 mm}

\tableofcontents

\vspace{1cm}

Data and code for all the figures can be found at (https://github.com/rosalieb/temporal-sampling). All the analyses were run in R [@RCoreTeam2019].


\clearpage


# Additional methods

We first detect the "true" changepoint of a full length time series with the function _e.divisive_ in the R package _ecp_ [@James2019]. We focus on the changepoint with the largest magnitude, although this package allows detection of further changepoints as well (Fig. \ref{fig:case_study_other_changepoints}). We then subsample the full time series [@White2020] with different numbers of subsamples and different sampling approaches. For each sampling approach, we wrote a custom function:

- sample_random(): sample first and last sample, in addition of n-2 random samples in between (n = maximum number of sample, chosen by the user),  
- sample_regular(): sample first and last sample, in addition of n-2 evenly spaced samples in between (n = maximum number of samples, chosen by the user), and  
- sample_iterative(): sample first and last sample as well as 3 evenly spaced samples in between (3 is the default but can be modified), in order to initiate the changepoint detection. Upon detection of the changepoint on this 5 samples time series, a new sample is added between the changepoint and the previous sample, to narrow down the real changepoint. Detection of changepoint and addition of sample is repeated, until it was narrowed down to two consecutive samples, or n (maximum number of samples, chosen by the user) was reached, whichever comes first.  
  
The subsampled time series are then compared to the full time series to assess the effectiveness of each subsampling approach.

Each function works with two types of input: vector and matrix. If the input is a matrix, the user must change the default argument *is_vector* to *FALSE*. The matrix is then transformed to independent vectors using Detrended Correspondence Analysis [@Hill1980], and a single axis on which changepoint analysis is run is chosen with the argument *DCA_axis* (default to 1, for first component). Component scores are returned by the function. A user can edit the function to use another ordination method (e.g., principal component analysis).

<!--
# Parameter sensitivity

We investigated how different parameters change the power of the analysis, and found that autocorrelation at lag-1 (phi) had little impact on the power of the analysis amplitude of the shift was the most important determinant of the power of the analysis. For null standard deviation, the analysis performed well independently of the shift size or autocorrelation. When standard deviation increased, the power of the analysis was higher for null autocorrelation (Fig. \ref{fig:parameter_sensitivity_plot}).
-->


# Minimum time for other sampling approaches

## Random sampling

```{r simulation_min_time_random,message=F,warning=F,echo=F,eval=F,cache=T}

source('R/sampling_random.R')
source('R/build_time_series.R')
source('R/updated_param_sensitivity.R')
source('R/calc_min_time2.R')
require(tidyverse)
require(stickylabeller)
require(ggplot2)
require(viridis)

# Sigma results
sigma_vec = seq(0.01,0.4,0.025)
min_time  = rep(0,times=length(sigma_vec))
sigma_output <- data.frame(sigma_vec,min_time)

for (param_num in 1:nrow(sigma_output)){
  sigma_output$min_time[param_num] <- calc_min_time('sigma',sigma_output$sigma_vec[param_num],'random')
  #print(sigma_output)
}

write.csv(x = sigma_output,file = 'Output/theoretical_results_random_sigma.csv',quote=FALSE)

# Phi results

phi_vec = seq(-0.8,0.8,0.05)
min_time  = rep(0,times=length(phi_vec))
phi_output <- data.frame(phi_vec,min_time)

for (param_num in 1:nrow(phi_output)){
  phi_output$min_time[param_num] <- calc_min_time('phi',phi_output$phi_vec[param_num],'random')
  #print(phi_output)
}

write.csv(x = phi_output,file = 'Output/theoretical_results_random_phi.csv',quote=FALSE)


# Shift size

shift_size_vec = rev(seq(0.1,0.9,0.05))
min_time  = rep(0,times=length(shift_size_vec))
shift_size_output <- data.frame(shift_size_vec,min_time)

for (param_num in 1:nrow(shift_size_output)){
  shift_size_output$min_time[param_num] <- calc_min_time('shift_size',shift_size_output$shift_size_vec[param_num],'random')
  #print(shift_size_output)
}

write.csv(x = shift_size_output,file = 'Output/theoretical_results_random_shift_size.csv',quote=FALSE)


```



```{r simulation_min_time_plot_random, fig.width = 7.2, fig.height= 3, fig.cap='Minimum number of samples required for 0.8 statistical power given different levels of (a) temporal variability ($\\sigma$), (b) temporal autocorrelation ($\\phi$), and (c) shift size. Random sampling was used with the default parameters are $\\sigma = 0.53$, $\\phi = 0.404$, and a shift size $=0.81$ to match the case study. The exact timing of the true changepoint varied for each simulation between time steps 30 and 70. Each vertical line indicates the respective parameter calculated from the case study time series. \\label{fig:simulation_min_time_random}',message=F,warning=F,echo=F,eval=F}
require(ggplot2)
require(dplyr)
require(stickylabeller)
par(mfrow=c(1,3))

sigma <- read.csv('Output/theoretical_results_random_sigma.csv',header=TRUE)

phi <- read.csv('Output/theoretical_results_random_phi.csv',header=TRUE)

shift_size <- read.csv('Output/theoretical_results_random_shift_size.csv',header=TRUE)


# Melt the 3 dataset to be able to use facet_wrap()
colnames(sigma) <- colnames(phi) <- colnames(shift_size) <- c("X", "Param", "min_time")
simulation_all <- 
  rbind(cbind("Parameter" = rep("Standard deviation", nrow(sigma)), sigma),
        cbind("Parameter" = rep("Autocorrelation at lag-1", nrow(phi)), phi),
        cbind("Parameter" = rep("Normalized amplitude of shift", nrow(shift_size)), shift_size))
# param_vr <- data.frame("Parameter" = unique(simulation_all$Parameter),
#                        "value" = c(ar_vr, cv_vr, shift_vr))

p1 <- 
  ggplot(simulation_all %>% filter(Parameter == unique(simulation_all$Parameter)[1]), aes(Param, min_time)) +
  geom_point() +
  geom_line() +
  geom_vline(xintercept=0.05, lty=2) + 
  facet_wrap(~Parameter, scales="free_x", 
             labeller = label_glue('a. {Parameter}')) +
  ylim(10,52) +
  labs(x=c("sigma"), y= "Minimum number of samples required") + 
  theme_bw() +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        strip.background = element_rect(colour="transparent", fill="transparent"),
        strip.text.x = element_text(angle = 0, hjust = 0))
p2 <- ggplot(simulation_all %>% filter(Parameter == unique(simulation_all$Parameter)[2]), aes(Param, min_time)) +
  geom_point() +
  geom_line() +
  geom_vline(xintercept = 0.404, lty=2) +
  facet_wrap(~Parameter, scales="free_x", 
             labeller = label_glue('b. {Parameter}')) +
  ylim(10,52) +
  labs(x="phi", y= "") + 
  theme_bw() +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        strip.background = element_rect(colour="transparent", fill="transparent"),
        strip.text.x = element_text(angle = 0, hjust = 0))
p3 <- ggplot(simulation_all %>% filter(Parameter == unique(simulation_all$Parameter)[3]), aes(Param, min_time))  +
  geom_point() +
  geom_line() + 
  geom_vline(xintercept = 0.81, lty=2) +
  facet_wrap(~Parameter, scales="free_x", 
             labeller = label_glue('c. {Parameter}')) +
  ylim(10,52) +
  labs(x="shift size", y= "") + 
  theme_bw() +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        strip.background = element_rect(colour="transparent", fill="transparent"),
        strip.text.x = element_text(angle = 0, hjust = 0))


# Remove the y-axis for the 2nd plot - p2 then merge 2 plots
cowplot::plot_grid(p1 + theme(plot.margin = unit(c(0.15, 0.1, 0.15, 0.15), "cm")), 
                   p2 + 
                     theme(plot.margin = unit(c(0, 0.1, 0, 0), "cm"),
                           axis.text.y = element_blank(),
                           axis.line.y = element_blank(),
                           axis.title.y= element_blank(),
                           axis.ticks.y= element_blank()),
                   p3 + 
                     theme(plot.margin = unit(c(0, 0.1, 0, 0), "cm"),
                           axis.text.y = element_blank(),
                           axis.line.y = element_blank(),
                           axis.title.y= element_blank(),
                           axis.ticks.y= element_blank()),
                   nrow = 1,
                   rel_widths = c(1.2, 1,1),
                   align = 'h', axis = 'tb')

```


```{r parameter_sensitivity_plot_random,echo=F,message=F,warnings=F,fig.height=4,fig.cap='Random sampling statistical power (fraction of 100 simulations which detected a changepoint within five time points of the true changepoint) for different levels of standard deviation ($\\sigma$), lag-1 autocorrelation ($\\phi$), and shift size ($\\delta$). For each parameter combination, 20 samples were used. An increase in samples would increase the statistical power across this graph.\\label{fig:parameter_sensitivity_plot_random}'}
output_theory_approach <- read.csv(file = 'Output/parameter_sensitivity_random.csv')

require(ggplot2)
require(viridis)
ggplot(aes(x=phi_vec,y=sigma_vec,fill=power),data=output_theory_approach) + geom_tile() +
  scale_fill_viridis(discrete=FALSE,begin = 0.1,end=0.9, limits=c(0,1)) +
  facet_wrap(~shift_size_vec) + 
  theme_classic(base_size = 14, base_family = "")+
  theme(plot.subtitle = element_text(hjust = 0.5)) +
  labs(fill='Power',subtitle = 'Shift size') +
  xlab("Lag-1 autocorrelation (phi)") +
  ylab("Standard deviation (sigma)")
```

## Iterative sampling

```{r simulation_min_time_iterative,message=F,warning=F,echo=F,eval=F,cache=T}


source('R/sampling_iterative.R')
source('R/build_time_series.R')
#source('R/updated_param_sensitivity.R')
source('R/calc_min_time2.R')
require(tidyverse)

# Sigma results
sigma_vec = seq(0.01,0.4,0.025)
min_time  = rep(0,times=length(sigma_vec))
sigma_output <- data.frame(sigma_vec,min_time)

for (param_num in 1:nrow(sigma_output)){
  sigma_output$min_time[param_num] <- calc_min_time('sigma',sigma_output$sigma_vec[param_num],'iterative')
  print(sigma_output)
}

write.csv(x = sigma_output,file = 'Output/theoretical_results_iterative_sigma.csv',quote=FALSE)

# Phi results

phi_vec = seq(-0.8,0.8,0.05)
min_time  = rep(0,times=length(phi_vec))
phi_output <- data.frame(phi_vec,min_time)

for (param_num in 1:nrow(phi_output)){
  phi_output$min_time[param_num] <- calc_min_time('phi',phi_output$phi_vec[param_num],'iterative')
  #print(phi_output)
}

write.csv(x = phi_output,file = 'Output/theoretical_results_iterative_phi.csv',quote=FALSE)


# Shift size

shift_size_vec = rev(seq(0.1,0.9,0.05))
min_time  = rep(0,times=length(shift_size_vec))
shift_size_output <- data.frame(shift_size_vec,min_time)

for (param_num in 1:nrow(shift_size_output)){
  shift_size_output$min_time[param_num] <- calc_min_time('shift_size',shift_size_output$shift_size_vec[param_num],'iterative')
  #print(shift_size_output)
}

write.csv(x = shift_size_output,file = 'Output/theoretical_results_iterative_shift_size.csv',quote=FALSE)


```



```{r simulation_min_time_plot_iterative, fig.width = 7.2, fig.height= 3, fig.cap='Minimum number of samples required for 0.8 statistical power given different levels of (a) temporal variability ($\\sigma$), (b) temporal autocorrelation ($\\phi$), and (c) shift size. Iterative sampling was used with the default parameters are $\\sigma = 0.53$, $\\phi = 0.404$, and a shift size $=0.81$ to match the case study. The exact timing of the true changepoint varied for each simulation between time steps 30 and 70. Each vertical line indicates the respective parameter calculated from the case study time series. \\label{fig:simulation_min_time}',message=F,warning=F,echo=F,eval=F}

par(mfrow=c(1,3))

sigma <- read.csv('Output/theoretical_results_iterative_sigma.csv',header=TRUE)

phi <- read.csv('Output/theoretical_results_iterative_phi.csv',header=TRUE)

shift_size <- read.csv('Output/theoretical_results_iterative_shift_size.csv',header=TRUE)


# Melt the 3 dataset to be able to use facet_wrap()
colnames(sigma) <- colnames(phi) <- colnames(shift_size) <- c("X", "Param", "min_time")
simulation_all <- 
  rbind(cbind("Parameter" = rep("Standard deviation", nrow(sigma)), sigma),
        cbind("Parameter" = rep("Autocorrelation at lag-1", nrow(phi)), phi),
        cbind("Parameter" = rep("Normalized amplitude of shift", nrow(shift_size)), shift_size))
# param_vr <- data.frame("Parameter" = unique(simulation_all$Parameter),
#                        "value" = c(ar_vr, cv_vr, shift_vr))

p1 <- 
  ggplot(simulation_all %>% filter(Parameter == unique(simulation_all$Parameter)[1]), aes(Param, min_time)) +
  geom_point() +
  geom_line() +
  geom_vline(xintercept=0.05, lty=2) + 
  facet_wrap(~Parameter, scales="free_x", 
             labeller = label_glue('a. {Parameter}')) +
  ylim(10,50) +
  labs(x=c("sigma"), y= "Minimum number of samples required") + 
  theme_bw() +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        strip.background = element_rect(colour="transparent", fill="transparent"),
        strip.text.x = element_text(angle = 0, hjust = 0))
p2 <- ggplot(simulation_all %>% filter(Parameter == unique(simulation_all$Parameter)[2]), aes(Param, min_time)) +
  geom_point() +
  geom_line() +
  geom_vline(xintercept = 0.404, lty=2) +
  facet_wrap(~Parameter, scales="free_x", 
             labeller = label_glue('b. {Parameter}')) +
  ylim(10,50) +
  labs(x="phi", y= "") + 
  theme_bw() +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        strip.background = element_rect(colour="transparent", fill="transparent"),
        strip.text.x = element_text(angle = 0, hjust = 0))
p3 <- ggplot(simulation_all %>% filter(Parameter == unique(simulation_all$Parameter)[3]), aes(Param, min_time))  +
  geom_point() +
  geom_line() + 
  geom_vline(xintercept = 0.81, lty=2) +
  facet_wrap(~Parameter, scales="free_x", 
             labeller = label_glue('c. {Parameter}')) +
  ylim(10,50) +
  labs(x="shift size", y= "") + 
  theme_bw() +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        strip.background = element_rect(colour="transparent", fill="transparent"),
        strip.text.x = element_text(angle = 0, hjust = 0))


# Remove the y-axis for the 2nd plot - p2 then merge 2 plots
cowplot::plot_grid(p1 + theme(plot.margin = unit(c(0.15, 0.1, 0.15, 0.15), "cm")), 
                   p2 + 
                     theme(plot.margin = unit(c(0, 0.1, 0, 0), "cm"),
                           axis.text.y = element_blank(),
                           axis.line.y = element_blank(),
                           axis.title.y= element_blank(),
                           axis.ticks.y= element_blank()),
                   p3 + 
                     theme(plot.margin = unit(c(0, 0.1, 0, 0), "cm"),
                           axis.text.y = element_blank(),
                           axis.line.y = element_blank(),
                           axis.title.y= element_blank(),
                           axis.ticks.y= element_blank()),
                   nrow = 1,
                   rel_widths = c(1.2, 1,1),
                   align = 'h', axis = 'tb')

```


```{r parameter_sensitivity_plot_iterative,echo=F,message=F,warnings=F,fig.height=4,fig.cap='Iterative sampling statistical power (fraction of 100 simulations which detected a changepoint within five time points of the true changepoint) for different levels of standard deviation ($\\sigma$), lag-1 autocorrelation ($\\phi$), and shift size ($\\delta$). For each parameter combination, 20 samples were used. An increase in samples would increase the statistical power across this graph.\\label{fig:parameter_sensitivity_plot_iterative}'}
output_theory_approach <- read.csv(file = 'Output/parameter_sensitivity_iterative.csv')

ggplot(aes(x=phi_vec,y=sigma_vec,fill=power),data=output_theory_approach) + geom_tile() +
  scale_fill_viridis(discrete=FALSE,begin = 0.1,end=0.9) +
  facet_wrap(~shift_size_vec) + 
  theme_classic(base_size = 14, base_family = "")+
  theme(plot.subtitle = element_text(hjust = 0.5)) +
  labs(fill='Power',subtitle = 'Shift size') +
  xlab("Lag-1 autocorrelation (phi)") +
  ylab("Standard deviation (sigma)")
```




# Detecting further changepoints

The initial study found 3 changepoints on the main axis, in 1926/1928, 1946/1948 and 1983/1988 [@Bruel2018]. We focused our analysis on the main changepoint, but the code also allows to target further changepoints, using the argument *c*. *c* is a numeric value indicating the maximum number of changepoints to find (Fig. \ref{fig:case_study_other_changepoints}).

The iterative method requires less subsamples overall to find the real changepoints (Fig. \ref{fig:case_study_other_changepoints}, 3rd column). Note that if the initial number of samples (controlled with the argument *n2* in the code) is at the default 5, the code struggles to rank appropriately the changepoints, resulting in a changepoint being found, but not the overall "true" second  (e.g., Fig. \ref{fig:case_study_other_changepoints}b, 3rd column), or "true" third. For a time-series, we recommend users examine the p-values returned by the *e.divisive* function, embedded in the function *sample_iterative* output. If the first few changepoints have close, low p-values, it may be useful to add another subsample further away from the presumed changepoint. This avoids the iterative algorithm mistakenly converging on the wrong changepoint. An alternative is to initiate the subsampling with an higher number of samples, e.g., 7 subsamples.

```{r case study read data output, include=FALSE}
myoutput <- read.table(paste0(getwd(),"/Output/output_changepoint_varese.txt"))
myoutput$method_f <- factor(myoutput$method, levels=c('Random', 'Regular', 'Iterative'))

```


```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width = 7.2, fig.height= 8,fig.cap='Distance to true (a) first changepoint (largest magnitude), (b) second changepoint, and (c) third changepoint for total number of samples analyzed, following random sampling, regular sampling, and iterative sampling. Total number of samples was set between 5 and 30, out of the 74 initial time series.  \\label{fig:case_study_other_changepoints}'}
library(ggplot2)
library(gridExtra)
p1 <- ggplot(myoutput[myoutput$target_cpt==1,], aes(final_n,diff_real, pch=factor(n2_init))) + 
  geom_hline(aes(yintercept=0), col=adjustcolor("black", alpha.f = .4)) + 
  geom_point(alpha = .3) +
  facet_wrap(~method_f) + theme_bw() +
  labs(title="a. First changepoint",x="", y="Distance to real changepoint") +
  theme(legend.position = c(0.93, 0.6),
        legend.background = element_rect(fill = "#ffffffaa", colour = NA),
        legend.title=element_text(size=7), 
        legend.text=element_text(size=8)) +
  guides(pch=guide_legend(title="Initial number\nof sample"))
p2 <- ggplot(myoutput[myoutput$target_cpt==2,], aes(final_n,diff_real, pch=factor(n2_init))) + 
  geom_hline(aes(yintercept=0), col=adjustcolor("black", alpha.f = .4)) + 
  geom_point(alpha = .3) +
  facet_wrap(~method_f) + theme_bw() +
  labs(title="b. Second changepoint",x="", y="Distance to real changepoint") +
  theme(legend.position = c(0.93, 0.6),
        legend.background = element_rect(fill = "#ffffffaa", colour = NA),
        legend.title=element_text(size=7), 
        legend.text=element_text(size=8)) +
  guides(pch=guide_legend(title="Initial number\nof sample"))
p3 <- ggplot(myoutput[myoutput$target_cpt==3,], aes(final_n,diff_real, pch=factor(n2_init))) + 
  geom_hline(aes(yintercept=0), col=adjustcolor("black", alpha.f = .4)) + 
  geom_point(alpha = .3) +
  facet_wrap(~method_f) + theme_bw() +
  labs(title="c. Third changepoint",x="", y="Distance to real changepoint") +
  theme(legend.position = c(0.93, 0.6),
        legend.background = element_rect(fill = "#ffffffaa", colour = NA),
        legend.title=element_text(size=7), 
        legend.text=element_text(size=8)) +
  guides(pch=guide_legend(title="Initial number\nof sample"))
grid.arrange(p1,p2,p3,nrow=3)
```














# Case study: Changepoint detection of abundance time series

```{r echo=FALSE, eval=TRUE, message=FALSE, warning=FALSE}
# Investigating change in species average biomass
# Read code
source('R/sampling_regular.R')
# Read data
getpath4data <- function() {
  if(Sys.getenv("USER")=="Rosalie") return("/Volumes/-/Script R/Data-off-GitHub/temporal-sampling/")
  if(Sys.getenv("USER")=="eastonwhite") return("~/Desktop/Research/soil-temporal-sampling/")
  if(Sys.getenv("USER")!="Rosalie"|Sys.getenv("USER")!="put here your USER") stop("You need to get the data and define their location.")
}
vr <- read.delim(paste0(getpath4data(),"VAR10-10-clado.txt"))
vr_count <- data.frame(Year=vr[,1], Biomass=rowSums(vr[,-1]))
cpvar_count <- sample_regular(vr_count[,2], nrow(vr_count), input_vector = T, messages = F)

```

As another example of our approach, we we re-used the community dataset from Lake Varese, but this time, looked at the total number of individuals per gram of sediment in each subsamples. Thus, this is a univariate time series as opposed to the community-level data used in the main text. The full time series show a changepoint in `r paste(vr_count$Year[cpvar_count$changepoint], vr_count$Year[cpvar_count$changepoint-1], sep="/")`. We used the three changepoint method to estimate changepoint on the vector, and found that the iterative method, with an initial number of 5 regularly-spaced subsamples, performed best (Fig. \ref{fig:case_study_number_sample_biomass}).


```{r echo=FALSE, eval=TRUE, message=FALSE, warning=FALSE, fig.width = 4, fig.height= 3, fig.cap='Number of individuals per gram of sediment, infered from Cladocera remains for the 74 subsamples of the Lake Varese sediment core. Vertical grey band indicates the "true" changepoint.   \\label{fig:case_study_biomass_per_year}'}
# plot
ggplot(vr_count, aes(Year, Biomass)) + 
  theme_bw() +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        strip.background = element_rect(colour="transparent", fill="transparent")) +
           geom_rect(mapping=aes(xmin=vr_count$Year[cpvar_count$changepoint-1], xmax=vr_count$Year[cpvar_count$changepoint], ymin=0, ymax=max(vr_count)), alpha=0.05, fill="grey") + 
  geom_line() +
  labs(y="Number of individuals / g")

```


```{r echo=FALSE, eval=T,message=FALSE, warning=FALSE, fig.width = 7.2, fig.height= 3,fig.cap='Distance to real changepoint in biomass per total number of samples analyzed, following (a) random sampling, (b) regular sampling, and (c) iterative sampling. Total number of samples was set between 5 and 30, out of the 74 initial time series.   \\label{fig:case_study_number_sample_biomass}'}

require(stickylabeller)

myoutput <- read.table(paste0(getwd(),"/Output/output_changepoint_varese_biomass.txt"))
myoutput$method <- factor(myoutput$method, levels=c('Random', 'Regular', 'Iterative'))
myoutput$diff_real <- abs(myoutput$diff_real)


# Plot
p1 <-
  ggplot(myoutput[myoutput$target_cpt==1 & myoutput$n2_init == 5,], aes(final_n,diff_real)) + 
  geom_hline(aes(yintercept=0), col=adjustcolor("black", alpha.f = .4)) + 
  geom_point(alpha = .3) +
  facet_wrap(~method,
             labeller = label_glue('{.l}. {method} sampling ')) + 
  theme_bw() +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        strip.background = element_rect(colour="transparent", fill="transparent"),
        strip.text.x = element_text(angle = 0, hjust = 0)) +
  labs(x="Total number of samples analyzed", y="Distance to real changepoint") 


p1


```


# Case study: Testing other types of models

Throughout the main text and supplementary material, we focus on detecting changepoints. However, other types of models and criteria may be more appropriate depending on the question. This is especially true for situations where changepoints are not expected _a priori_. To address this, we used our random and regular sampling approaches with two additional models: linear models and generalized additive models. For simple linear regression, we measured the percent difference in the estimated slope between the subsampled data and the full time series (Fig. \ref{fig:lineartrends}). This is similar to past work [@White2019e; @White2020] on detecting population trends. 

```{r echo=FALSE, eval=T,message=FALSE, warning=FALSE, fig.width = 7.2, fig.height= 3,fig.cap='Percent difference for the estimated slope from linear regression for the abundance time series using (a-d) random and (e-h) regular sampling. Iterative sampling could be used for linear trends, but would require additional decisions on how to add samples. \\label{fig:lineartrends}'}

vr <- read.delim(paste0(getpath4data(),"VAR10-10-clado.txt"))
vr_count <- data.frame(Year=vr[,1], Biomass=rowSums(vr[,-1]))
cpvar_count <- sample_regular(vr_count[,2], nrow(vr_count), input_vector = T, messages = F)

#plot(vr_count)
#abline(lm(vr_count$Biomass~vr_count$Year))
true_slope <- as.numeric(coef(lm(vr_count$Biomass~vr_count$Year))[2])

regular_linear <- function(x,y,num_samples){
  true_slope <- as.numeric(coef(lm(y~x))[2])
  
  subset_index <- c(seq(from = 1, to = length(x),length.out =num_samples),length(x))
  subset_x <- x[subset_index]
  subset_y <- y[subset_index]
  
  subset_slope <- as.numeric(coef(lm(subset_y~subset_x))[2])

 return(abs(true_slope - subset_slope)/true_slope)  
}

spacing_vec <- seq(3,70,by=3)
distance <- vector(mode='numeric',length = length(spacing_vec))
for (i in 1:length(spacing_vec)){
  distance[i] <- regular_linear(x = vr_count$Year,y=vr_count$Biomass,num_samples =spacing_vec[i])
  
}
par(mfrow=c(1,2),mar=c(0.5,0.1,1.5,0.1),oma=c(4,4,0.5,0.5))
plot(spacing_vec,distance,las=1,ylim=c(0,1.2),col='darkgrey',pch=16)
abline(h=0,col='darkgrey',lwd=1.5)
mtext('a. Random sampling - linear model',side = 3,line = 0,adj = 0)



# Random sampling 

random_linear <- function(x,y,num_samples){
  true_slope <- as.numeric(coef(lm(y~x))[2])
  
  subset_index <- sort(c(1,sample(2:(length(x)-1),size = num_samples),length(x)))
  subset_x <- x[subset_index]
  subset_y <- y[subset_index]
  
  subset_slope <- as.numeric(coef(lm(subset_y~subset_x))[2])
  
  return(abs(true_slope - subset_slope)/true_slope)  
}

#spacing_vec <- 3:60
distance <- vector(mode='numeric',length = length(spacing_vec))
for (i in 1:length(spacing_vec)){
  distance[i] <- random_linear(x = vr_count$Year,y=vr_count$Biomass,num_samples =spacing_vec[i])
  
}
plot(spacing_vec,distance,las=1,ylim=c(0,1.2),yaxt='n',col='darkgrey',pch=16)
abline(h=0,col='darkgrey',lwd=1.5)
mtext('b. Regular sampling - linear model',side = 3,line = 0,adj = 0)
mtext('Percent difference',side = 2,line = 2.5,outer = T,font = 2)
mtext('Total number of samples analyzed',side = 1,line = 2.5,outer = T,font = 2)


```

Similarly, for a generalized additive model, we examined the shape of the estimated curve for subsets of data compared to the full time series (Fig. \ref{fig:gamtrends} \textcolor{red}{the caption is not the correct one, I edited it, but check you agree with it!}). In line with our other results, the regular sampling performed better than random sampling upon visual inspection. Both sampling approaches also visually approach the full data set with fewer samples compared to the linear models \textcolor{red}{(I do not understand this sentence)}. Iterative sampling requires only 8 samples to find the changepoint, but the a larger initial number is also necessary to capture the whole dynamic. 

```{r echo=FALSE, eval=T,message=FALSE, warning=FALSE, fig.width = 7.2, fig.height= 4.4,fig.cap='The fitted smooth functions for Year for different subset of the zooplankton abundance dataset for Lake Varese (IT). Subsamples using (a-d) random and (e-h) regular sampling. (i-j) shows the influence of the initial number of sample  (n2) when using the iterative method. For low initial numbers, the dynamic of the abundance is not caught. Choosing a larger initial number (n2) allows to capture the dynamic and find the real changepoint with a limited total number of samples (n). \\label{fig:gamtrends}'}

vr <- read.delim(paste0(getpath4data(),"VAR10-10-clado.txt"))
vr_count <- data.frame(Year=vr[,1], Biomass=rowSums(vr[,-1]))
cpvar_count <- sample_regular(vr_count[,2], nrow(vr_count), input_vector = T, messages = F)

smoothing_parameter=5
require(mgcv)

par(mfrow=c(3,4),mar=c(0.5,0.1,1.5,0.1),oma=c(4,4,0.5,0.5))

# Random GAMs

subset_index <- sort(c(1,sample(2:(length(vr_count$Year)-1),size = 10),length(vr_count$Year)))
vr_count_subset <- vr_count[subset_index,]
gam1_rand <- gam(vr_count_subset$Biomass ~ s(vr_count_subset$Year,k=smoothing_parameter),family=gaussian,)
plot.gam(gam1_rand,ylim=c(-50000,50000),xaxt='n', shade = T)
axis(1, at=vr_count_subset$Year, NA, tcl=0.3, lwd=.5)
mtext('a. Random (n=10)',side = 3,line = 0,adj = 0)

subset_index <- sort(c(1,sample(2:(length(vr_count$Year)-1),size = 20),length(vr_count$Year)))
vr_count_subset <- vr_count[subset_index,]
gam2_rand <- gam(vr_count_subset$Biomass ~ s(vr_count_subset$Year,k=smoothing_parameter),family=gaussian,)
plot.gam(gam2_rand,ylim=c(-50000,50000),xaxt='n',yaxt='n', shade = T)
axis(1, at=vr_count_subset$Year, NA, tcl=0.3, lwd=.5)
mtext('b. Random (n=20)',side = 3,line = 0,adj = 0)

subset_index <- sort(c(1,sample(2:(length(vr_count$Year)-1),size = 30),length(vr_count$Year)))
vr_count_subset <- vr_count[subset_index,]
gam3_rand <- gam(vr_count_subset$Biomass ~ s(vr_count_subset$Year,k=smoothing_parameter),family=gaussian)
plot.gam(gam3_rand,ylim=c(-50000,50000),xaxt='n',yaxt='n', shade = T)
axis(1, at=vr_count_subset$Year, NA, tcl=0.3, lwd=.5)
mtext('c. Random (n=30)',side = 3,line = 0,adj = 0)


gam4_rand <- gam(vr_count$Biomass ~ s(vr_count$Year,k=smoothing_parameter),family=gaussian)
plot.gam(gam4_rand,ylim=c(-50000,50000),xaxt='n',yaxt='n', shade = T)
axis(1, at=vr_count$Year, NA, tcl=0.3, lwd=.5)
mtext('d. Random (n=74)',side = 3,line = 0,adj = 0)


# Regular GAMs

subset_index <- c(seq(from = 1, to = length(vr_count$Year),length.out =10),length(vr_count$Year))
vr_count_subset <- vr_count[subset_index,]
gam1_rand <- gam(vr_count_subset$Biomass ~ s(vr_count_subset$Year,k=smoothing_parameter),family=gaussian,)
plot.gam(gam1_rand,ylim=c(-50000,50000),xaxt='n', shade = T)
axis(1, at=vr_count_subset$Year, NA, tcl=0.3, lwd=.5)
mtext('e. Regular (n=10)',side = 3,line = 0,adj = 0)

subset_index <- c(seq(from = 1, to = length(vr_count$Year),length.out =20),length(vr_count$Year))
vr_count_subset <- vr_count[subset_index,]
gam2_rand <- gam(vr_count_subset$Biomass ~ s(vr_count_subset$Year,k=smoothing_parameter),family=gaussian,)
plot.gam(gam2_rand,ylim=c(-50000,50000),xaxt='n',yaxt='n', shade = T)
axis(1, at=vr_count_subset$Year, NA, tcl=0.3, lwd=.5)
mtext('f. Regular (n=20)',side = 3,line = 0,adj = 0)

subset_index <- c(seq(from = 1, to = length(vr_count$Year),length.out =30),length(vr_count$Year))
vr_count_subset <- vr_count[subset_index,]
gam3_rand <- gam(vr_count_subset$Biomass ~ s(vr_count_subset$Year,k=smoothing_parameter),family=gaussian,)
plot.gam(gam3_rand,ylim=c(-50000,50000),xaxt='n',yaxt='n', shade = T)
axis(1, at=vr_count_subset$Year, NA, tcl=0.3, lwd=.5)
mtext('g. Regular (n=30)',side = 3,line = 0,adj = 0)


gam4_rand <- gam(vr_count$Biomass ~ s(vr_count$Year,k=smoothing_parameter),family=gaussian)
plot.gam(gam4_rand,ylim=c(-50000,50000),xaxt='n',yaxt='n', shade = T)
axis(1, at=vr_count$Year, NA, tcl=0.3, lwd=.5)
mtext('h. Regular (n=74)',side = 3,line = 0,adj = 0)

# Iterative GAMs

subset_index <- sample_iterative(vr, xcol = 1, n = 10, input_vector = F, DCA_axis = 1)
subset_index <- subset_index$matrix$index
vr_count_subset <- vr_count[subset_index,]
gam1_it <- gam(vr_count_subset$Biomass ~ s(vr_count_subset$Year,k=smoothing_parameter),family=gaussian,)
plot.gam(gam1_it,ylim=c(-50000,50000), shade = T)
mtext('i. Iterative (n=8, n2=5)',side = 3,line = 0,adj = 0)

subset_index <- sample_iterative(vr, xcol = 1, n = 20, n2 = 10, input_vector = F, DCA_axis = 1)
subset_index <- subset_index$matrix$index
vr_count_subset <- vr_count[subset_index,]
gam2_it <- gam(vr_count_subset$Biomass ~ s(vr_count_subset$Year,k=smoothing_parameter),family=gaussian,)
plot.gam(gam2_it,ylim=c(-50000,50000),yaxt='n', shade = T)
mtext('j. Iterative (n=13, n2=10)',side = 3,line = 0,adj = 0)

subset_index <- sample_iterative(vr, xcol = 1, n = 50, n2 = 30, input_vector = F, DCA_axis = 1)
subset_index <- subset_index$matrix$index
vr_count_subset <- vr_count[subset_index,]
gam3_it <- gam(vr_count_subset$Biomass ~ s(vr_count_subset$Year,k=smoothing_parameter),family=gaussian,)
plot.gam(gam3_it,ylim=c(-50000,50000),yaxt='n', shade = T)
mtext('h. Iterative (n=32, n2=30)',side = 3,line = 0,adj = 0)

gam4_it <- gam(vr_count$Biomass ~ s(vr_count$Year,k=smoothing_parameter),family=gaussian)
plot.gam(gam4_it,ylim=c(-50000,50000), shade = T)
mtext('k. Iterative (n2=74)',side = 3,line = 0,adj = 0)


mtext('Standardized abundance',side = 2,line = 2.5,outer = T,font = 2)
mtext('Time (year)',side = 1,line = 2.5,outer = T,font = 2)

```

# Word of caution

The limits of our three sub-sampling functions are tied to the limits of the _e.divise()_ function [@James2014]. In a situation where the change is a linear change instead of a sudden changepoint, the function _e.divise()_ will find a changepoint in the middle of the linear decline (Fig. \ref{fig:caution1}). In such situation, iterative sampling still performs the best.

```{r echo=FALSE, eval=T,message=FALSE, warning=FALSE, fig.width = 6, fig.height= 8,fig.cap='Examples of two time-series where shift lasted over (a) 5 and (b) 10 observations. Building off the example time-series in (b), we tested the three sampling method in (c-d). (c-d) are same as in Fig. 1 in the main manuscript. \\label{fig:caution1}'}

mypal <- wes_palette("Chevalier1")
mypal <- wes_palette("Zissou1", max(vr_out_order$count)+5, type = "continuous")
mypal[4] <- "#90b581"
mypal <- mypal[c(1,3,4,7,9)]

ar1 <- build_time_series()
mydata <- (ar1 - min(ar1))/(max(ar1) - min(ar1))

cex_titles <- .8

# Require an output from any of the 3 functions sampling_regular, sampling_random, sampling_iterative
pol_break <- function(cpt, col=1, y1=-.2, y2=1.2) {
  if(col==1) mycol=mypal[1]
  if(col==2) mycol=mypal[2]
  x1   <- cpt$changepoint
  x2   <- cpt$matrix$index[which(cpt$matrix$index==cpt$changepoint)-1]
  pol_x <- c(x1, x2, x2, x1)
  pol_y <- c(y1, y1, y2, y2)
  polygon(pol_x, pol_y, col= adjustcolor(mycol, alpha.f = .3), density = NULL, border = NA)
}

layout(matrix(c(1,2,0,
         3,4,5,
         0,0,6,
         0,0,7), ncol=3, byrow = T))
par(mar=c(1,2,6,2))


# plot 1: shift_duration = 5 ####
ar1 <- build_time_series(shift_duration = 5, shift_time = 62)
ar1 <- (ar1 - min(ar1))/(max(ar1) - min(ar1))
mydata <- ar1


plot(mydata, col="black", type="l", lwd=1.5, axes=F, xlab="",ylab="")
axis(1, lwd=2, at = c(-10,120))
axis(2, lwd=2, at = c(-1,2))
cp <- sample_regular(mydata, n=length(mydata), messages = F)
pol_break(cpt = cp)
lines(mydata, col="black", lwd=1.5)
text(cp$changepoint, .8, labels = "'real' \nchange point",pos = 4, col=mypal[1])
mtext("a. Example time-series with a shift \nlasting over 5 observations", side = 3, line = 2.5, adj = 0, font=1, cex=cex_titles)

# plot2: shift_duration = 10  ####
ar1 <- build_time_series(shift_duration = 10, shift_time = 35)
ar1 <- (ar1 - min(ar1))/(max(ar1) - min(ar1))
mydata <- ar1

plot(mydata, col="black", type="l", lwd=1.5, axes=F, xlab="",ylab="")
axis(1, lwd=2, at = c(-10,120))
axis(2, lwd=2, at = c(-1,2))
cp <- sample_regular(mydata, n=length(mydata), messages = F)
pol_break(cpt = cp)
lines(mydata, col="black", lwd=1.5)
text(cp$changepoint, .8, labels = "'real' \nchange point",pos = 4, col=mypal[1])
mtext("a. Example time-series with a shift \nlasting over 10 observations", side = 3, line = 2.5, adj = 0, font=1, cex=cex_titles)


# Plot 3 -- random sampling ####
plot(mydata, col=adjustcolor("grey", alpha.f = .4), type="l", lwd=1.5, axes=F, xlab="",ylab="")
axis(1, lwd=2, at = c(-10,120))
axis(2, lwd=2, at = c(-1,2))
cp_random <- sample_random(mydata, n=15, messages = F)
points(cp_random$matrix$index, cp_random$matrix$x, pch=4, lwd=2)
pol_break(cpt = cp, col=1)
pol_break(cpt = cp_random, col=2)
lines(mydata, col="black", lwd=1.5)
text(100,0.95,labels = paste0("n = ",cp_random$final_n), pos = 2)
mtext("c. Random sampling", side = 3, line = 1, adj = 0, font=1, cex=cex_titles)

# Plot 4 -- regular sampling ####
plot(mydata, col=adjustcolor("grey", alpha.f = .4), type="l", lwd=1.5, axes=F, xlab="",ylab="")
axis(1, lwd=2, at = c(-10,120))
axis(2, lwd=2, at = c(-1,2))
cp_regular <- sample_regular(mydata, n=15, messages = F)
points(cp_regular$matrix$index, cp_regular$matrix$x, pch=4, lwd=2)
pol_break(cpt = cp, col=1)
pol_break(cpt = cp_regular, col=2)
lines(mydata, col="black", lwd=1.5)
text(100,0.95,labels = paste0("n = ",cp_regular$final_n), pos = 2)
mtext("d. Regular sampling", side = 3, line = 1, adj = 0, font=1, cex=cex_titles)

# Plot 5 -- iterative sampling (1/3) ####
plot(mydata, col=adjustcolor("grey", alpha.f = .4), type="l", lwd=1.5, axes=F, xlab="",ylab="")
cp_iterative <- sample_regular(mydata, n=5, messages = F)
pol_break(cpt = cp, col=1)
pol_break(cpt = cp_iterative, col=2)
lines(mydata, col="black", lwd=1.5)
axis(1, lwd=2, at = c(-10,120))
axis(2, lwd=2, at = c(-1,2))
points(cp_iterative$matrix$index, cp_iterative$matrix$x, pch=4, lwd=2)
text(100,0.95,labels = paste0("n = ",cp_iterative$final_n), pos = 2)
mtext("e. Iterative sampling (initial)", side = 3, line = 1, adj = 0, font=1, cex=cex_titles)
xleft_temp <- cp_iterative$matrix$index[which(cp_iterative$matrix$index==cp_iterative$changepoint)-1]-4 
xright_temp <- cp_iterative$changepoint+4 
par(xpd=T)
rect(xleft = xleft_temp, xright = xright_temp, ybottom = -0.1, ytop = 1.1, lty = 2)
text(xright_temp, 1, labels = "f", pos = 4)
par(xpd=F)

# Plot 6 -- iterative sampling (2/3) ####
plot(mydata, col=adjustcolor("grey", alpha.f = .4), type="l", lwd=1.5, axes=F, xlab="",ylab="", xlim=c(xleft_temp,xright_temp))
cp_iterative1 <- sample_regular(mydata, n=5, messages = F)
cp_iterative2 <- sample_iterative(mydata, n=6, messages = F)
cp_iterative3 <- sample_iterative(mydata, n=7, messages = F)
cp_iterative4 <- sample_iterative(mydata, n=8, messages = F)
axis(1, lwd=2, at = c(-10,120))
axis(2, lwd=2, at = c(-1,2))
mypal2 <- wes_palette("GrandBudapest2")
mypal2 <- wes_palette("Zissou1", max(vr_out_order$count)+5,type = "continuous")
mypal2[4] <- "#90b581"
mypal2 <- mypal[c(1,3,4,7,9)]

points(cp_iterative1$matrix$index, cp_iterative$matrix$x, col=grey(0.5), pch=4, lwd=2)
points(cp_iterative2$matrix$index[!cp_iterative2$matrix$index%in%cp_iterative1$matrix$index], cp_iterative2$matrix$x[!cp_iterative2$matrix$index%in%cp_iterative1$matrix$index], 
       col=mypal2[1], pch=4, lwd=2)
points(cp_iterative3$matrix$index[!cp_iterative3$matrix$index%in%cp_iterative2$matrix$index], cp_iterative3$matrix$x[!cp_iterative3$matrix$index%in%cp_iterative2$matrix$index], 
       col=mypal2[2], pch=4, lwd=2)
points(cp_iterative4$matrix$index[!cp_iterative4$matrix$index%in%cp_iterative3$matrix$index], cp_iterative4$matrix$x[!cp_iterative4$matrix$index%in%cp_iterative3$matrix$index], 
       col=mypal2[3], pch=4, lwd=2)
legend("topright", legend = c("t=0", "t=1", "t=2", "t=3"),
       col = c(grey(0.5), mypal2), pch=4, lwd=2, bty="n", lty=NA)
mtext("f. Zoom from e\n(slowly adding samples)", side = 3, line = 1, adj = 0, font=1, cex=cex_titles)

# Plot 7 -- iterative sampling (3/3) ####
plot(mydata, col=adjustcolor("grey", alpha.f = .4), type="l", lwd=1.5, axes=F, xlab="",ylab="")
cp_iterative <- sample_iterative(mydata, n=15, messages = F)
pol_break(cpt = cp, col=1)
pol_break(cpt = cp_iterative, col=2)
lines(mydata, col="black", lwd=1.5)
axis(1, lwd=2, at = c(-10,120))
axis(2, lwd=2, at = c(-1,2))
points(cp_iterative$matrix$index, cp_iterative$matrix$x, pch=4, lwd=2)
text(100,0.95,labels = paste0("n = ",cp_iterative$final_n), pos = 2)
mtext("g. Iterative sampling (final)", side = 3, line = 1, adj = 0, font=1, cex=cex_titles)


```


Another issue emerge in time series with no changepoint. The first case is when the time-series follows a declining trend. In some situations, depending on the noise and the trend, a changepoint will still be detected (Fig. \ref{fig:caution2}). Again, this would be the case even with a continuous sampling, but the iterative method can falsely reinfore this (by converging to a changepoint, and hiding the general trend). In this scenario, increasing the initial number of samples analyzed (~30% of the initial samples) would allow to roughly model the behavior of the dataset (Fig. \ref{fig:gamtrends}).

Overall, while it is important for potential users to be aware of the behavior of the code, community changes would more often lead to larger restructurations (alternative stable states, @Bruel2018). We caution in general against using too few samples for a dataset or a type of data we know too little about.

```{r echo=FALSE, eval=T,message=FALSE, warning=FALSE, fig.width = 7.2, fig.height= 6,fig.cap='Examples of two time-series where shift lasted over (a) 5 and (b) 10 observations. Building , off the example time-series in (b), we tested the three sampling method in (c-d). (c-d) are same as in Fig. 1 in the main manuscript. \\label{fig:caution2}'}

par(mfrow=c(2,2))
# Plot 1: no shift, declining trend ####
ar1 <- -0.0001*x + 1 + rnorm(length(x),0,0.001)
ar1 <- (ar1 - min(ar1))/(max(ar1) - min(ar1))
mydata <- ar1

plot(mydata, col="black", type="l", lwd=1.5, axes=F, xlab="",ylab="")
axis(1, lwd=2, at = c(-10,120))
axis(2, lwd=2, at = c(-1,2))
cp <- sample_regular(mydata, n=length(mydata), messages = F)
if(!is.na(cp$changepoint)) pol_break(cpt = cp)
lines(mydata, col="black", lwd=1.5)
if(!is.na(cp$changepoint)) {
  text(cp$changepoint, .8, labels = "change point \ndetected",pos = 4, col=mypal[1])
} else {
    text(3,.95, labels = "No changepoint", adj = 0, col=mypal[1])
  }
mtext("a. Example time-series with no shift, only a declining trend. \nNo changepoint should have been detected.", side = 3, line = 1.5, adj = 0, font=1, cex=cex_titles)

# Plot 2: no shift, trend, subsample ####
plot(mydata, col=adjustcolor("grey", alpha.f = .4), type="l", lwd=1.5, axes=F, xlab="",ylab="")
axis(1, lwd=2, at = c(-10,120))
axis(2, lwd=2, at = c(-1,2))
cp_regular <- sample_regular(mydata, n=15, messages = F)
points(cp_regular$matrix$index, cp_regular$matrix$x, pch=4, lwd=2)
#pol_break(cpt = cp, col=1)
pol_break(cpt = cp_regular, col=2)
text(100,0.95,labels = paste0("n = ",cp_regular$final_n), pos = 2)
mtext("b. Subsampling a time-series with no shift and a trend. \nNo changepoint should have beend detected.", side = 3, line = 1.5, adj = 0, font=1, cex=cex_titles)

# Plot 2: no shift, no trend ####
ar1 <- -0.000001*x + 1 + rnorm(length(x),0,0.001)
ar1 <- (ar1 - min(ar1))/(max(ar1) - min(ar1))
mydata <- ar1

plot(mydata, col="black", type="l", lwd=1.5, axes=F, xlab="",ylab="")
axis(1, lwd=2, at = c(-10,120))
axis(2, lwd=2, at = c(-1,2))
cp <- sample_regular(mydata, n=length(mydata), messages = F)
if(!is.na(cp$changepoint)) pol_break(cpt = cp)
lines(mydata, col="black", lwd=1.5)
if(!is.na(cp$changepoint)) {
  text(cp$changepoint, .8, labels = "'real' \nchange point",pos = 4, col=mypal[1])
} else {
    text(3,.95, labels = "No changepoint", adj = 0, col=mypal[1])
  }
mtext("c. Example time-series with no shift and no trend.\nNo changepoint should have beend detected.", side = 3, line = 1.5, adj = 0, font=1, cex=cex_titles)



plot(mydata, col=adjustcolor("grey", alpha.f = .4), type="l", lwd=1.5, axes=F, xlab="",ylab="")
axis(1, lwd=2, at = c(-10,120))
axis(2, lwd=2, at = c(-1,2))
cp_regular <- sample_regular(mydata, n=15, messages = F)
points(cp_regular$matrix$index, cp_regular$matrix$x, pch=4, lwd=2)
pol_break(cpt = cp_regular, col=2)
text(100,0.95,labels = paste0("n = ",cp_regular$final_n), pos = 2)
mtext("d. Subsampling a time-series with no shift and no trend. \nNo changepoint detected.", side = 3, line = 1.5, adj = 0, font=1, cex=cex_titles)


```

Finally, note that the p-values in the output of each functions are calculated within the function _e.divisive()_ (package _ecp_, @James2014). By default, the p-value of 0.005 means that no other permutation within the matrix lead to more energy needed to break the matrix. The p-value is in fact a frequency: only one configuration in 200 scenarios (initial matrix and 199 permutations) led to that much energy (1/200 = 0.005). We included in the three functions we coded (*sampling_random(), sampling_regular(), sampling_iterative()*) the arguments _R_ (default = 199) and _sig.lvl_ (default = 0.005), two arguments within the initial _e.divisive()_ function. By changing the default value, the user can choose a different number of permutation (if R = 19, then the minimum p-value will be 1/(1+19) = 0.05), and another significance level (if R = 199, the default, and the user wants to stop looking for changepoints once 10 configurations result in higher energy, then sig.lvl must be changed to 10/(1+199) = 0.05). Refer to the initial function for better understanding [@James2014].

\clearpage

# References