---
bibliography: ChangepointSampling.bib
csl: ecology-letters.csl
editor_options:
  chunk_output_type: console
fontsize: 12pt
geometry: margin=1in
header-includes: \usepackage{float} \usepackage{lineno} \usepackage{setspace}\doublespacing
  \usepackage[round]{natbib} \bibpunct[; ]{(}{)}{,}{a}{}{,} \usepackage{color} \usepackage{totcount}
  \newtotcounter{citenum} \def\oldcite{} \let\oldcite=\bibcite \def\bibcite{\stepcounter{citenum}\oldcite}
  \renewcommand{\thepage}{S\arabic{page}}
  \renewcommand{\thesection}{Appendix S\arabic{section}} \renewcommand{\thetable}{S\arabic{table}}
  \renewcommand{\thefigure}{S\arabic{figure}}
  \usepackage{amsmath}
  \renewcommand{\theequation}{S\arabic{equation}} \floatplacement{figure}{H}
output:
  pdf_document:
    fig_caption: yes
    keep_tex: yes
    number_sections: no
---

```{r setup, include=FALSE}
knitr::opts_knit$set(root.dir = dirname(getwd()))
```




\begin{center}
\textbf{\Large Supplementary material for: Sampling requirements and approaches to detect ecosystem shifts}
\vspace{5 mm}
	
\textsc{Rosalie Bruel$^{*}$\footnote{*The authors contributed equally to this work. $\dagger$Corresponding author (eastonrwhite@gmail.com)} and Easton R. White$^{* \dagger 2,3}$}
\vspace{3 mm}

\normalsize{\indent $^{1}$Rubenstein Ecosystem Science Laboratory, University of Vermont, VT, USA \\ $^{2}$Department of Biology, University of Vermont, VT, USA \\
$^{3}$Gund Institute for Environment, University of Vermont, VT, USA}
\end{center}

\vspace{2 mm}

\tableofcontents

\vspace{1cm}

Data and code for all the figures can be found at (https://github.com/rosalieb/temporal-sampling). All the analyses were run in R [@RCoreTeam2019].


\clearpage


# Additional methods

We first detect the "true" changepoint of a full length time series with the function _e.divisive_ in the R package _ecp_ [@James2019]. We focus on the changepoint with the largest magnitude, although this package allows detection of further changepoints as well (Fig. \ref{fig:case_study_other_changepoints}). We then subsample the full time series [@White2020] with different numbers of subsamples and different sampling approaches. For each sampling approach, we wrote a custom function:

- sample_random(): sample first and last sample, in addition of n-2 random samples in between (n = maximum number of sample, chosen by the user),  
- sample_regular(): sample first and last sample, in addition of n-2 evenly spaced samples in between (n = maximum number of samples, chosen by the user), and  
- sample_iterative(): sample first and last sample as well as 3 evenly spaced samples in between (3 is the default but can be modified), in order to initiate the changepoint detection. Upon detection of the changepoint on this 5 samples time series, a new sample is added between the changepoint and the previous sample, to narrow down the real changepoint. Detection of changepoint and addition of sample is repeated, until it was narrowed down to two consecutive samples, or n (maximum number of samples, chosen by the user) was reached, whichever comes first.  
  
The subsampled time series are then compared to the full time series to assess the effectiveness of each subsampling approach.

Each function works with two types of input: vector and matrix. If the input is a matrix, the user must change the default argument *is_vector* to *FALSE*. The matrix is then transformed to independent vectors using Detrended Correspondence Analysis [@Hill1980], and a single axis on which changepoint analysis is run is chosen with the argument *DCA_axis* (default to 1, for first component). Component scores are returned by the function. A user can edit the function to use another ordination method (e.g., principal component analysis).

<!--
# Parameter sensitivity

We investigated how different parameters change the power of the analysis, and found that autocorrelation at lag-1 (phi) had little impact on the power of the analysis amplitude of the shift was the most important determinant of the power of the analysis. For null standard deviation, the analysis performed well independently of the shift size or autocorrelation. When standard deviation increased, the power of the analysis was higher for null autocorrelation (Fig. \ref{fig:parameter_sensitivity_plot}).
-->


# Minimum time for other sampling approaches

## Random sampling

```{r simulation_min_time_random,message=F,warning=F,echo=F,eval=F,cache=T}

source('R/sampling_random.R')
source('R/build_time_series.R')
source('R/updated_param_sensitivity.R')
source('R/calc_min_time2.R')
require(tidyverse)
require(stickylabeller)
require(ggplot2)
require(viridis)

# Sigma results
sigma_vec = seq(0.01,0.4,0.025)
min_time  = rep(0,times=length(sigma_vec))
sigma_output <- data.frame(sigma_vec,min_time)

for (param_num in 1:nrow(sigma_output)){
  sigma_output$min_time[param_num] <- calc_min_time('sigma',sigma_output$sigma_vec[param_num],'random')
  #print(sigma_output)
}

write.csv(x = sigma_output,file = 'Output/theoretical_results_random_sigma.csv',quote=FALSE)

# Phi results

phi_vec = seq(-0.8,0.8,0.05)
min_time  = rep(0,times=length(phi_vec))
phi_output <- data.frame(phi_vec,min_time)

for (param_num in 1:nrow(phi_output)){
  phi_output$min_time[param_num] <- calc_min_time('phi',phi_output$phi_vec[param_num],'random')
  #print(phi_output)
}

write.csv(x = phi_output,file = 'Output/theoretical_results_random_phi.csv',quote=FALSE)


# Shift size

shift_size_vec = rev(seq(0.1,0.9,0.05))
min_time  = rep(0,times=length(shift_size_vec))
shift_size_output <- data.frame(shift_size_vec,min_time)

for (param_num in 1:nrow(shift_size_output)){
  shift_size_output$min_time[param_num] <- calc_min_time('shift_size',shift_size_output$shift_size_vec[param_num],'random')
  #print(shift_size_output)
}

write.csv(x = shift_size_output,file = 'Output/theoretical_results_random_shift_size.csv',quote=FALSE)


```



```{r simulation_min_time_plot_random, fig.width = 7.2, fig.height= 3, fig.cap='Minimum number of samples required for 0.8 statistical power given different levels of (a) temporal variability ($\\sigma$), (b) temporal autocorrelation ($\\phi$), and (c) shift size. Random sampling was used with the default parameters are $\\sigma = 0.53$, $\\phi = 0.404$, and a shift size $=0.81$ to match the case study. The exact timing of the true changepoint varied for each simulation between time steps 30 and 70. Each vertical line indicates the respective parameter calculated from the case study time series. \\label{fig:simulation_min_time_random}',message=F,warning=F,echo=F,eval=F}
require(ggplot2)
require(dplyr)
require(stickylabeller)
par(mfrow=c(1,3))

sigma <- read.csv('Output/theoretical_results_random_sigma.csv',header=TRUE)

phi <- read.csv('Output/theoretical_results_random_phi.csv',header=TRUE)

shift_size <- read.csv('Output/theoretical_results_random_shift_size.csv',header=TRUE)


# Melt the 3 dataset to be able to use facet_wrap()
colnames(sigma) <- colnames(phi) <- colnames(shift_size) <- c("X", "Param", "min_time")
simulation_all <- 
  rbind(cbind("Parameter" = rep("Standard deviation", nrow(sigma)), sigma),
        cbind("Parameter" = rep("Autocorrelation at lag-1", nrow(phi)), phi),
        cbind("Parameter" = rep("Normalized amplitude of shift", nrow(shift_size)), shift_size))
# param_vr <- data.frame("Parameter" = unique(simulation_all$Parameter),
#                        "value" = c(ar_vr, cv_vr, shift_vr))

p1 <- 
  ggplot(simulation_all %>% filter(Parameter == unique(simulation_all$Parameter)[1]), aes(Param, min_time)) +
  geom_point() +
  geom_line() +
  geom_vline(xintercept=0.05, lty=2) + 
  facet_wrap(~Parameter, scales="free_x", 
             labeller = label_glue('a. {Parameter}')) +
  ylim(10,52) +
  labs(x=c("sigma"), y= "Minimum number of samples required") + 
  theme_bw() +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        strip.background = element_rect(colour="transparent", fill="transparent"),
        strip.text.x = element_text(angle = 0, hjust = 0))
p2 <- ggplot(simulation_all %>% filter(Parameter == unique(simulation_all$Parameter)[2]), aes(Param, min_time)) +
  geom_point() +
  geom_line() +
  geom_vline(xintercept = 0.404, lty=2) +
  facet_wrap(~Parameter, scales="free_x", 
             labeller = label_glue('b. {Parameter}')) +
  ylim(10,52) +
  labs(x="phi", y= "") + 
  theme_bw() +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        strip.background = element_rect(colour="transparent", fill="transparent"),
        strip.text.x = element_text(angle = 0, hjust = 0))
p3 <- ggplot(simulation_all %>% filter(Parameter == unique(simulation_all$Parameter)[3]), aes(Param, min_time))  +
  geom_point() +
  geom_line() + 
  geom_vline(xintercept = 0.81, lty=2) +
  facet_wrap(~Parameter, scales="free_x", 
             labeller = label_glue('c. {Parameter}')) +
  ylim(10,52) +
  labs(x="shift size", y= "") + 
  theme_bw() +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        strip.background = element_rect(colour="transparent", fill="transparent"),
        strip.text.x = element_text(angle = 0, hjust = 0))


# Remove the y-axis for the 2nd plot - p2 then merge 2 plots
cowplot::plot_grid(p1 + theme(plot.margin = unit(c(0.15, 0.1, 0.15, 0.15), "cm")), 
                   p2 + 
                     theme(plot.margin = unit(c(0, 0.1, 0, 0), "cm"),
                           axis.text.y = element_blank(),
                           axis.line.y = element_blank(),
                           axis.title.y= element_blank(),
                           axis.ticks.y= element_blank()),
                   p3 + 
                     theme(plot.margin = unit(c(0, 0.1, 0, 0), "cm"),
                           axis.text.y = element_blank(),
                           axis.line.y = element_blank(),
                           axis.title.y= element_blank(),
                           axis.ticks.y= element_blank()),
                   nrow = 1,
                   rel_widths = c(1.2, 1,1),
                   align = 'h', axis = 'tb')

```


```{r parameter_sensitivity_plot_random,echo=F,message=F,warnings=F,fig.height=4,fig.cap='Random sampling statistical power (fraction of 100 simulations which detected a changepoint within five time points of the true changepoint) for different levels of standard deviation ($\\sigma$), lag-1 autocorrelation ($\\phi$), and shift size ($\\delta$). For each parameter combination, 20 samples were used. An increase in samples would increase the statistical power across this graph.\\label{fig:parameter_sensitivity_plot_random}'}
output_theory_approach <- read.csv(file = 'Output/parameter_sensitivity_random.csv')

require(ggplot2)
require(viridis)
ggplot(aes(x=phi_vec,y=sigma_vec,fill=power),data=output_theory_approach) + geom_tile() +
  scale_fill_viridis(discrete=FALSE,begin = 0.1,end=0.9, limits=c(0,1)) +
  facet_wrap(~shift_size_vec) + 
  theme_classic(base_size = 14, base_family = "")+
  theme(plot.subtitle = element_text(hjust = 0.5)) +
  labs(fill='Power',subtitle = 'Shift size') +
  xlab("Lag-1 autocorrelation (phi)") +
  ylab("Standard deviation (sigma)")
```

## Iterative sampling

```{r simulation_min_time_iterative,message=F,warning=F,echo=F,eval=F,cache=T}


source('R/sampling_iterative.R')
source('R/build_time_series.R')
#source('R/updated_param_sensitivity.R')
source('R/calc_min_time2.R')
require(tidyverse)

# Sigma results
sigma_vec = seq(0.01,0.4,0.025)
min_time  = rep(0,times=length(sigma_vec))
sigma_output <- data.frame(sigma_vec,min_time)

for (param_num in 1:nrow(sigma_output)){
  sigma_output$min_time[param_num] <- calc_min_time('sigma',sigma_output$sigma_vec[param_num],'iterative')
  print(sigma_output)
}

write.csv(x = sigma_output,file = 'Output/theoretical_results_iterative_sigma.csv',quote=FALSE)

# Phi results

phi_vec = seq(-0.8,0.8,0.05)
min_time  = rep(0,times=length(phi_vec))
phi_output <- data.frame(phi_vec,min_time)

for (param_num in 1:nrow(phi_output)){
  phi_output$min_time[param_num] <- calc_min_time('phi',phi_output$phi_vec[param_num],'iterative')
  #print(phi_output)
}

write.csv(x = phi_output,file = 'Output/theoretical_results_iterative_phi.csv',quote=FALSE)


# Shift size

shift_size_vec = rev(seq(0.1,0.9,0.05))
min_time  = rep(0,times=length(shift_size_vec))
shift_size_output <- data.frame(shift_size_vec,min_time)

for (param_num in 1:nrow(shift_size_output)){
  shift_size_output$min_time[param_num] <- calc_min_time('shift_size',shift_size_output$shift_size_vec[param_num],'iterative')
  #print(shift_size_output)
}

write.csv(x = shift_size_output,file = 'Output/theoretical_results_iterative_shift_size.csv',quote=FALSE)


```



```{r simulation_min_time_plot_iterative, fig.width = 7.2, fig.height= 3, fig.cap='Minimum number of samples required for 0.8 statistical power given different levels of (a) temporal variability ($\\sigma$), (b) temporal autocorrelation ($\\phi$), and (c) shift size. Iterative sampling was used with the default parameters are $\\sigma = 0.53$, $\\phi = 0.404$, and a shift size $=0.81$ to match the case study. The exact timing of the true changepoint varied for each simulation between time steps 30 and 70. Each vertical line indicates the respective parameter calculated from the case study time series. \\label{fig:simulation_min_time}',message=F,warning=F,echo=F,eval=F}

par(mfrow=c(1,3))

sigma <- read.csv('Output/theoretical_results_iterative_sigma.csv',header=TRUE)

phi <- read.csv('Output/theoretical_results_iterative_phi.csv',header=TRUE)

shift_size <- read.csv('Output/theoretical_results_iterative_shift_size.csv',header=TRUE)


# Melt the 3 dataset to be able to use facet_wrap()
colnames(sigma) <- colnames(phi) <- colnames(shift_size) <- c("X", "Param", "min_time")
simulation_all <- 
  rbind(cbind("Parameter" = rep("Standard deviation", nrow(sigma)), sigma),
        cbind("Parameter" = rep("Autocorrelation at lag-1", nrow(phi)), phi),
        cbind("Parameter" = rep("Normalized amplitude of shift", nrow(shift_size)), shift_size))
# param_vr <- data.frame("Parameter" = unique(simulation_all$Parameter),
#                        "value" = c(ar_vr, cv_vr, shift_vr))

p1 <- 
  ggplot(simulation_all %>% filter(Parameter == unique(simulation_all$Parameter)[1]), aes(Param, min_time)) +
  geom_point() +
  geom_line() +
  geom_vline(xintercept=0.05, lty=2) + 
  facet_wrap(~Parameter, scales="free_x", 
             labeller = label_glue('a. {Parameter}')) +
  ylim(10,50) +
  labs(x=c("sigma"), y= "Minimum number of samples required") + 
  theme_bw() +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        strip.background = element_rect(colour="transparent", fill="transparent"),
        strip.text.x = element_text(angle = 0, hjust = 0))
p2 <- ggplot(simulation_all %>% filter(Parameter == unique(simulation_all$Parameter)[2]), aes(Param, min_time)) +
  geom_point() +
  geom_line() +
  geom_vline(xintercept = 0.404, lty=2) +
  facet_wrap(~Parameter, scales="free_x", 
             labeller = label_glue('b. {Parameter}')) +
  ylim(10,50) +
  labs(x="phi", y= "") + 
  theme_bw() +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        strip.background = element_rect(colour="transparent", fill="transparent"),
        strip.text.x = element_text(angle = 0, hjust = 0))
p3 <- ggplot(simulation_all %>% filter(Parameter == unique(simulation_all$Parameter)[3]), aes(Param, min_time))  +
  geom_point() +
  geom_line() + 
  geom_vline(xintercept = 0.81, lty=2) +
  facet_wrap(~Parameter, scales="free_x", 
             labeller = label_glue('c. {Parameter}')) +
  ylim(10,50) +
  labs(x="shift size", y= "") + 
  theme_bw() +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        strip.background = element_rect(colour="transparent", fill="transparent"),
        strip.text.x = element_text(angle = 0, hjust = 0))


# Remove the y-axis for the 2nd plot - p2 then merge 2 plots
cowplot::plot_grid(p1 + theme(plot.margin = unit(c(0.15, 0.1, 0.15, 0.15), "cm")), 
                   p2 + 
                     theme(plot.margin = unit(c(0, 0.1, 0, 0), "cm"),
                           axis.text.y = element_blank(),
                           axis.line.y = element_blank(),
                           axis.title.y= element_blank(),
                           axis.ticks.y= element_blank()),
                   p3 + 
                     theme(plot.margin = unit(c(0, 0.1, 0, 0), "cm"),
                           axis.text.y = element_blank(),
                           axis.line.y = element_blank(),
                           axis.title.y= element_blank(),
                           axis.ticks.y= element_blank()),
                   nrow = 1,
                   rel_widths = c(1.2, 1,1),
                   align = 'h', axis = 'tb')

```


```{r parameter_sensitivity_plot_iterative,echo=F,message=F,warnings=F,fig.height=4,fig.cap='Iterative sampling statistical power (fraction of 100 simulations which detected a changepoint within five time points of the true changepoint) for different levels of standard deviation ($\\sigma$), lag-1 autocorrelation ($\\phi$), and shift size ($\\delta$). For each parameter combination, 20 samples were used. An increase in samples would increase the statistical power across this graph.\\label{fig:parameter_sensitivity_plot_iterative}'}
output_theory_approach <- read.csv(file = 'Output/parameter_sensitivity_iterative.csv')

ggplot(aes(x=phi_vec,y=sigma_vec,fill=power),data=output_theory_approach) + geom_tile() +
  scale_fill_viridis(discrete=FALSE,begin = 0.1,end=0.9) +
  facet_wrap(~shift_size_vec) + 
  theme_classic(base_size = 14, base_family = "")+
  theme(plot.subtitle = element_text(hjust = 0.5)) +
  labs(fill='Power',subtitle = 'Shift size') +
  xlab("Lag-1 autocorrelation (phi)") +
  ylab("Standard deviation (sigma)")
```




# Detecting further changepoints

The initial study found 3 changepoints on the main axis, in 1926/1928, 1946/1948 and 1983/1988 [@Bruel2018]. We focused our analysis on the main changepoint, but the code also allows to target further changepoints, using the argument *c*. *c* is a numeric value indicating the maximum number of changepoints to find (Fig. \ref{fig:case_study_other_changepoints}).

The iterative method requires less subsamples overall to find the real changepoints (Fig. \ref{fig:case_study_other_changepoints}, 3rd column). Note that if the initial number of samples (controlled with the argument *n2* in the code) is at the default 5, the code struggles to rank appropriately the changepoints, resulting in a changepoint being found, but not the overall "true" second  (e.g., Fig. \ref{fig:case_study_other_changepoints}b, 3rd column), or "true" third. For a time-series, we recommend users examine the p-values returned by the *e.divisive* function, embedded in the function *sample_iterative* output. If the first few changepoints have close, low p-values, it may be useful to add another subsample further away from the presumed changepoint. This avoids the iterative algorithm mistakenly converging on the wrong changepoint. An alternative is to initiate the subsampling with an higher number of samples, e.g., 7 subsamples.

```{r case study read data output, include=FALSE}
myoutput <- read.table(paste0(getwd(),"/Output/output_changepoint_varese.txt"))
myoutput$method_f <- factor(myoutput$method, levels=c('Random', 'Regular', 'Iterative'))

```


```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width = 7.2, fig.height= 8,fig.cap='Distance to true (a) first changepoint (largest magnitude), (b) second changepoint, and (c) third changepoint for total number of samples analyzed, following random sampling, regular sampling, and iterative sampling. Total number of samples was set between 5 and 30, out of the 74 initial time series.  \\label{fig:case_study_other_changepoints}'}
library(ggplot2)
library(gridExtra)
p1 <- ggplot(myoutput[myoutput$target_cpt==1,], aes(final_n,diff_real, pch=factor(n2_init))) + 
  geom_hline(aes(yintercept=0), col=adjustcolor("black", alpha.f = .4)) + 
  geom_point(alpha = .3) +
  facet_wrap(~method_f) + theme_bw() +
  labs(title="a. First changepoint",x="", y="Distance to real changepoint") +
  theme(legend.position = c(0.93, 0.6),
        legend.background = element_rect(fill = "#ffffffaa", colour = NA),
        legend.title=element_text(size=7), 
        legend.text=element_text(size=8)) +
  guides(pch=guide_legend(title="Initial number\nof sample"))
p2 <- ggplot(myoutput[myoutput$target_cpt==2,], aes(final_n,diff_real, pch=factor(n2_init))) + 
  geom_hline(aes(yintercept=0), col=adjustcolor("black", alpha.f = .4)) + 
  geom_point(alpha = .3) +
  facet_wrap(~method_f) + theme_bw() +
  labs(title="b. Second changepoint",x="", y="Distance to real changepoint") +
  theme(legend.position = c(0.93, 0.6),
        legend.background = element_rect(fill = "#ffffffaa", colour = NA),
        legend.title=element_text(size=7), 
        legend.text=element_text(size=8)) +
  guides(pch=guide_legend(title="Initial number\nof sample"))
p3 <- ggplot(myoutput[myoutput$target_cpt==3,], aes(final_n,diff_real, pch=factor(n2_init))) + 
  geom_hline(aes(yintercept=0), col=adjustcolor("black", alpha.f = .4)) + 
  geom_point(alpha = .3) +
  facet_wrap(~method_f) + theme_bw() +
  labs(title="c. Third changepoint",x="", y="Distance to real changepoint") +
  theme(legend.position = c(0.93, 0.6),
        legend.background = element_rect(fill = "#ffffffaa", colour = NA),
        legend.title=element_text(size=7), 
        legend.text=element_text(size=8)) +
  guides(pch=guide_legend(title="Initial number\nof sample"))
grid.arrange(p1,p2,p3,nrow=3)
```














# Case study: Changepoint detection of abundance time series

```{r echo=FALSE, eval=TRUE, message=FALSE, warning=FALSE}
# Investigating change in species average biomass
# Read code
source('R/sampling_regular.R')
# Read data
getpath4data <- function() {
  if(Sys.getenv("USER")=="Rosalie") return("/Volumes/-/Script R/Data-off-GitHub/temporal-sampling/")
  if(Sys.getenv("USER")=="eastonwhite") return("~/Desktop/Research/soil-temporal-sampling/")
  if(Sys.getenv("USER")!="Rosalie"|Sys.getenv("USER")!="put here your USER") stop("You need to get the data and define their location.")
}
vr <- read.delim(paste0(getpath4data(),"VAR10-10-clado.txt"))
vr_count <- data.frame(Year=vr[,1], Biomass=rowSums(vr[,-1]))
cpvar_count <- sample_regular(vr_count[,2], nrow(vr_count), input_vector = T, messages = F)

```

As another example of our approach, we we re-used the community dataset from Lake Varese, but this time, looked at the total number of individuals per gram of sediment in each subsamples. Thus, this is a univariate time series as opposed to the community-level data used in the main text. The full time series show a changepoint in `r paste(vr_count$Year[cpvar_count$changepoint], vr_count$Year[cpvar_count$changepoint-1], sep="/")`. We used the three changepoint method to estimate changepoint on the vector, and found that the iterative method, with an initial number of 5 regularly-spaced subsamples, performed best (Fig. \ref{fig:case_study_number_sample_biomass}).


```{r echo=FALSE, eval=TRUE, message=FALSE, warning=FALSE, fig.width = 4, fig.height= 3, fig.cap='Number of individuals per gram of sediment, infered from Cladocera remains for the 74 subsamples of the Lake Varese sediment core. Vertical grey band indicates the "true" changepoint.   \\label{fig:case_study_biomass_per_year}'}
# plot
ggplot(vr_count, aes(Year, Biomass)) + 
  theme_bw() +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        strip.background = element_rect(colour="transparent", fill="transparent")) +
           geom_rect(mapping=aes(xmin=vr_count$Year[cpvar_count$changepoint-1], xmax=vr_count$Year[cpvar_count$changepoint], ymin=0, ymax=max(vr_count)), alpha=0.05, fill="grey") + 
  geom_line() +
  labs(y="Number of individuals / g")

```


```{r echo=FALSE, eval=T,message=FALSE, warning=FALSE, fig.width = 7.2, fig.height= 3,fig.cap='Distance to real changepoint in biomass per total number of samples analyzed, following (a) random sampling, (b) regular sampling, and (c) iterative sampling. Total number of samples was set between 5 and 30, out of the 74 initial time series.   \\label{fig:case_study_number_sample_biomass}'}

require(stickylabeller)

myoutput <- read.table(paste0(getwd(),"/Output/output_changepoint_varese_biomass.txt"))
myoutput$method <- factor(myoutput$method, levels=c('Random', 'Regular', 'Iterative'))
myoutput$diff_real <- abs(myoutput$diff_real)


# Plot
p1 <-
  ggplot(myoutput[myoutput$target_cpt==1 & myoutput$n2_init == 5,], aes(final_n,diff_real)) + 
  geom_hline(aes(yintercept=0), col=adjustcolor("black", alpha.f = .4)) + 
  geom_point(alpha = .3) +
  facet_wrap(~method,
             labeller = label_glue('{.l}. {method} sampling ')) + 
  theme_bw() +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        strip.background = element_rect(colour="transparent", fill="transparent"),
        strip.text.x = element_text(angle = 0, hjust = 0)) +
  labs(x="Total number of samples analyzed", y="Distance to real changepoint") 


p1


```


# Case study: Testing other types of models

Throughout the main text and supplementary material, we focus on detecting changepoints. However, other types of models and criteria may be more appropriate depending on the question. This is especially true for situations where changepoints are not expected apriori. To address this, we used our random and regular sampling approaches with two additional models: linear models and generalized additive models. For simple linear regression, we measured the percent difference in the estimated slope between the subsampled data and the full time series (Fig. \ref{fig:lineartrends}). This is similar to past work [@White2019e; @White2020] on detecting population trends. 

```{r echo=FALSE, eval=T,message=FALSE, warning=FALSE, fig.width = 7.2, fig.height= 3,fig.cap='Percent difference for the estimated slope from linear regression for the abundance time series using (a) random and (b) regular sampling. Iterative sampling could be used for linear trends, but would require additional decisions on how to add samples. \\label{fig:lineartrends}'}

vr <- read.delim(paste0(getpath4data(),"VAR10-10-clado.txt"))
vr_count <- data.frame(Year=vr[,1], Biomass=rowSums(vr[,-1]))
cpvar_count <- sample_regular(vr_count[,2], nrow(vr_count), input_vector = T, messages = F)

#plot(vr_count)
#abline(lm(vr_count$Biomass~vr_count$Year))
true_slope <- as.numeric(coef(lm(vr_count$Biomass~vr_count$Year))[2])

regular_linear <- function(x,y,num_samples){
  true_slope <- as.numeric(coef(lm(y~x))[2])
  
  subset_index <- c(seq(from = 1, to = length(x),length.out =num_samples),length(x))
  subset_x <- x[subset_index]
  subset_y <- y[subset_index]
  
  subset_slope <- as.numeric(coef(lm(subset_y~subset_x))[2])

 return(abs(true_slope - subset_slope)/true_slope)  
}

spacing_vec <- seq(3,70,by=3)
distance <- vector(mode='numeric',length = length(spacing_vec))
for (i in 1:length(spacing_vec)){
  distance[i] <- regular_linear(x = vr_count$Year,y=vr_count$Biomass,num_samples =spacing_vec[i])
  
}
par(mfrow=c(1,2),mar=c(0.5,0.1,1.5,0.1),oma=c(4,4,0.5,0.5))
plot(spacing_vec,distance,las=1,ylim=c(0,1.2),col='darkgrey',pch=16)
abline(h=0,col='darkgrey',lwd=1.5)
mtext('a. Random sampling - linear model',side = 3,line = 0,adj = 0)



# Random sampling 

random_linear <- function(x,y,num_samples){
  true_slope <- as.numeric(coef(lm(y~x))[2])
  
  subset_index <- sort(c(1,sample(2:(length(x)-1),size = num_samples),length(x)))
  subset_x <- x[subset_index]
  subset_y <- y[subset_index]
  
  subset_slope <- as.numeric(coef(lm(subset_y~subset_x))[2])
  
  return(abs(true_slope - subset_slope)/true_slope)  
}

#spacing_vec <- 3:60
distance <- vector(mode='numeric',length = length(spacing_vec))
for (i in 1:length(spacing_vec)){
  distance[i] <- random_linear(x = vr_count$Year,y=vr_count$Biomass,num_samples =spacing_vec[i])
  
}
plot(spacing_vec,distance,las=1,ylim=c(0,1.2),yaxt='n',col='darkgrey',pch=16)
abline(h=0,col='darkgrey',lwd=1.5)
mtext('b. Regular sampling - linear model',side = 3,line = 0,adj = 0)
mtext('Percent difference',side = 2,line = 2.5,outer = T,font = 2)
mtext('Total number of samples analyzed',side = 1,line = 2.5,outer = T,font = 2)


```

Similarly, for a generalized additive model, we examined the shape of the estimated curve for subsets of data compared to the full time series (Fig. \ref{fig:gamtrends}). In line with our other results, the regular sampling performed better than random sampling upon visual inspection. Both sampling approaches also visually approach the full data set with fewer samples compared to the linear models.

```{r echo=FALSE, eval=T,message=FALSE, warning=FALSE, fig.width = 7.2, fig.height= 3,fig.cap='Percent difference for the estimated slope from linear regression for the abundance time series using (a) random and (b) regular sampling. Iterative sampling could be used for linear trends, but would require additional decisions on how to add samples. \\label{fig:gamtrends}'}

vr <- read.delim(paste0(getpath4data(),"VAR10-10-clado.txt"))
vr_count <- data.frame(Year=vr[,1], Biomass=rowSums(vr[,-1]))
cpvar_count <- sample_regular(vr_count[,2], nrow(vr_count), input_vector = T, messages = F)

smoothing_parameter=5
require(mgcv)

par(mfrow=c(2,4),mar=c(0.5,0.1,1.5,0.1),oma=c(4,4,0.5,0.5))

subset_index <- sort(c(1,sample(2:(length(vr_count$Year)-1),size = 10),length(vr_count$Year)))
vr_count_subset <- vr_count[subset_index,]
gam1_rand <- gam(vr_count_subset$Biomass ~ s(vr_count_subset$Year,k=smoothing_parameter),family=gaussian,)
plot(gam1_rand,ylim=c(-50000,50000),xaxt='n')
mtext('a. Random (n=10)',side = 3,line = 0,adj = 0)

subset_index <- sort(c(1,sample(2:(length(vr_count$Year)-1),size = 20),length(vr_count$Year)))
vr_count_subset <- vr_count[subset_index,]
gam2_rand <- gam(vr_count_subset$Biomass ~ s(vr_count_subset$Year,k=smoothing_parameter),family=gaussian,)
plot(gam2_rand,ylim=c(-50000,50000),xaxt='n',yaxt='n')
mtext('b. Random (n=20)',side = 3,line = 0,adj = 0)

subset_index <- sort(c(1,sample(2:(length(vr_count$Year)-1),size = 30),length(vr_count$Year)))
vr_count_subset <- vr_count[subset_index,]
gam3_rand <- gam(vr_count_subset$Biomass ~ s(vr_count_subset$Year,k=smoothing_parameter),family=gaussian,)
plot(gam3_rand,ylim=c(-50000,50000),xaxt='n',yaxt='n')
mtext('c. Random (n=30)',side = 3,line = 0,adj = 0)


gam4_rand <- gam(vr_count$Biomass ~ s(vr_count$Year,k=smoothing_parameter),family=gaussian)
plot(gam4_rand,ylim=c(-50000,50000),xaxt='n',yaxt='n')
mtext('d. Random (n=74)',side = 3,line = 0,adj = 0)


# Regular GAMs

subset_index <- c(seq(from = 1, to = length(vr_count$Year),length.out =10),length(vr_count$Year))
vr_count_subset <- vr_count[subset_index,]
gam1_rand <- gam(vr_count_subset$Biomass ~ s(vr_count_subset$Year,k=smoothing_parameter),family=gaussian,)
plot(gam1_rand,ylim=c(-50000,50000))
mtext('e. Regular (n=10)',side = 3,line = 0,adj = 0)

subset_index <- c(seq(from = 1, to = length(vr_count$Year),length.out =20),length(vr_count$Year))
vr_count_subset <- vr_count[subset_index,]
gam2_rand <- gam(vr_count_subset$Biomass ~ s(vr_count_subset$Year,k=smoothing_parameter),family=gaussian,)
plot(gam2_rand,ylim=c(-50000,50000),yaxt='n')
mtext('f. Regular (n=20)',side = 3,line = 0,adj = 0)

subset_index <- c(seq(from = 1, to = length(vr_count$Year),length.out =30),length(vr_count$Year))
vr_count_subset <- vr_count[subset_index,]
gam3_rand <- gam(vr_count_subset$Biomass ~ s(vr_count_subset$Year,k=smoothing_parameter),family=gaussian,)
plot(gam3_rand,ylim=c(-50000,50000),yaxt='n')
mtext('g. Regular (n=30)',side = 3,line = 0,adj = 0)


gam4_rand <- gam(vr_count$Biomass ~ s(vr_count$Year,k=smoothing_parameter),family=gaussian)
plot(gam4_rand,ylim=c(-50000,50000),yaxt='n')
mtext('h. Regular (n=40)',side = 3,line = 0,adj = 0)

mtext('Standardized abundance',side = 2,line = 2.5,outer = T,font = 2)
mtext('Time (year)',side = 1,line = 2.5,outer = T,font = 2)

```

\clearpage

# References